{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_W_D_W&D.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNfX3S+LCN1muP6/xkay82B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fellowship/deep-and-wide-bandit/blob/dev/TensorFlow_W_D_W%26D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2n6dXxu3CjB"
      },
      "source": [
        "# Required Installs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjlUj5ABCuck"
      },
      "source": [
        "# Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oFqkg0Mp_hu4",
        "outputId": "15984a0f-aad4-4539-f0c3-18ad550ff10f"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ladp5N5YYeP3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPSINg6-DWp7",
        "outputId": "41103aa4-e8f8-487c-c93d-7fb4259c983b"
      },
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grwgxGGsELGO"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from zipfile import ZipFile\n",
        "import re\n",
        "import json\n",
        "import pickle as pkl\n",
        "import re\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from functools import partial\n",
        "import random\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "from pprint import pprint\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from IPython.core.interactiveshell import InteractiveShell  \n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "#Makes panda and numpy easier to read\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_saGcV11C0Ol"
      },
      "source": [
        "# Get the data ready\n",
        "\n",
        "\n",
        "We extract the weekly dataset CSVs & shortlisted train+valid index CSVs s.t. we have 1 train and 1 valid index CSV per weekly CSV.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDkGrySRyK7q"
      },
      "source": [
        "def delete_all(directory):\n",
        "    for item in sorted(directory.rglob('*')):        \n",
        "        if item.is_file():\n",
        "          print(f\"[INFO] Deleting file {item.name}\")\n",
        "          item.unlink()\n",
        "        elif item.is_dir():\n",
        "          delete_all(item)\n",
        "          item.rmdir()\n",
        "          print(f\"[INFO] Deleting folder {item.name}\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6ZYrGnOG_T7"
      },
      "source": [
        "overwrite_zip_extract = False\n",
        "\n",
        "#Build the list of train_valid items\n",
        "data_paths_l = [Path(\"/content/drive/MyDrive/Bandit_Project/BanditsData/Jul_Dec_2019.zip\"),\n",
        "                 Path(\"/content/drive/MyDrive/Bandit_Project/BanditsData/Jan_Jun_2020.zip\"),\n",
        "                 Path(\"/content/drive/MyDrive/Bandit_Project/BanditsData/Jul_Sep_2020.zip\"),\n",
        "                Path(\"/content/drive/MyDrive/Bandit_Project/BanditsData/Old/Train_5pct_Jul_Dec_2019.zip\"),\n",
        "                Path(\"/content/drive/MyDrive/Bandit_Project/BanditsData/Old/Train_5pct_Jan_Jun_2020.zip\")]\n",
        "\n",
        "#Extract items to a folder in your GDrive\n",
        "folder = Path(\"/content/drive/MyDrive/Bandit_Project/aleksey\")\n",
        "\n",
        "#If overwrite flag is set to True - we delete all the files\n",
        "if overwrite_zip_extract:\n",
        "  delete_all(folder)\n",
        "  for path in data_paths_l:\n",
        "      with ZipFile(path, 'r') as zip_obj:\n",
        "        print(f\"[INFO] Extracting {path.name}:\")\n",
        "        zip_obj.extractall(folder)\n",
        "\n",
        "#Create train, valid & test folder\n",
        "train_folder = (folder/'train')\n",
        "train_folder.mkdir(exist_ok=True)\n",
        "\n",
        "valid_folder = (folder/'valid')\n",
        "valid_folder.mkdir(exist_ok=True)\n",
        "\n",
        "test_folder = (folder/'test')\n",
        "test_folder.mkdir(exist_ok=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z9jUTvmcUzq"
      },
      "source": [
        "## Subsetting Train & Valid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e4ZXk7DRLH_"
      },
      "source": [
        "#Function to export data corresponding to chosen indices to train/valid folder\n",
        "def export_data_subset(data_l, save_folder, idx_l=None, overwrite_flag = False):\n",
        "\n",
        "  cnt = 0\n",
        "\n",
        "  #If the folder is not empty and overwrite_flag is set to True, delete all files in the folder\n",
        "  if overwrite_flag:\n",
        "    \n",
        "    print(f\"\\n[INFO] Deleting files in {save_folder.name} directory...\")\n",
        "    delete_all(save_folder)\n",
        "  \n",
        "  #Check whether idx_l is None\n",
        "  if not(idx_l):\n",
        "    \n",
        "    #Just copy-paste all files from data_l to save_folder\n",
        "    for data_path in data_l:\n",
        "      \n",
        "      destination = save_folder/(data_path.name)\n",
        "      if not destination.exists():\n",
        "        data_path.replace(destination)\n",
        "  \n",
        "  else:\n",
        "\n",
        "    #Iterate over the (weekly data path, training set indices path) zipped object\n",
        "    for (data_path, idx_path) in zip(data_l, idx_l):\n",
        "\n",
        "      if (cnt + 1) % 5 == 0:\n",
        "        print(f\"[INFO] Building {save_folder.name} file from {data_path.name}\")\n",
        "      \n",
        "      #Use pandas to read weekly data + corresponding index CSV files\n",
        "      data = pd.read_csv(data_path)\n",
        "      idx = pd.read_csv(idx_path, header=None, squeeze=True).tolist()\n",
        "\n",
        "      #Subset the data and save it to the appropriate csv file\n",
        "      data_subset = data.iloc[idx, :]\n",
        "\n",
        "      #Save the data subset\n",
        "      data_subset.to_csv(save_folder/(data_path.name), index=False, compression=\"gzip\", header=True)\n",
        "\n",
        "    #Increment Counter\n",
        "    cnt += 1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8azEEnPWI87F"
      },
      "source": [
        "def process_input(source, dest, x_cols, y_col, overwrite=True, ctype = \"infer\"):\n",
        "  \n",
        "  filename = source.name + \".csv.gz\"\n",
        "  dest_path = dest/filename\n",
        "  \n",
        "  #Check whether files exist in destination + should not overwrite - If they do, print an error message  \n",
        "  if dest_path.exists():\n",
        "    if not(overwrite):\n",
        "      print(f\"[ERROR] {dest_path.name} currently exists. Pls set overwrite flag to True!\")\n",
        "      return dest_path\n",
        "    else:\n",
        "      #Delete current files in the dest folder\n",
        "      delete_all(dest)\n",
        "\n",
        "  cnt = 0\n",
        "\n",
        "  #Iterate over each file in source\n",
        "  for file_path in source.iterdir():\n",
        "\n",
        "    #Print update\n",
        "    print(f\"[INFO] Currently working on {source.name}: {file_path.name}\")\n",
        "    \n",
        "    #Read in the data\n",
        "    data = pd.read_csv(file_path, compression=ctype, header=[0])\n",
        "\n",
        "    #Shortlist columns to get the overall CSV\n",
        "    cols = x_cols + y_col\n",
        "    subset = data[cols]     \n",
        "\n",
        "    #Check whether first CSV file â€”> include header, otherwise ignore\n",
        "    header_flag = True if not(cnt) else False\n",
        "\n",
        "    #Save to dest    \n",
        "    subset.to_csv(dest_path, mode='a', compression=\"gzip\", header=header_flag, index=False)\n",
        "\n",
        "    #Increment counter\n",
        "    cnt += 1\n",
        "  \n",
        "  #Return the path to the processed input file\n",
        "  return dest_path"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWlsPxcLfpNY",
        "outputId": "7d439828-a8ed-4604-c448-d7616b13ba7e"
      },
      "source": [
        "#Setting overwrite flag\n",
        "overwrite_dset = False\n",
        "\n",
        "#We need to connect a file like \"sends_2019_wk26.csv\" WITH \"selected_rows_sends_2019_wk26.csv\"\n",
        "weekly_data_path_l = sorted([i for i in folder.iterdir() if re.search(\"/sends\", str(i), re.I)], key=lambda x: str(x))\n",
        "valid_indices_path_l = sorted([i for i in folder.iterdir() if re.search(\"/selected_rows_train\", str(i), re.I)], key=lambda x: str(x))\n",
        "#print(f\"[INFO] Displaying indices to build validation data for {str(weekly_data_path_l[0])}: {valid_indices_path_l[0]}\")\n",
        "\n",
        "#train_indices_path_l = sorted([i for i in folder.iterdir() if re.search(\"/selected_rows_sends\", str(i), re.I)], key=lambda x: str(x))\n",
        "#print(f\"[INFO] Displaying indices to build training data for {str(weekly_data_path_l[0])}: {train_indices_path_l[0]}\")\n",
        "\n",
        "#Execute only if valid folder is empty\n",
        "if not(list(valid_folder.iterdir())) or overwrite_dset:\n",
        "\n",
        "  #Run the export function\n",
        "  export_data_subset(data_l = weekly_data_path_l, save_folder = valid_folder, \n",
        "                     idx_l = valid_indices_path_l, overwrite_flag = overwrite_dset)\n",
        "\n",
        "else:\n",
        "  print(\"[INFO] Validation Data Already Created...\\n\")\n",
        "\n",
        "#Check if training folder is empty\n",
        "if not(list(train_folder.iterdir())) or overwrite_dset:\n",
        "\n",
        "  #Run the export function\n",
        "  export_data_subset(data_l = weekly_data_path_l, save_folder = train_folder, overwrite_flag = overwrite_dset)\n",
        "\n",
        "else:\n",
        "  print(\"\\n[INFO] Training Data Already Created...\")\n",
        "\n",
        "train_list = sorted([file_path for file_path in train_folder.iterdir()], key = lambda x: str(x))\n",
        "valid_list = sorted([file_path for file_path in valid_folder.iterdir()], key = lambda x: str(x))\n",
        "test_list = sorted([file_path for file_path in test_folder.iterdir()], key = lambda x: str(x))\n",
        "\n",
        "print(\"\\n[INFO] Displaying the first 5 elements of train_list:\")\n",
        "pprint(train_list[:5])\n",
        "print(\"\\n[INFO] Displaying the first 5 elements of valid_list:\")\n",
        "pprint(valid_list[:5])\n",
        "print(\"\\n[INFO] Displaying the first 5 elements of test_list:\")\n",
        "pprint(test_list[:5])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Validation Data Already Created...\n",
            "\n",
            "\n",
            "[INFO] Training Data Already Created...\n",
            "\n",
            "[INFO] Displaying the first 5 elements of train_list:\n",
            "[PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/train/sends_2019_wk26.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/train/sends_2019_wk27.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/train/sends_2019_wk28.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/train/sends_2019_wk29.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/train/sends_2019_wk30.csv')]\n",
            "\n",
            "[INFO] Displaying the first 5 elements of valid_list:\n",
            "[PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/valid/sends_2019_wk26.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/valid/sends_2019_wk27.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/valid/sends_2019_wk28.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/valid/sends_2019_wk29.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/valid/sends_2019_wk30.csv')]\n",
            "\n",
            "[INFO] Displaying the first 5 elements of test_list:\n",
            "[PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/test/sends_2020_wk26.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/test/sends_2020_wk27.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/test/sends_2020_wk28.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/test/sends_2020_wk29.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/test/sends_2020_wk30.csv')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZggU1qg8xvVv",
        "outputId": "2c556238-8c78-4d66-ca8a-1c284624045c"
      },
      "source": [
        "#Display head of first element of both training & validation subset\n",
        "train_sample = pd.read_csv(train_list[0], header=[0])\n",
        "print(\"[INFO] Sample Training Data\")\n",
        "train_sample.head()\n",
        "valid_sample = pd.read_csv(valid_list[0], compression='gzip', header=[0])\n",
        "print(\"[INFO] Sample Validation Data\")\n",
        "valid_sample.head()\n",
        "test_sample = pd.read_csv(test_list[0], header=[0])\n",
        "print(\"[INFO] Sample Test Data\")\n",
        "test_sample.head()\n",
        "\n",
        "#Get the column names of the data\n",
        "data_col_names = train_sample.columns.tolist()\n",
        "print(\"[INFO] The full list of column names include:\")\n",
        "pprint(data_col_names)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Sample Training Data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>riid</th>\n",
              "      <th>retention_score</th>\n",
              "      <th>frequency_score</th>\n",
              "      <th>recency_score</th>\n",
              "      <th>sends_since_last_open</th>\n",
              "      <th>times_seen</th>\n",
              "      <th>times_open</th>\n",
              "      <th>days_subscr</th>\n",
              "      <th>aq_year</th>\n",
              "      <th>aq_week</th>\n",
              "      <th>aq_dayofweek</th>\n",
              "      <th>aq_period</th>\n",
              "      <th>campaign_id</th>\n",
              "      <th>campaign_category</th>\n",
              "      <th>campaign_Brand</th>\n",
              "      <th>campaign_Core</th>\n",
              "      <th>campaign_Dedicated</th>\n",
              "      <th>campaign_InnovationSpotlight</th>\n",
              "      <th>campaign_NewArrivals</th>\n",
              "      <th>campaign_ProductSpotlight</th>\n",
              "      <th>campaign_Replen</th>\n",
              "      <th>campaign_Tops</th>\n",
              "      <th>campaign_Trend</th>\n",
              "      <th>campaign_Other</th>\n",
              "      <th>discount</th>\n",
              "      <th>promo</th>\n",
              "      <th>sale</th>\n",
              "      <th>is_one_for_free</th>\n",
              "      <th>free_shipping</th>\n",
              "      <th>is_exclusive</th>\n",
              "      <th>has_urgency</th>\n",
              "      <th>sl_contains_price</th>\n",
              "      <th>is_discount_mentioned</th>\n",
              "      <th>message_size</th>\n",
              "      <th>sent_week</th>\n",
              "      <th>sent_dayofweek</th>\n",
              "      <th>sent_hr</th>\n",
              "      <th>opened</th>\n",
              "      <th>unsub</th>\n",
              "      <th>rev_3dv2</th>\n",
              "      <th>reward</th>\n",
              "      <th>optimal_action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>194725242</td>\n",
              "      <td>1.474</td>\n",
              "      <td>4</td>\n",
              "      <td>0.451</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1352</td>\n",
              "      <td>2015</td>\n",
              "      <td>41</td>\n",
              "      <td>6</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>157528</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>77.320</td>\n",
              "      <td>-10</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>232343542</td>\n",
              "      <td>28.000</td>\n",
              "      <td>26</td>\n",
              "      <td>7.870</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>544</td>\n",
              "      <td>2018</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>157792</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>17.980</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>58700702</td>\n",
              "      <td>28.000</td>\n",
              "      <td>39</td>\n",
              "      <td>13.089</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1581</td>\n",
              "      <td>2015</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>157878</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>17.980</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>263122862</td>\n",
              "      <td>28.000</td>\n",
              "      <td>53</td>\n",
              "      <td>15.349</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>109</td>\n",
              "      <td>2019</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>158325</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>35.960</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>987902</td>\n",
              "      <td>28.000</td>\n",
              "      <td>26</td>\n",
              "      <td>11.127</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2372</td>\n",
              "      <td>2013</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Other</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>156214</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>36.970</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        riid  retention_score  ...  reward  optimal_action\n",
              "0  194725242            1.474  ...     -10               1\n",
              "1  232343542           28.000  ...       8               1\n",
              "2   58700702           28.000  ...       9               1\n",
              "3  263122862           28.000  ...       9               1\n",
              "4     987902           28.000  ...       9               1\n",
              "\n",
              "[5 rows x 42 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] Sample Validation Data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>riid</th>\n",
              "      <th>retention_score</th>\n",
              "      <th>frequency_score</th>\n",
              "      <th>recency_score</th>\n",
              "      <th>sends_since_last_open</th>\n",
              "      <th>times_seen</th>\n",
              "      <th>times_open</th>\n",
              "      <th>days_subscr</th>\n",
              "      <th>aq_year</th>\n",
              "      <th>aq_week</th>\n",
              "      <th>aq_dayofweek</th>\n",
              "      <th>aq_period</th>\n",
              "      <th>campaign_id</th>\n",
              "      <th>campaign_category</th>\n",
              "      <th>campaign_Brand</th>\n",
              "      <th>campaign_Core</th>\n",
              "      <th>campaign_Dedicated</th>\n",
              "      <th>campaign_InnovationSpotlight</th>\n",
              "      <th>campaign_NewArrivals</th>\n",
              "      <th>campaign_ProductSpotlight</th>\n",
              "      <th>campaign_Replen</th>\n",
              "      <th>campaign_Tops</th>\n",
              "      <th>campaign_Trend</th>\n",
              "      <th>campaign_Other</th>\n",
              "      <th>discount</th>\n",
              "      <th>promo</th>\n",
              "      <th>sale</th>\n",
              "      <th>is_one_for_free</th>\n",
              "      <th>free_shipping</th>\n",
              "      <th>is_exclusive</th>\n",
              "      <th>has_urgency</th>\n",
              "      <th>sl_contains_price</th>\n",
              "      <th>is_discount_mentioned</th>\n",
              "      <th>message_size</th>\n",
              "      <th>sent_week</th>\n",
              "      <th>sent_dayofweek</th>\n",
              "      <th>sent_hr</th>\n",
              "      <th>opened</th>\n",
              "      <th>unsub</th>\n",
              "      <th>rev_3dv2</th>\n",
              "      <th>reward</th>\n",
              "      <th>optimal_action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>263471642</td>\n",
              "      <td>28.000</td>\n",
              "      <td>8</td>\n",
              "      <td>5.379</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>92</td>\n",
              "      <td>2019</td>\n",
              "      <td>12</td>\n",
              "      <td>6</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>156730</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>75.000</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>218682462</td>\n",
              "      <td>28.000</td>\n",
              "      <td>25</td>\n",
              "      <td>3.873</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>804</td>\n",
              "      <td>2017</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>158117</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>17.980</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>232668962</td>\n",
              "      <td>28.000</td>\n",
              "      <td>84</td>\n",
              "      <td>15.843</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>538</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>157875</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>42.560</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>165404162</td>\n",
              "      <td>0.757</td>\n",
              "      <td>4</td>\n",
              "      <td>0.109</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1474</td>\n",
              "      <td>2015</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>155171</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>211481602</td>\n",
              "      <td>1.647</td>\n",
              "      <td>6</td>\n",
              "      <td>0.391</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>960</td>\n",
              "      <td>2016</td>\n",
              "      <td>45</td>\n",
              "      <td>6</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>157379</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-18</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        riid  retention_score  ...  reward  optimal_action\n",
              "0  263471642           28.000  ...       8               1\n",
              "1  218682462           28.000  ...       9               1\n",
              "2  232668962           28.000  ...       9               1\n",
              "3  165404162            0.757  ...     -38               0\n",
              "4  211481602            1.647  ...     -18               0\n",
              "\n",
              "[5 rows x 42 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] Sample Test Data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>riid</th>\n",
              "      <th>retention_score</th>\n",
              "      <th>frequency_score</th>\n",
              "      <th>recency_score</th>\n",
              "      <th>sends_since_last_open</th>\n",
              "      <th>times_seen</th>\n",
              "      <th>times_open</th>\n",
              "      <th>days_subscr</th>\n",
              "      <th>aq_year</th>\n",
              "      <th>aq_week</th>\n",
              "      <th>aq_dayofweek</th>\n",
              "      <th>aq_period</th>\n",
              "      <th>campaign_id</th>\n",
              "      <th>campaign_category</th>\n",
              "      <th>campaign_Brand</th>\n",
              "      <th>campaign_Core</th>\n",
              "      <th>campaign_Dedicated</th>\n",
              "      <th>campaign_InnovationSpotlight</th>\n",
              "      <th>campaign_NewArrivals</th>\n",
              "      <th>campaign_ProductSpotlight</th>\n",
              "      <th>campaign_Replen</th>\n",
              "      <th>campaign_Tops</th>\n",
              "      <th>campaign_Trend</th>\n",
              "      <th>campaign_Other</th>\n",
              "      <th>discount</th>\n",
              "      <th>promo</th>\n",
              "      <th>sale</th>\n",
              "      <th>is_one_for_free</th>\n",
              "      <th>free_shipping</th>\n",
              "      <th>is_exclusive</th>\n",
              "      <th>has_urgency</th>\n",
              "      <th>sl_contains_price</th>\n",
              "      <th>is_discount_mentioned</th>\n",
              "      <th>message_size</th>\n",
              "      <th>sent_week</th>\n",
              "      <th>sent_dayofweek</th>\n",
              "      <th>sent_hr</th>\n",
              "      <th>opened</th>\n",
              "      <th>unsub</th>\n",
              "      <th>rev_3dv2</th>\n",
              "      <th>reward</th>\n",
              "      <th>optimal_action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6092702</td>\n",
              "      <td>28.000</td>\n",
              "      <td>16</td>\n",
              "      <td>5.860</td>\n",
              "      <td>0</td>\n",
              "      <td>96</td>\n",
              "      <td>16</td>\n",
              "      <td>2732</td>\n",
              "      <td>2013</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59445402</td>\n",
              "      <td>Dedicated</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>130782</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>99.960</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>197298302</td>\n",
              "      <td>0.622</td>\n",
              "      <td>1</td>\n",
              "      <td>0.207</td>\n",
              "      <td>45</td>\n",
              "      <td>87</td>\n",
              "      <td>7</td>\n",
              "      <td>1681</td>\n",
              "      <td>2015</td>\n",
              "      <td>47</td>\n",
              "      <td>6</td>\n",
              "      <td>Holiday</td>\n",
              "      <td>59445402</td>\n",
              "      <td>Dedicated</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>130531</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>79.920</td>\n",
              "      <td>-36</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>276708662</td>\n",
              "      <td>28.000</td>\n",
              "      <td>10</td>\n",
              "      <td>12.000</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>2020</td>\n",
              "      <td>24</td>\n",
              "      <td>5</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59445402</td>\n",
              "      <td>Dedicated</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>129919</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>49.980</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>277083802</td>\n",
              "      <td>28.000</td>\n",
              "      <td>4</td>\n",
              "      <td>11.429</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2020</td>\n",
              "      <td>26</td>\n",
              "      <td>3</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59445402</td>\n",
              "      <td>Dedicated</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>130201</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>97.900</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>253962622</td>\n",
              "      <td>28.000</td>\n",
              "      <td>11</td>\n",
              "      <td>3.434</td>\n",
              "      <td>0</td>\n",
              "      <td>132</td>\n",
              "      <td>18</td>\n",
              "      <td>662</td>\n",
              "      <td>2018</td>\n",
              "      <td>36</td>\n",
              "      <td>3</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59445402</td>\n",
              "      <td>Dedicated</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>130968</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>84.900</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        riid  retention_score  ...  reward  optimal_action\n",
              "0    6092702           28.000  ...       9               1\n",
              "1  197298302            0.622  ...     -36               1\n",
              "2  276708662           28.000  ...       9               1\n",
              "3  277083802           28.000  ...       9               1\n",
              "4  253962622           28.000  ...       9               1\n",
              "\n",
              "[5 rows x 42 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] The full list of column names include:\n",
            "['riid',\n",
            " 'retention_score',\n",
            " 'frequency_score',\n",
            " 'recency_score',\n",
            " 'sends_since_last_open',\n",
            " 'times_seen',\n",
            " 'times_open',\n",
            " 'days_subscr',\n",
            " 'aq_year',\n",
            " 'aq_week',\n",
            " 'aq_dayofweek',\n",
            " 'aq_period',\n",
            " 'campaign_id',\n",
            " 'campaign_category',\n",
            " 'campaign_Brand',\n",
            " 'campaign_Core',\n",
            " 'campaign_Dedicated',\n",
            " 'campaign_InnovationSpotlight',\n",
            " 'campaign_NewArrivals',\n",
            " 'campaign_ProductSpotlight',\n",
            " 'campaign_Replen',\n",
            " 'campaign_Tops',\n",
            " 'campaign_Trend',\n",
            " 'campaign_Other',\n",
            " 'discount',\n",
            " 'promo',\n",
            " 'sale',\n",
            " 'is_one_for_free',\n",
            " 'free_shipping',\n",
            " 'is_exclusive',\n",
            " 'has_urgency',\n",
            " 'sl_contains_price',\n",
            " 'is_discount_mentioned',\n",
            " 'message_size',\n",
            " 'sent_week',\n",
            " 'sent_dayofweek',\n",
            " 'sent_hr',\n",
            " 'opened',\n",
            " 'unsub',\n",
            " 'rev_3dv2',\n",
            " 'reward',\n",
            " 'optimal_action']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTnk1mZnIxKe",
        "outputId": "b87f8e9f-887e-41e6-d345-1e0a628f7b38"
      },
      "source": [
        "#Setting overwrite flag for Processed Data\n",
        "overwrite_processed = False\n",
        "\n",
        "#Create a folders to contain the processed records\n",
        "dest_folder = Path(\"/content/drive/MyDrive/Bandit_Project/aleksey/processed\")\n",
        "dest_folder.mkdir(exist_ok=True)\n",
        "\n",
        "#Build the context that you would like to keep track of\n",
        "user_context_cols = ['riid', 'retention_score', 'frequency_score', 'recency_score', 'sends_since_last_open'] #Not using any aquisition features\n",
        "\n",
        "campaign_context_cols = ['campaign_category', 'discount', 'promo', 'sale', 'is_one_for_free','free_shipping','is_exclusive','has_urgency']\n",
        "\n",
        "email_context_cols = ['sl_contains_price','is_discount_mentioned','sent_week','sent_dayofweek','sent_hr']\n",
        "\n",
        "context_cols = user_context_cols + campaign_context_cols + email_context_cols\n",
        "print(f\"[INFO] The context is {len(context_cols)} cols long...\")\n",
        "\n",
        "#Not sure how to handle train_y: For now, optimal_action classification \n",
        "outcomes_cols = [\"opened\", \"unsub\", \"rev_3dv2\"]\n",
        "reward_cols = [\"reward\"]\n",
        "action_cols = [\"optimal_action\"]\n",
        "\n",
        "#Process the files in both train & valid\n",
        "train_file_path = process_input(train_folder, dest_folder, context_cols, action_cols, overwrite_processed)\n",
        "val_file_path = process_input(valid_folder, dest_folder, context_cols, action_cols, overwrite_processed, ctype=\"gzip\")\n",
        "test_file_path = process_input(test_folder, dest_folder, context_cols, action_cols, overwrite_processed)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] The context is 18 cols long...\n",
            "[ERROR] train.csv.gz currently exists. Pls set overwrite flag to True!\n",
            "[ERROR] valid.csv.gz currently exists. Pls set overwrite flag to True!\n",
            "[ERROR] test.csv.gz currently exists. Pls set overwrite flag to True!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ennp6DLmgNs"
      },
      "source": [
        "# Statistical Analysis Of Outcomes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dBKxRgBpjy5"
      },
      "source": [
        "[DONE] In this section, we have confirmed 3 things:\n",
        "\n",
        "\n",
        "1.   How many people opened vs did not open\n",
        "2.   How many people unsubscribed vs did not unsubscribe\n",
        "3.   Whether optimal action is 1 for open OR purchase and 0 for not open OR unsub\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "xuhMtUMFp8-I",
        "outputId": "e8616c51-d9a9-4de9-d337-b683d4157176"
      },
      "source": [
        "\"\"\"\n",
        "length_l = []\n",
        "opens_l = []\n",
        "unsubs_l = []\n",
        "unique_users_s = set()\n",
        "campaign_types_s = set()\n",
        "\n",
        "#Iterate through the weekly data\n",
        "for file_path in weekly_data_path_l:\n",
        "\n",
        "  #Print status update\n",
        "  print(f\"[INFO] Working on {file_path.name}\")\n",
        "  \n",
        "  #Count the number of elements\n",
        "  df = pd.read_csv(file_path)\n",
        "  length = len(df)\n",
        "  length_l.append(length)\n",
        "\n",
        "  #Count Opens vs Not Opens\n",
        "  opens = (df[\"opened\"] == 1).astype(int).sum()\n",
        "  opens_l.append(opens)\n",
        "\n",
        "  #Count Unsubs vs Not Unsubs\n",
        "  unsubs = (df[\"unsub\"] == 1).astype(int).sum()\n",
        "  unsubs_l.append(unsubs)\n",
        "\n",
        "  #Assert whether optimal action follows the rule\n",
        "  #Create a Panda Series that follows this rule and assert\n",
        "  check_series = pd.Series(np.ones_like(df[\"optimal_action\"].values, dtype=int))\n",
        "\n",
        "  check_series[df[\"opened\"] == 0] = 0\n",
        "  check_series[df[\"unsub\"] == 1] = 0\n",
        "  assert (df[\"optimal_action\"] == check_series).all()\n",
        "\n",
        "  #Update the unique user set with user IDs from the df\n",
        "  unique_users = list(df[\"riid\"].unique())\n",
        "  unique_users_s.update(unique_users)\n",
        "\n",
        "  #Update the unique campaign type set from the df\n",
        "  campaign_types = list(df['campaign_category'].unique())\n",
        "  campaign_types_s.update(campaign_types)\n",
        "\n",
        "total_length = sum(length_l)\n",
        "print(f\"[INFO] Total number of data - {total_length}\")\n",
        "\n",
        "opened = sum(opens_l)/total_length\n",
        "unsub = sum(unsubs_l)/total_length\n",
        "\n",
        "print(f\"[INFO] % of opened - {opened}\")\n",
        "print(f\"[INFO] % of unsubscribed - {unsub}\")\n",
        "print(f\"[INFO] # of unique users - {len(unique_users_s)}\")\n",
        "print(f\"[INFO] # of unique campaign types - {len(campaign_types_s)}\")\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nlength_l = []\\nopens_l = []\\nunsubs_l = []\\nunique_users_s = set()\\ncampaign_types_s = set()\\n\\n#Iterate through the weekly data\\nfor file_path in weekly_data_path_l:\\n\\n  #Print status update\\n  print(f\"[INFO] Working on {file_path.name}\")\\n  \\n  #Count the number of elements\\n  df = pd.read_csv(file_path)\\n  length = len(df)\\n  length_l.append(length)\\n\\n  #Count Opens vs Not Opens\\n  opens = (df[\"opened\"] == 1).astype(int).sum()\\n  opens_l.append(opens)\\n\\n  #Count Unsubs vs Not Unsubs\\n  unsubs = (df[\"unsub\"] == 1).astype(int).sum()\\n  unsubs_l.append(unsubs)\\n\\n  #Assert whether optimal action follows the rule\\n  #Create a Panda Series that follows this rule and assert\\n  check_series = pd.Series(np.ones_like(df[\"optimal_action\"].values, dtype=int))\\n\\n  check_series[df[\"opened\"] == 0] = 0\\n  check_series[df[\"unsub\"] == 1] = 0\\n  assert (df[\"optimal_action\"] == check_series).all()\\n\\n  #Update the unique user set with user IDs from the df\\n  unique_users = list(df[\"riid\"].unique())\\n  unique_users_s.update(unique_users)\\n\\n  #Update the unique campaign type set from the df\\n  campaign_types = list(df[\\'campaign_category\\'].unique())\\n  campaign_types_s.update(campaign_types)\\n\\ntotal_length = sum(length_l)\\nprint(f\"[INFO] Total number of data - {total_length}\")\\n\\nopened = sum(opens_l)/total_length\\nunsub = sum(unsubs_l)/total_length\\n\\nprint(f\"[INFO] % of opened - {opened}\")\\nprint(f\"[INFO] % of unsubscribed - {unsub}\")\\nprint(f\"[INFO] # of unique users - {len(unique_users_s)}\")\\nprint(f\"[INFO] # of unique campaign types - {len(campaign_types_s)}\")\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcoRpQ7bAF2a"
      },
      "source": [
        "# Preparing the Dataset for Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFaTHGQwxgls"
      },
      "source": [
        "## Creating the Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2BSe0utxpvq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "3e5a932c-c821-4e91-bb1a-0286f8fe27b5"
      },
      "source": [
        "#Given CSV file patterns, convert them to DL\n",
        "def csvs_to_dataloader(file_pattern, target = \"optimal_action\", shuffle=True, \n",
        "                       batch_size=1024, ctype=None, n_epochs = None):\n",
        "  \n",
        "  return tf.data.experimental.make_csv_dataset(file_pattern = file_pattern, \n",
        "                                             batch_size = batch_size,\n",
        "                                             label_name= target,\n",
        "                                             header=True, \n",
        "                                             num_epochs=n_epochs,\n",
        "                                             shuffle=True,  \n",
        "                                             shuffle_seed=42,\n",
        "                                             compression_type=ctype, \n",
        "                                             ignore_errors=True)\n",
        "  \n",
        "\n",
        "\"\"\"\n",
        "#Given a dataframe in memory\n",
        "def df_to_dataloader(dataframe, target, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop(target)\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  dl = ds.batch(batch_size)\n",
        "  return dl\n",
        "\"\"\"  "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#Given a dataframe in memory\\ndef df_to_dataloader(dataframe, target, shuffle=True, batch_size=32):\\n  dataframe = dataframe.copy()\\n  labels = dataframe.pop(target)\\n  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\\n  if shuffle:\\n    ds = ds.shuffle(buffer_size=len(dataframe))\\n  dl = ds.batch(batch_size)\\n  return dl\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "ysesaBYW--TX",
        "outputId": "ede8a6e4-f8e8-4b3c-84ad-ccade323171c"
      },
      "source": [
        "\"\"\"\n",
        "def input_fn(df_data, target, num_epochs = 50, shuffle = True, batch_size = 32):\n",
        "  \n",
        "  #df_data = pd.read_csv(data_file, header=[0], skiprows=1)\n",
        "  # remove NaN elements\n",
        "  #df_data = df_data.dropna(how=\"any\", axis=0)\n",
        "  df_data = df_data.copy()\n",
        "  labels = df_data.pop(target)\n",
        "  return tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
        "      x=df_data,\n",
        "      y=labels,\n",
        "      batch_size=batch_size,\n",
        "      num_epochs=num_epochs,\n",
        "      shuffle=shuffle)\n",
        "\"\"\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef input_fn(df_data, target, num_epochs = 50, shuffle = True, batch_size = 32):\\n  \\n  #df_data = pd.read_csv(data_file, header=[0], skiprows=1)\\n  # remove NaN elements\\n  #df_data = df_data.dropna(how=\"any\", axis=0)\\n  df_data = df_data.copy()\\n  labels = df_data.pop(target)\\n  return tf.compat.v1.estimator.inputs.pandas_input_fn(\\n      x=df_data,\\n      y=labels,\\n      batch_size=batch_size,\\n      num_epochs=num_epochs,\\n      shuffle=shuffle)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPDdpqjh6ZYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5227a450-a9c3-4f28-860e-10b4fcec6322"
      },
      "source": [
        "batch_size=1024\n",
        "n_epochs=25\n",
        "\n",
        "train_file_pattern = \"/content/drive/MyDrive/Bandit_Project/aleksey/processed/train.csv.gz\"\n",
        "train_dl_train = lambda : csvs_to_dataloader(train_file_pattern, n_epochs = n_epochs, ctype=\"GZIP\")\n",
        "train_dl_fit = csvs_to_dataloader(train_file_pattern, n_epochs = n_epochs, ctype=\"GZIP\")\n",
        "\n",
        "val_file_pattern = \"/content/drive/MyDrive/Bandit_Project/aleksey/processed/valid.csv.gz\"\n",
        "val_dl_train = lambda : csvs_to_dataloader(val_file_pattern, n_epochs = n_epochs, ctype=\"GZIP\")\n",
        "val_dl_fit = csvs_to_dataloader(val_file_pattern, n_epochs = n_epochs, ctype=\"GZIP\")\n",
        "\n",
        "test_file_pattern = \"/content/drive/MyDrive/Bandit_Project/aleksey/processed/test.csv.gz\"\n",
        "test_dl_train = lambda : csvs_to_dataloader(test_file_pattern, n_epochs = n_epochs, ctype=\"GZIP\")\n",
        "test_dl_fit = csvs_to_dataloader(test_file_pattern, n_epochs = n_epochs, ctype=\"GZIP\")\n",
        "\n",
        "pprint(train_dl_fit)\n",
        "pprint(val_dl_fit)\n",
        "pprint(test_dl_fit)\n",
        "\n",
        "#train_dl = input_fn(train, \"optimal_action\", batch_size=batch_size)\n",
        "#val_dl = input_fn(val, \"optimal_action\", shuffle=False, batch_size=batch_size)\n",
        "#train_dl = lambda : df_to_dataloader(train, \"optimal_action\", batch_size=batch_size)\n",
        "#val_dl = lambda : df_to_dataloader(val, \"optimal_action\", shuffle=False, batch_size=batch_size)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: (OrderedDict([(riid, (None,)), (retention_score, (None,)), (frequency_score, (None,)), (recency_score, (None,)), (sends_since_last_open, (None,)), (campaign_category, (None,)), (discount, (None,)), (promo, (None,)), (sale, (None,)), (is_one_for_free, (None,)), (free_shipping, (None,)), (is_exclusive, (None,)), (has_urgency, (None,)), (sl_contains_price, (None,)), (is_discount_mentioned, (None,)), (sent_week, (None,)), (sent_dayofweek, (None,)), (sent_hr, (None,))]), (None,)), types: (OrderedDict([(riid, tf.int32), (retention_score, tf.float32), (frequency_score, tf.int32), (recency_score, tf.float32), (sends_since_last_open, tf.int32), (campaign_category, tf.string), (discount, tf.int32), (promo, tf.int32), (sale, tf.int32), (is_one_for_free, tf.int32), (free_shipping, tf.int32), (is_exclusive, tf.int32), (has_urgency, tf.int32), (sl_contains_price, tf.int32), (is_discount_mentioned, tf.int32), (sent_week, tf.int32), (sent_dayofweek, tf.int32), (sent_hr, tf.int32)]), tf.int32)>\n",
            "<PrefetchDataset shapes: (OrderedDict([(riid, (None,)), (retention_score, (None,)), (frequency_score, (None,)), (recency_score, (None,)), (sends_since_last_open, (None,)), (campaign_category, (None,)), (discount, (None,)), (promo, (None,)), (sale, (None,)), (is_one_for_free, (None,)), (free_shipping, (None,)), (is_exclusive, (None,)), (has_urgency, (None,)), (sl_contains_price, (None,)), (is_discount_mentioned, (None,)), (sent_week, (None,)), (sent_dayofweek, (None,)), (sent_hr, (None,))]), (None,)), types: (OrderedDict([(riid, tf.int32), (retention_score, tf.float32), (frequency_score, tf.int32), (recency_score, tf.float32), (sends_since_last_open, tf.int32), (campaign_category, tf.string), (discount, tf.int32), (promo, tf.int32), (sale, tf.int32), (is_one_for_free, tf.int32), (free_shipping, tf.int32), (is_exclusive, tf.int32), (has_urgency, tf.int32), (sl_contains_price, tf.int32), (is_discount_mentioned, tf.int32), (sent_week, tf.int32), (sent_dayofweek, tf.int32), (sent_hr, tf.int32)]), tf.int32)>\n",
            "<PrefetchDataset shapes: (OrderedDict([(riid, (None,)), (retention_score, (None,)), (frequency_score, (None,)), (recency_score, (None,)), (sends_since_last_open, (None,)), (campaign_category, (None,)), (discount, (None,)), (promo, (None,)), (sale, (None,)), (is_one_for_free, (None,)), (free_shipping, (None,)), (is_exclusive, (None,)), (has_urgency, (None,)), (sl_contains_price, (None,)), (is_discount_mentioned, (None,)), (sent_week, (None,)), (sent_dayofweek, (None,)), (sent_hr, (None,))]), (None,)), types: (OrderedDict([(riid, tf.int32), (retention_score, tf.float32), (frequency_score, tf.int32), (recency_score, tf.float32), (sends_since_last_open, tf.int32), (campaign_category, tf.string), (discount, tf.int32), (promo, tf.int32), (sale, tf.int32), (is_one_for_free, tf.int32), (free_shipping, tf.int32), (is_exclusive, tf.int32), (has_urgency, tf.int32), (sl_contains_price, tf.int32), (is_discount_mentioned, tf.int32), (sent_week, tf.int32), (sent_dayofweek, tf.int32), (sent_hr, tf.int32)]), tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iU4WWkdVh99"
      },
      "source": [
        "## Base Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oos9W28ePJqH"
      },
      "source": [
        "### Numeric Columns\n",
        "\n",
        "1. 'retention_score'\n",
        "2. 'frequency_score'\n",
        "3. 'recency_score'\n",
        "4. 'sends_since_last_open'\n",
        "5. 'discount'\n",
        "6. 'sent_week'\n",
        "7. 'sent_dayofweek'\n",
        "8. 'sent_hr'\n",
        "\n",
        "To bucketize or not?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFSVOXh0Vs59"
      },
      "source": [
        "#Get the global mean & std statistics\n",
        "rolling_stats_filepath = Path(\"/content/drive/MyDrive/Bandit_Project/rolling_statistics.pkl\")\n",
        "with rolling_stats_filepath.open(mode='rb') as rolling_stats_file:\n",
        "  rolling_stats = pkl.load(rolling_stats_file)\n",
        "\n",
        "#Create a function that standardizes the column to N(0, 1)\n",
        "def standardize_column(data, mean, std):                       \n",
        "  data = (tf.cast(data, dtype=tf.float32) - mean)/std\n",
        "  return tf.reshape(data, [-1, 1])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg29RlFLhLRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6744d2c8-4479-47cc-928c-80ecb3ebb695"
      },
      "source": [
        "#Initialize a list to contain the numeric feature columns\n",
        "numeric_feature_layer = []\n",
        "numeric_feature_layer_input = {}\n",
        "\n",
        "#Create 2 dictionaries with key as numeric feature column name\n",
        "#and val as the value of the numeric feature column name\n",
        "numeric_feature_col_names = ['retention_score','recency_score','frequency_score',\n",
        "                             'sent_week','sent_dayofweek','sent_hr','discount',\n",
        "                             'sends_since_last_open']\n",
        "MEANS = {feature: rolling_stats[feature][\"mean\"] for feature in numeric_feature_col_names}\n",
        "STDS = {feature: rolling_stats[feature][\"std\"] for feature in numeric_feature_col_names}\n",
        "\n",
        "#Generate numeric cols\n",
        "for feature in numeric_feature_col_names:\n",
        "  numeric_feature_col = tf.feature_column.numeric_column(feature, \n",
        "                                                     normalizer_fn=partial(standardize_column, mean=MEANS[feature], std=STDS[feature]))\n",
        "  numeric_feature_layer.append(numeric_feature_col)\n",
        "  numeric_feature_layer_input[feature] = tf.keras.Input(shape=(), name=feature, dtype=tf.float32)\n",
        "\n",
        "#Display the columns\n",
        "print(\"[INFO] The numeric feature columns are:\")\n",
        "pprint(numeric_feature_layer)\n",
        "\n",
        "print(\"\\n[INFO] The inputs to numeric feature columns are:\")\n",
        "pprint(numeric_feature_layer_input)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] The numeric feature columns are:\n",
            "[NumericColumn(key='retention_score', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=11.467980895825553, std=11.35391986430546)),\n",
            " NumericColumn(key='recency_score', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=1.23904221901564, std=2.216794122042123)),\n",
            " NumericColumn(key='frequency_score', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=14.977138288600283, std=20.754428265423773)),\n",
            " NumericColumn(key='sent_week', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=28.594960628048973, std=14.377041994557581)),\n",
            " NumericColumn(key='sent_dayofweek', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=2.638369149867888, std=2.163983074344224)),\n",
            " NumericColumn(key='sent_hr', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=17.291778162586947, std=3.9303658889007997)),\n",
            " NumericColumn(key='discount', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=11.300244227332426, std=19.60056341212265)),\n",
            " NumericColumn(key='sends_since_last_open', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=11.36886785974351, std=14.997835900332081))]\n",
            "\n",
            "[INFO] The inputs to numeric feature columns are:\n",
            "{'discount': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'discount')>,\n",
            " 'frequency_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'frequency_score')>,\n",
            " 'recency_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'recency_score')>,\n",
            " 'retention_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'retention_score')>,\n",
            " 'sends_since_last_open': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sends_since_last_open')>,\n",
            " 'sent_dayofweek': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sent_dayofweek')>,\n",
            " 'sent_hr': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sent_hr')>,\n",
            " 'sent_week': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sent_week')>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK-7fdg8VXLK"
      },
      "source": [
        "### Categorical Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yioe3gpreW65"
      },
      "source": [
        "1. 'riid'\n",
        "2. 'campaign_category'\n",
        "3. 'promo'\n",
        "4. 'sale'\n",
        "5. 'is_one_for_free'\n",
        "6. 'free_shipping'\n",
        "7. 'is_exclusive'\n",
        "8. 'has_urgency'\n",
        "9. 'sl_contains_price'\n",
        "10.'is_discount_mentioned'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASAhKBYMktBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d21b4979-dc8b-41cd-dd8b-603714723d1a"
      },
      "source": [
        "#Initialize a list to contain the categorical feature columns\n",
        "categorical_feature_layer = []\n",
        "categorical_feature_layer_input = {}\n",
        "\n",
        "CATEGORIES = {\n",
        "    'promo' : [0, 1],\n",
        "    'sale' : [0, 1],\n",
        "    'campaign_category': ['Trend', 'NewArrivals', 'Dedicated', 'InnovationSpotlight', 'Core', 'Replen', 'ProductSpotlight', 'Other', 'Brand', 'Tops'],\n",
        "    'is_one_for_free': [0, 1],\n",
        "    'free_shipping': [0, 1],\n",
        "    'is_exclusive': [0, 1],\n",
        "    'has_urgency': [0, 1],\n",
        "    'sl_contains_price': [0, 1],\n",
        "    'is_discount_mentioned': [0, 1],\n",
        "}\n",
        "\n",
        "#Generate categorical cols\n",
        "for (feature, vocab) in CATEGORIES.items():\n",
        "  categorical_feature_col = tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list(feature, vocab))\n",
        "  #categorical_feature_col = tf.feature_column.categorical_column_with_vocabulary_list(feature, vocab)\n",
        "  categorical_feature_layer.append(categorical_feature_col)\n",
        "  if feature == 'campaign_category':\n",
        "    categorical_feature_layer_input[feature] = tf.keras.Input(shape=(), name=feature, dtype=tf.string)\n",
        "  else:\n",
        "    categorical_feature_layer_input[feature] = tf.keras.Input(shape=(), name=feature, dtype=tf.int64)\n",
        "\n",
        "#Display the columns\n",
        "print(\"[INFO] The categorical feature columns are:\")\n",
        "pprint(categorical_feature_layer)\n",
        "\n",
        "print(\"\\n[INFO] The inputs to categorical columns are:\")\n",
        "pprint(categorical_feature_layer_input)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] The categorical feature columns are:\n",
            "[VocabularyListCategoricalColumn(key='promo', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0),\n",
            " VocabularyListCategoricalColumn(key='sale', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0),\n",
            " VocabularyListCategoricalColumn(key='campaign_category', vocabulary_list=('Trend', 'NewArrivals', 'Dedicated', 'InnovationSpotlight', 'Core', 'Replen', 'ProductSpotlight', 'Other', 'Brand', 'Tops'), dtype=tf.string, default_value=-1, num_oov_buckets=0),\n",
            " VocabularyListCategoricalColumn(key='is_one_for_free', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0),\n",
            " VocabularyListCategoricalColumn(key='free_shipping', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0),\n",
            " VocabularyListCategoricalColumn(key='is_exclusive', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0),\n",
            " VocabularyListCategoricalColumn(key='has_urgency', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0),\n",
            " VocabularyListCategoricalColumn(key='sl_contains_price', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0),\n",
            " VocabularyListCategoricalColumn(key='is_discount_mentioned', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0)]\n",
            "\n",
            "[INFO] The inputs to categorical columns are:\n",
            "{'campaign_category': <KerasTensor: shape=(None,) dtype=string (created by layer 'campaign_category')>,\n",
            " 'free_shipping': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'free_shipping')>,\n",
            " 'has_urgency': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'has_urgency')>,\n",
            " 'is_discount_mentioned': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_discount_mentioned')>,\n",
            " 'is_exclusive': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_exclusive')>,\n",
            " 'is_one_for_free': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_one_for_free')>,\n",
            " 'promo': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'promo')>,\n",
            " 'sale': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sale')>,\n",
            " 'sl_contains_price': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sl_contains_price')>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_x2LwfeXHlR",
        "outputId": "6d99bddb-9ba5-45ee-993f-5cab1fc2c440"
      },
      "source": [
        "#Create the riid embedding col\n",
        "embedding_feature_layer = []\n",
        "embedding_feature_layer_input = {}\n",
        "\n",
        "riid = tf.feature_column.categorical_column_with_hash_bucket(\"riid\", hash_bucket_size=2000000, dtype=tf.int64)\n",
        "riid_embedding = tf.feature_column.embedding_column(riid, dimension=32)\n",
        "embedding_feature_layer.append(riid_embedding)\n",
        "embedding_feature_layer_input[\"riid\"] = tf.keras.Input(shape=(), name=\"riid\", dtype=tf.int64)\n",
        "\n",
        "#Display the columns\n",
        "print(\"[INFO] The numeric embedding columns are:\")\n",
        "pprint(embedding_feature_layer)\n",
        "\n",
        "print(\"\\n[INFO] The inputs to embedding feature columns are:\")\n",
        "pprint(embedding_feature_layer_input)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] The numeric embedding columns are:\n",
            "[EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='riid', hash_bucket_size=2000000, dtype=tf.int64), dimension=32, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f141067cd10>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True)]\n",
            "\n",
            "[INFO] The inputs to embedding feature columns are:\n",
            "{'riid': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid')>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xu73TFwVnET"
      },
      "source": [
        "## For Wide Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlFehJ-ZVbbn"
      },
      "source": [
        "### Crossed Columns\n",
        "\n",
        "Crossing the following feature-combinations:\n",
        "\n",
        "1.   'riid' vs. 'campaign_category'\n",
        "2.   'riid' vs. 'discount'\n",
        "3.   'riid' vs. 'is_one_for_free'\n",
        "4.   'riid' vs. 'free_shipping'\n",
        "5.   'riid' vs. 'is_exclusive'\n",
        "6.   'riid' vs. 'has_urgency',\n",
        "7.   'riid' vs. 'sl_contains_price',\n",
        "8. 'riid' vs. 'is_discount_mentioned',\n",
        "9. 'riid' vs. 'sent_week',\n",
        "10. 'riid' vs. 'sent_dayofweek',\n",
        "11. 'riid' vs. 'sent_hr'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y527yMOStXRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddc933f7-2bb5-4ff4-b9d6-feac12a28531"
      },
      "source": [
        "crossed_columns = [\n",
        "  tf.feature_column.crossed_column([\"riid\", 'campaign_category'], hash_bucket_size=20000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'discount'], hash_bucket_size=10000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'is_one_for_free'], hash_bucket_size=4000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'free_shipping'], hash_bucket_size=4000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'is_exclusive'], hash_bucket_size=4000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'has_urgency'], hash_bucket_size=4000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'sl_contains_price'], hash_bucket_size=4000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'is_discount_mentioned'], hash_bucket_size=4000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'sent_week'], hash_bucket_size=10000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'sent_dayofweek'], hash_bucket_size=60000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'sent_hr'], hash_bucket_size=50000000),\n",
        "]\n",
        "crossed_columns_names = [\"riid_X_campaign_category\",\n",
        "                         \"riid_X_discount\",\n",
        "                         \"riid_X_is_one_for_free\",\n",
        "                         \"riid_X_free_shipping\",\n",
        "                         \"riid_X_is_exclusive\",\n",
        "                         \"riid_X_has_urgency\",\n",
        "                         \"riid_X_sl_contains_price\",\n",
        "                         \"riid_X_is_discount_mentioned\",\n",
        "                         \"riid_X_sent_week\",\n",
        "                         \"riid_X_sent_dayofweek\",\n",
        "                         \"riid_X_sent_hr\"]\n",
        "crossed_columns_input = {colname: tf.keras.Input(shape=(), name=colname, dtype=tf.int64)\n",
        "                                    for colname in crossed_columns_names}\n",
        "#Display the columns\n",
        "print(\"[INFO] The crossed feature columns are:\")\n",
        "pprint(crossed_columns)\n",
        "\n",
        "print(\"\\n[INFO] The inputs to crossed feature columns are:\")\n",
        "pprint(crossed_columns_input)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] The crossed feature columns are:\n",
            "[CrossedColumn(keys=('riid', 'campaign_category'), hash_bucket_size=20000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'discount'), hash_bucket_size=10000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'is_one_for_free'), hash_bucket_size=4000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'free_shipping'), hash_bucket_size=4000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'is_exclusive'), hash_bucket_size=4000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'has_urgency'), hash_bucket_size=4000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'sl_contains_price'), hash_bucket_size=4000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'is_discount_mentioned'), hash_bucket_size=4000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'sent_week'), hash_bucket_size=10000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'sent_dayofweek'), hash_bucket_size=60000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'sent_hr'), hash_bucket_size=50000000, hash_key=None)]\n",
            "\n",
            "[INFO] The inputs to crossed feature columns are:\n",
            "{'riid_X_campaign_category': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_campaign_category')>,\n",
            " 'riid_X_discount': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_discount')>,\n",
            " 'riid_X_free_shipping': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_free_shipping')>,\n",
            " 'riid_X_has_urgency': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_has_urgency')>,\n",
            " 'riid_X_is_discount_mentioned': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_is_discount_mentioned')>,\n",
            " 'riid_X_is_exclusive': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_is_exclusive')>,\n",
            " 'riid_X_is_one_for_free': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_is_one_for_free')>,\n",
            " 'riid_X_sent_dayofweek': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_sent_dayofweek')>,\n",
            " 'riid_X_sent_hr': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_sent_hr')>,\n",
            " 'riid_X_sent_week': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_sent_week')>,\n",
            " 'riid_X_sl_contains_price': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_sl_contains_price')>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BztuFL9GXnci",
        "outputId": "6c4086e3-1ffc-4f41-a4be-2a50b2636d19"
      },
      "source": [
        "wide_columns = numeric_feature_layer + crossed_columns\n",
        "wide_columns_input = {**numeric_feature_layer_input, **crossed_columns_input}\n",
        "\n",
        "#Display the columns\n",
        "print(\"[INFO] The wide columns are:\")\n",
        "pprint(wide_columns)\n",
        "\n",
        "print(\"\\n[INFO] The inputs to wide columns are:\")\n",
        "pprint(wide_columns_input)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] The wide columns are:\n",
            "[NumericColumn(key='retention_score', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=11.467980895825553, std=11.35391986430546)),\n",
            " NumericColumn(key='recency_score', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=1.23904221901564, std=2.216794122042123)),\n",
            " NumericColumn(key='frequency_score', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=14.977138288600283, std=20.754428265423773)),\n",
            " NumericColumn(key='sent_week', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=28.594960628048973, std=14.377041994557581)),\n",
            " NumericColumn(key='sent_dayofweek', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=2.638369149867888, std=2.163983074344224)),\n",
            " NumericColumn(key='sent_hr', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=17.291778162586947, std=3.9303658889007997)),\n",
            " NumericColumn(key='discount', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=11.300244227332426, std=19.60056341212265)),\n",
            " NumericColumn(key='sends_since_last_open', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=11.36886785974351, std=14.997835900332081)),\n",
            " CrossedColumn(keys=('riid', 'campaign_category'), hash_bucket_size=20000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'discount'), hash_bucket_size=10000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'is_one_for_free'), hash_bucket_size=4000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'free_shipping'), hash_bucket_size=4000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'is_exclusive'), hash_bucket_size=4000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'has_urgency'), hash_bucket_size=4000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'sl_contains_price'), hash_bucket_size=4000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'is_discount_mentioned'), hash_bucket_size=4000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'sent_week'), hash_bucket_size=10000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'sent_dayofweek'), hash_bucket_size=60000000, hash_key=None),\n",
            " CrossedColumn(keys=('riid', 'sent_hr'), hash_bucket_size=50000000, hash_key=None)]\n",
            "\n",
            "[INFO] The inputs to wide columns are:\n",
            "{'discount': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'discount')>,\n",
            " 'frequency_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'frequency_score')>,\n",
            " 'recency_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'recency_score')>,\n",
            " 'retention_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'retention_score')>,\n",
            " 'riid_X_campaign_category': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_campaign_category')>,\n",
            " 'riid_X_discount': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_discount')>,\n",
            " 'riid_X_free_shipping': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_free_shipping')>,\n",
            " 'riid_X_has_urgency': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_has_urgency')>,\n",
            " 'riid_X_is_discount_mentioned': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_is_discount_mentioned')>,\n",
            " 'riid_X_is_exclusive': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_is_exclusive')>,\n",
            " 'riid_X_is_one_for_free': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_is_one_for_free')>,\n",
            " 'riid_X_sent_dayofweek': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_sent_dayofweek')>,\n",
            " 'riid_X_sent_hr': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_sent_hr')>,\n",
            " 'riid_X_sent_week': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_sent_week')>,\n",
            " 'riid_X_sl_contains_price': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid_X_sl_contains_price')>,\n",
            " 'sends_since_last_open': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sends_since_last_open')>,\n",
            " 'sent_dayofweek': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sent_dayofweek')>,\n",
            " 'sent_hr': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sent_hr')>,\n",
            " 'sent_week': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sent_week')>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOwuJPTiVplF"
      },
      "source": [
        "## For Deep Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szxxbbWfvMaC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efc9e062-b6bf-4ed9-dc9c-824188b04a38"
      },
      "source": [
        "deep_columns = numeric_feature_layer + categorical_feature_layer + embedding_feature_layer\n",
        "deep_columns_input = {**numeric_feature_layer_input, **categorical_feature_layer_input, **embedding_feature_layer_input}\n",
        "\n",
        "#Display the columns\n",
        "print(\"[INFO] The deep feature columns are:\")\n",
        "pprint(deep_columns)\n",
        "\n",
        "print(\"\\n[INFO] The inputs to deep feature columns are:\")\n",
        "pprint(deep_columns_input)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] The deep feature columns are:\n",
            "[NumericColumn(key='retention_score', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=11.467980895825553, std=11.35391986430546)),\n",
            " NumericColumn(key='recency_score', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=1.23904221901564, std=2.216794122042123)),\n",
            " NumericColumn(key='frequency_score', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=14.977138288600283, std=20.754428265423773)),\n",
            " NumericColumn(key='sent_week', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=28.594960628048973, std=14.377041994557581)),\n",
            " NumericColumn(key='sent_dayofweek', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=2.638369149867888, std=2.163983074344224)),\n",
            " NumericColumn(key='sent_hr', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=17.291778162586947, std=3.9303658889007997)),\n",
            " NumericColumn(key='discount', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=11.300244227332426, std=19.60056341212265)),\n",
            " NumericColumn(key='sends_since_last_open', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=11.36886785974351, std=14.997835900332081)),\n",
            " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='promo', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0)),\n",
            " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='sale', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0)),\n",
            " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='campaign_category', vocabulary_list=('Trend', 'NewArrivals', 'Dedicated', 'InnovationSpotlight', 'Core', 'Replen', 'ProductSpotlight', 'Other', 'Brand', 'Tops'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
            " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='is_one_for_free', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0)),\n",
            " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='free_shipping', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0)),\n",
            " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='is_exclusive', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0)),\n",
            " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='has_urgency', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0)),\n",
            " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='sl_contains_price', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0)),\n",
            " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='is_discount_mentioned', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0)),\n",
            " EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='riid', hash_bucket_size=2000000, dtype=tf.int64), dimension=32, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f141067cd10>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True)]\n",
            "\n",
            "[INFO] The inputs to deep feature columns are:\n",
            "{'campaign_category': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'campaign_category')>,\n",
            " 'discount': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'discount')>,\n",
            " 'free_shipping': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'free_shipping')>,\n",
            " 'frequency_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'frequency_score')>,\n",
            " 'has_urgency': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'has_urgency')>,\n",
            " 'is_discount_mentioned': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_discount_mentioned')>,\n",
            " 'is_exclusive': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_exclusive')>,\n",
            " 'is_one_for_free': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_one_for_free')>,\n",
            " 'promo': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'promo')>,\n",
            " 'recency_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'recency_score')>,\n",
            " 'retention_score': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'retention_score')>,\n",
            " 'riid': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'riid')>,\n",
            " 'sale': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sale')>,\n",
            " 'sends_since_last_open': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sends_since_last_open')>,\n",
            " 'sent_dayofweek': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sent_dayofweek')>,\n",
            " 'sent_hr': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sent_hr')>,\n",
            " 'sent_week': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sent_week')>,\n",
            " 'sl_contains_price': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'sl_contains_price')>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whQg0MKDyqZ-"
      },
      "source": [
        "# Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQWxDAhCyZG1"
      },
      "source": [
        "models_dir = Path(\"/content/drive/MyDrive/Bandit_Project/models\")\n",
        "wmodel = (models_dir/\"Wide\").mkdir(exist_ok=True)\n",
        "dmodel = (models_dir/\"Deep\").mkdir(exist_ok=True)\n",
        "wdmodel = (models_dir/\"W&D\").mkdir(exist_ok=True)\n",
        "bwdmodel = (models_dir/\"Bayesian W&D\").mkdir(exist_ok=True)\n",
        "\n",
        "#Hyperparameters\n",
        "lr = 1e-3\n",
        "n_steps = 1500000"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjxXAMxOyx-t"
      },
      "source": [
        "## Simple Wide Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6ukiLoh_xh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f4840f7-0a67-483b-d484-909037da8897"
      },
      "source": [
        "wm = tf.estimator.LinearClassifier(\n",
        "    model_dir=wmodel, \n",
        "    feature_columns=wide_columns,\n",
        "    n_classes=2,\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate = lr))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpct35zelt\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpct35zelt', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHoaO7tZ6Ryl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a224cb5d-3cd4-44d5-9378-16f865e1278d"
      },
      "source": [
        "wm.train(train_dl_train, steps=n_steps)\n",
        "wm_results = wm.evaluate(test_dl_train, steps=None)\n",
        "for key in sorted(wm_results):\n",
        "  print(\"%s: %s\" % (key, wm_results[key]))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpct35zelt/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
            "INFO:tensorflow:loss = 0.6931472, step = 0\n",
            "INFO:tensorflow:global_step/sec: 9.17988\n",
            "INFO:tensorflow:loss = 0.56775856, step = 100 (10.896 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52557\n",
            "INFO:tensorflow:loss = 0.4798948, step = 200 (10.500 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.41919\n",
            "INFO:tensorflow:loss = 0.43979174, step = 300 (10.617 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.41907\n",
            "INFO:tensorflow:loss = 0.47828147, step = 400 (10.615 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.48341\n",
            "INFO:tensorflow:loss = 0.4324661, step = 500 (10.547 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.46936\n",
            "INFO:tensorflow:loss = 0.41626912, step = 600 (10.560 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.45005\n",
            "INFO:tensorflow:loss = 0.40145394, step = 700 (10.581 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.45294\n",
            "INFO:tensorflow:loss = 0.39401987, step = 800 (10.577 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.48944\n",
            "INFO:tensorflow:loss = 0.38942423, step = 900 (10.541 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.44479\n",
            "INFO:tensorflow:loss = 0.3829737, step = 1000 (10.587 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.4972\n",
            "INFO:tensorflow:loss = 0.3790929, step = 1100 (10.531 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.47396\n",
            "INFO:tensorflow:loss = 0.3302543, step = 1200 (10.555 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51515\n",
            "INFO:tensorflow:loss = 0.3481812, step = 1300 (10.506 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52576\n",
            "INFO:tensorflow:loss = 0.3914228, step = 1400 (10.499 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.46307\n",
            "INFO:tensorflow:loss = 0.32610047, step = 1500 (10.569 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.48598\n",
            "INFO:tensorflow:loss = 0.32949966, step = 1600 (10.541 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52044\n",
            "INFO:tensorflow:loss = 0.33363512, step = 1700 (10.505 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52473\n",
            "INFO:tensorflow:loss = 0.3296168, step = 1800 (10.498 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52136\n",
            "INFO:tensorflow:loss = 0.32248306, step = 1900 (10.503 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52962\n",
            "INFO:tensorflow:loss = 0.32283387, step = 2000 (10.493 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.49013\n",
            "INFO:tensorflow:loss = 0.3188834, step = 2100 (10.538 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52614\n",
            "INFO:tensorflow:loss = 0.31105897, step = 2200 (10.496 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53136\n",
            "INFO:tensorflow:loss = 0.30255556, step = 2300 (10.493 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.49076\n",
            "INFO:tensorflow:loss = 0.27161834, step = 2400 (10.537 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.45946\n",
            "INFO:tensorflow:loss = 0.3000354, step = 2500 (10.571 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51776\n",
            "INFO:tensorflow:loss = 0.29918563, step = 2600 (10.507 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52722\n",
            "INFO:tensorflow:loss = 0.31665826, step = 2700 (10.494 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52292\n",
            "INFO:tensorflow:loss = 0.26095927, step = 2800 (10.501 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51869\n",
            "INFO:tensorflow:loss = 0.2351628, step = 2900 (10.504 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.49972\n",
            "INFO:tensorflow:loss = 0.24289683, step = 3000 (10.530 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52563\n",
            "INFO:tensorflow:loss = 0.2658563, step = 3100 (10.496 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55183\n",
            "INFO:tensorflow:loss = 0.27691424, step = 3200 (10.468 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.50859\n",
            "INFO:tensorflow:loss = 0.26782593, step = 3300 (10.517 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.50725\n",
            "INFO:tensorflow:loss = 0.24948478, step = 3400 (10.520 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51659\n",
            "INFO:tensorflow:loss = 0.27864414, step = 3500 (10.506 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52485\n",
            "INFO:tensorflow:loss = 0.2617838, step = 3600 (10.499 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.49301\n",
            "INFO:tensorflow:loss = 0.25019032, step = 3700 (10.536 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.38381\n",
            "INFO:tensorflow:loss = 0.23695552, step = 3800 (10.657 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.43853\n",
            "INFO:tensorflow:loss = 0.2301905, step = 3900 (10.595 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.37033\n",
            "INFO:tensorflow:loss = 0.22471331, step = 4000 (10.670 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.45517\n",
            "INFO:tensorflow:loss = 0.22581305, step = 4100 (10.579 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.42511\n",
            "INFO:tensorflow:loss = 0.24018201, step = 4200 (10.608 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52014\n",
            "INFO:tensorflow:loss = 0.2112997, step = 4300 (10.506 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53299\n",
            "INFO:tensorflow:loss = 0.21917593, step = 4400 (10.490 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.48451\n",
            "INFO:tensorflow:loss = 0.22808701, step = 4500 (10.545 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.49373\n",
            "INFO:tensorflow:loss = 0.2230061, step = 4600 (10.533 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.46276\n",
            "INFO:tensorflow:loss = 0.23125806, step = 4700 (10.565 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.42604\n",
            "INFO:tensorflow:loss = 0.25092825, step = 4800 (10.608 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.46991\n",
            "INFO:tensorflow:loss = 0.18792179, step = 4900 (10.563 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.46403\n",
            "INFO:tensorflow:loss = 0.18396293, step = 5000 (10.563 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.48853\n",
            "INFO:tensorflow:loss = 0.18707061, step = 5100 (10.539 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.50489\n",
            "INFO:tensorflow:loss = 0.23298462, step = 5200 (10.522 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.48935\n",
            "INFO:tensorflow:loss = 0.2130503, step = 5300 (10.536 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.47117\n",
            "INFO:tensorflow:loss = 0.20767653, step = 5400 (10.561 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.48409\n",
            "INFO:tensorflow:loss = 0.19952533, step = 5500 (10.541 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.4845\n",
            "INFO:tensorflow:loss = 0.21827188, step = 5600 (10.543 sec)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 5684...\n",
            "INFO:tensorflow:Saving checkpoints for 5684 into /tmp/tmpct35zelt/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 5684...\n",
            "INFO:tensorflow:global_step/sec: 1.47018\n",
            "INFO:tensorflow:loss = 0.19548449, step = 5700 (68.021 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.47743\n",
            "INFO:tensorflow:loss = 0.19066021, step = 5800 (10.551 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.48308\n",
            "INFO:tensorflow:loss = 0.15783033, step = 5900 (10.546 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.48043\n",
            "INFO:tensorflow:loss = 0.1659426, step = 6000 (10.548 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51635\n",
            "INFO:tensorflow:loss = 0.16774057, step = 6100 (10.508 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.4566\n",
            "INFO:tensorflow:loss = 0.16211714, step = 6200 (10.572 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.42862\n",
            "INFO:tensorflow:loss = 0.18875563, step = 6300 (10.609 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.45695\n",
            "INFO:tensorflow:loss = 0.1616275, step = 6400 (10.571 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.40634\n",
            "INFO:tensorflow:loss = 0.16588978, step = 6500 (10.634 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.43937\n",
            "INFO:tensorflow:loss = 0.18609612, step = 6600 (10.593 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.50261\n",
            "INFO:tensorflow:loss = 0.17594782, step = 6700 (10.522 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53479\n",
            "INFO:tensorflow:loss = 0.18179218, step = 6800 (10.488 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.46622\n",
            "INFO:tensorflow:loss = 0.12994456, step = 6900 (10.564 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54598\n",
            "INFO:tensorflow:loss = 0.1327176, step = 7000 (10.477 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51525\n",
            "INFO:tensorflow:loss = 0.13717486, step = 7100 (10.508 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51219\n",
            "INFO:tensorflow:loss = 0.13334031, step = 7200 (10.516 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53765\n",
            "INFO:tensorflow:loss = 0.18294105, step = 7300 (10.482 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51773\n",
            "INFO:tensorflow:loss = 0.16091035, step = 7400 (10.509 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52806\n",
            "INFO:tensorflow:loss = 0.16287422, step = 7500 (10.491 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52255\n",
            "INFO:tensorflow:loss = 0.14443117, step = 7600 (10.503 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53562\n",
            "INFO:tensorflow:loss = 0.16414355, step = 7700 (10.489 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.50665\n",
            "INFO:tensorflow:loss = 0.14869265, step = 7800 (10.515 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53475\n",
            "INFO:tensorflow:loss = 0.15504545, step = 7900 (10.490 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.50984\n",
            "INFO:tensorflow:loss = 0.10920585, step = 8000 (10.518 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.4989\n",
            "INFO:tensorflow:loss = 0.11586198, step = 8100 (10.526 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51855\n",
            "INFO:tensorflow:loss = 0.107729524, step = 8200 (10.507 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54384\n",
            "INFO:tensorflow:loss = 0.11077592, step = 8300 (10.477 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.50611\n",
            "INFO:tensorflow:loss = 0.15992913, step = 8400 (10.519 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55668\n",
            "INFO:tensorflow:loss = 0.13113233, step = 8500 (10.462 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52875\n",
            "INFO:tensorflow:loss = 0.12707025, step = 8600 (10.498 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53781\n",
            "INFO:tensorflow:loss = 0.14210509, step = 8700 (10.485 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54277\n",
            "INFO:tensorflow:loss = 0.12695248, step = 8800 (10.476 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.50981\n",
            "INFO:tensorflow:loss = 0.1323193, step = 8900 (10.519 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.50346\n",
            "INFO:tensorflow:loss = 0.091539145, step = 9000 (10.519 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55531\n",
            "INFO:tensorflow:loss = 0.089718655, step = 9100 (10.468 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.49288\n",
            "INFO:tensorflow:loss = 0.09242821, step = 9200 (10.534 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52083\n",
            "INFO:tensorflow:loss = 0.09535067, step = 9300 (10.505 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.50062\n",
            "INFO:tensorflow:loss = 0.14488152, step = 9400 (10.526 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51918\n",
            "INFO:tensorflow:loss = 0.13009049, step = 9500 (10.506 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51565\n",
            "INFO:tensorflow:loss = 0.13240555, step = 9600 (10.507 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51552\n",
            "INFO:tensorflow:loss = 0.13130692, step = 9700 (10.507 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51862\n",
            "INFO:tensorflow:loss = 0.13010322, step = 9800 (10.507 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.5436\n",
            "INFO:tensorflow:loss = 0.1266016, step = 9900 (10.479 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51676\n",
            "INFO:tensorflow:loss = 0.11608347, step = 10000 (10.505 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54164\n",
            "INFO:tensorflow:loss = 0.07583195, step = 10100 (10.482 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54464\n",
            "INFO:tensorflow:loss = 0.0786736, step = 10200 (10.478 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.5508\n",
            "INFO:tensorflow:loss = 0.08224085, step = 10300 (10.470 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53695\n",
            "INFO:tensorflow:loss = 0.12855592, step = 10400 (10.488 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55951\n",
            "INFO:tensorflow:loss = 0.113945395, step = 10500 (10.462 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.4989\n",
            "INFO:tensorflow:loss = 0.1018836, step = 10600 (10.525 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51341\n",
            "INFO:tensorflow:loss = 0.10437548, step = 10700 (10.513 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54978\n",
            "INFO:tensorflow:loss = 0.113591716, step = 10800 (10.472 sec)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 10845...\n",
            "INFO:tensorflow:Saving checkpoints for 10845 into /tmp/tmpct35zelt/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 10845...\n",
            "INFO:tensorflow:global_step/sec: 3.00264\n",
            "INFO:tensorflow:loss = 0.11966136, step = 10900 (33.300 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53009\n",
            "INFO:tensorflow:loss = 0.12128566, step = 11000 (10.493 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51261\n",
            "INFO:tensorflow:loss = 0.07027398, step = 11100 (10.512 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.56324\n",
            "INFO:tensorflow:loss = 0.06417068, step = 11200 (10.460 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53861\n",
            "INFO:tensorflow:loss = 0.06669068, step = 11300 (10.483 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53025\n",
            "INFO:tensorflow:loss = 0.061398357, step = 11400 (10.493 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52808\n",
            "INFO:tensorflow:loss = 0.10300412, step = 11500 (10.493 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53648\n",
            "INFO:tensorflow:loss = 0.10563506, step = 11600 (10.487 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52588\n",
            "INFO:tensorflow:loss = 0.11300895, step = 11700 (10.497 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54118\n",
            "INFO:tensorflow:loss = 0.10764778, step = 11800 (10.481 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53361\n",
            "INFO:tensorflow:loss = 0.12042035, step = 11900 (10.492 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54814\n",
            "INFO:tensorflow:loss = 0.09991307, step = 12000 (10.470 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.45854\n",
            "INFO:tensorflow:loss = 0.059246764, step = 12100 (10.574 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53505\n",
            "INFO:tensorflow:loss = 0.05931518, step = 12200 (10.488 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54999\n",
            "INFO:tensorflow:loss = 0.05306241, step = 12300 (10.474 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54571\n",
            "INFO:tensorflow:loss = 0.054271564, step = 12400 (10.471 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52841\n",
            "INFO:tensorflow:loss = 0.09614095, step = 12500 (10.497 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53686\n",
            "INFO:tensorflow:loss = 0.099026054, step = 12600 (10.484 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54015\n",
            "INFO:tensorflow:loss = 0.10255939, step = 12700 (10.484 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53328\n",
            "INFO:tensorflow:loss = 0.09642366, step = 12800 (10.487 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52213\n",
            "INFO:tensorflow:loss = 0.10415769, step = 12900 (10.504 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.5295\n",
            "INFO:tensorflow:loss = 0.09808166, step = 13000 (10.492 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.4904\n",
            "INFO:tensorflow:loss = 0.08822589, step = 13100 (10.539 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52187\n",
            "INFO:tensorflow:loss = 0.046369866, step = 13200 (10.501 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.4892\n",
            "INFO:tensorflow:loss = 0.04983796, step = 13300 (10.538 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.47639\n",
            "INFO:tensorflow:loss = 0.047615424, step = 13400 (10.553 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51061\n",
            "INFO:tensorflow:loss = 0.044827346, step = 13500 (10.514 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.49519\n",
            "INFO:tensorflow:loss = 0.09106871, step = 13600 (10.534 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54562\n",
            "INFO:tensorflow:loss = 0.08902196, step = 13700 (10.475 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53882\n",
            "INFO:tensorflow:loss = 0.07611927, step = 13800 (10.482 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53028\n",
            "INFO:tensorflow:loss = 0.09378283, step = 13900 (10.496 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52531\n",
            "INFO:tensorflow:loss = 0.084176786, step = 14000 (10.497 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52138\n",
            "INFO:tensorflow:loss = 0.0874453, step = 14100 (10.503 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53375\n",
            "INFO:tensorflow:loss = 0.042139158, step = 14200 (10.486 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54499\n",
            "INFO:tensorflow:loss = 0.041801617, step = 14300 (10.477 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54856\n",
            "INFO:tensorflow:loss = 0.038936246, step = 14400 (10.476 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52048\n",
            "INFO:tensorflow:loss = 0.03956219, step = 14500 (10.503 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54267\n",
            "INFO:tensorflow:loss = 0.09066816, step = 14600 (10.479 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.49479\n",
            "INFO:tensorflow:loss = 0.09557696, step = 14700 (10.534 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53248\n",
            "INFO:tensorflow:loss = 0.08645091, step = 14800 (10.487 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51778\n",
            "INFO:tensorflow:loss = 0.080157, step = 14900 (10.509 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.5373\n",
            "INFO:tensorflow:loss = 0.0818819, step = 15000 (10.487 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.56022\n",
            "INFO:tensorflow:loss = 0.07920376, step = 15100 (10.458 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54799\n",
            "INFO:tensorflow:loss = 0.054400045, step = 15200 (10.472 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52924\n",
            "INFO:tensorflow:loss = 0.03308146, step = 15300 (10.494 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51537\n",
            "INFO:tensorflow:loss = 0.033813305, step = 15400 (10.511 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51327\n",
            "INFO:tensorflow:loss = 0.0348216, step = 15500 (10.512 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.50265\n",
            "INFO:tensorflow:loss = 0.08708456, step = 15600 (10.523 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54632\n",
            "INFO:tensorflow:loss = 0.08019724, step = 15700 (10.474 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53409\n",
            "INFO:tensorflow:loss = 0.077412665, step = 15800 (10.487 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.48174\n",
            "INFO:tensorflow:loss = 0.079042576, step = 15900 (10.550 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52767\n",
            "INFO:tensorflow:loss = 0.067136206, step = 16000 (10.492 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.45467\n",
            "INFO:tensorflow:loss = 0.087538466, step = 16100 (10.578 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.49833\n",
            "INFO:tensorflow:loss = 0.074771844, step = 16200 (10.528 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.45228\n",
            "INFO:tensorflow:loss = 0.02863018, step = 16300 (10.581 sec)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 16342...\n",
            "INFO:tensorflow:Saving checkpoints for 16342 into /tmp/tmpct35zelt/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 16342...\n",
            "INFO:tensorflow:global_step/sec: 3.06965\n",
            "INFO:tensorflow:loss = 0.029965319, step = 16400 (32.576 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.50214\n",
            "INFO:tensorflow:loss = 0.02840677, step = 16500 (10.525 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54204\n",
            "INFO:tensorflow:loss = 0.028665245, step = 16600 (10.479 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.50912\n",
            "INFO:tensorflow:loss = 0.08138119, step = 16700 (10.518 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51145\n",
            "INFO:tensorflow:loss = 0.082707234, step = 16800 (10.513 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54873\n",
            "INFO:tensorflow:loss = 0.07173612, step = 16900 (10.472 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55121\n",
            "INFO:tensorflow:loss = 0.07818409, step = 17000 (10.470 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51758\n",
            "INFO:tensorflow:loss = 0.07334139, step = 17100 (10.509 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54139\n",
            "INFO:tensorflow:loss = 0.07737318, step = 17200 (10.477 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52988\n",
            "INFO:tensorflow:loss = 0.02490658, step = 17300 (10.495 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55811\n",
            "INFO:tensorflow:loss = 0.024965042, step = 17400 (10.462 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.5261\n",
            "INFO:tensorflow:loss = 0.023492241, step = 17500 (10.498 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51675\n",
            "INFO:tensorflow:loss = 0.024458133, step = 17600 (10.510 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.56824\n",
            "INFO:tensorflow:loss = 0.06326469, step = 17700 (10.451 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52676\n",
            "INFO:tensorflow:loss = 0.071514994, step = 17800 (10.499 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54312\n",
            "INFO:tensorflow:loss = 0.06784141, step = 17900 (10.474 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.48584\n",
            "INFO:tensorflow:loss = 0.07176431, step = 18000 (10.542 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.50706\n",
            "INFO:tensorflow:loss = 0.064794466, step = 18100 (10.518 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51459\n",
            "INFO:tensorflow:loss = 0.061431557, step = 18200 (10.511 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53454\n",
            "INFO:tensorflow:loss = 0.06639259, step = 18300 (10.491 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54776\n",
            "INFO:tensorflow:loss = 0.02303245, step = 18400 (10.471 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.48158\n",
            "INFO:tensorflow:loss = 0.020854088, step = 18500 (10.548 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53571\n",
            "INFO:tensorflow:loss = 0.021526078, step = 18600 (10.487 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52472\n",
            "INFO:tensorflow:loss = 0.07053002, step = 18700 (10.500 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.564\n",
            "INFO:tensorflow:loss = 0.06756252, step = 18800 (10.455 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54876\n",
            "INFO:tensorflow:loss = 0.064630166, step = 18900 (10.470 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.48634\n",
            "INFO:tensorflow:loss = 0.064843774, step = 19000 (10.543 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.50802\n",
            "INFO:tensorflow:loss = 0.0632112, step = 19100 (10.517 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55945\n",
            "INFO:tensorflow:loss = 0.074064784, step = 19200 (10.463 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55166\n",
            "INFO:tensorflow:loss = 0.067400634, step = 19300 (10.467 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54537\n",
            "INFO:tensorflow:loss = 0.017884647, step = 19400 (10.477 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53096\n",
            "INFO:tensorflow:loss = 0.017936163, step = 19500 (10.490 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54135\n",
            "INFO:tensorflow:loss = 0.018044347, step = 19600 (10.481 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55579\n",
            "INFO:tensorflow:loss = 0.01717221, step = 19700 (10.466 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54584\n",
            "INFO:tensorflow:loss = 0.05181358, step = 19800 (10.474 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53897\n",
            "INFO:tensorflow:loss = 0.062446445, step = 19900 (10.483 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55317\n",
            "INFO:tensorflow:loss = 0.055424992, step = 20000 (10.468 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54602\n",
            "INFO:tensorflow:loss = 0.06534804, step = 20100 (10.476 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54962\n",
            "INFO:tensorflow:loss = 0.06803773, step = 20200 (10.474 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54078\n",
            "INFO:tensorflow:loss = 0.06372324, step = 20300 (10.481 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53902\n",
            "INFO:tensorflow:loss = 0.023739964, step = 20400 (10.485 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54784\n",
            "INFO:tensorflow:loss = 0.016318642, step = 20500 (10.471 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54836\n",
            "INFO:tensorflow:loss = 0.016404284, step = 20600 (10.471 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.46904\n",
            "INFO:tensorflow:loss = 0.015655957, step = 20700 (10.563 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52517\n",
            "INFO:tensorflow:loss = 0.053918753, step = 20800 (10.497 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.57309\n",
            "INFO:tensorflow:loss = 0.057087794, step = 20900 (10.448 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53676\n",
            "INFO:tensorflow:loss = 0.05176111, step = 21000 (10.483 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54571\n",
            "INFO:tensorflow:loss = 0.049124256, step = 21100 (10.477 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.602\n",
            "INFO:tensorflow:loss = 0.05452036, step = 21200 (10.413 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54142\n",
            "INFO:tensorflow:loss = 0.054482527, step = 21300 (10.484 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.58433\n",
            "INFO:tensorflow:loss = 0.055157833, step = 21400 (10.432 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.56233\n",
            "INFO:tensorflow:loss = 0.016473433, step = 21500 (10.459 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55083\n",
            "INFO:tensorflow:loss = 0.015103004, step = 21600 (10.468 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.57441\n",
            "INFO:tensorflow:loss = 0.013566047, step = 21700 (10.446 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54687\n",
            "INFO:tensorflow:loss = 0.014245827, step = 21800 (10.475 sec)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 21855...\n",
            "INFO:tensorflow:Saving checkpoints for 21855 into /tmp/tmpct35zelt/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 21855...\n",
            "INFO:tensorflow:global_step/sec: 3.07108\n",
            "INFO:tensorflow:loss = 0.05127943, step = 21900 (32.561 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.5589\n",
            "INFO:tensorflow:loss = 0.053891428, step = 22000 (10.464 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52791\n",
            "INFO:tensorflow:loss = 0.046933595, step = 22100 (10.495 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51934\n",
            "INFO:tensorflow:loss = 0.048912253, step = 22200 (10.501 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54732\n",
            "INFO:tensorflow:loss = 0.05415868, step = 22300 (10.476 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.5373\n",
            "INFO:tensorflow:loss = 0.046577994, step = 22400 (10.486 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55571\n",
            "INFO:tensorflow:loss = 0.01305033, step = 22500 (10.465 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55429\n",
            "INFO:tensorflow:loss = 0.012764441, step = 22600 (10.465 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.5599\n",
            "INFO:tensorflow:loss = 0.011360114, step = 22700 (10.463 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.56076\n",
            "INFO:tensorflow:loss = 0.01187447, step = 22800 (10.459 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53452\n",
            "INFO:tensorflow:loss = 0.044717714, step = 22900 (10.487 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55426\n",
            "INFO:tensorflow:loss = 0.04585816, step = 23000 (10.469 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55714\n",
            "INFO:tensorflow:loss = 0.049243398, step = 23100 (10.460 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52774\n",
            "INFO:tensorflow:loss = 0.052317373, step = 23200 (10.499 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.56939\n",
            "INFO:tensorflow:loss = 0.050869204, step = 23300 (10.451 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.5289\n",
            "INFO:tensorflow:loss = 0.05711635, step = 23400 (10.491 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52074\n",
            "INFO:tensorflow:loss = 0.04217651, step = 23500 (10.505 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.47737\n",
            "INFO:tensorflow:loss = 0.009982868, step = 23600 (10.552 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53413\n",
            "INFO:tensorflow:loss = 0.011725452, step = 23700 (10.486 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53854\n",
            "INFO:tensorflow:loss = 0.00961862, step = 23800 (10.485 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52376\n",
            "INFO:tensorflow:loss = 0.051484767, step = 23900 (10.501 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55526\n",
            "INFO:tensorflow:loss = 0.050163988, step = 24000 (10.465 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55579\n",
            "INFO:tensorflow:loss = 0.04639195, step = 24100 (10.463 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51469\n",
            "INFO:tensorflow:loss = 0.044708077, step = 24200 (10.513 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53137\n",
            "INFO:tensorflow:loss = 0.04039691, step = 24300 (10.493 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55569\n",
            "INFO:tensorflow:loss = 0.047506656, step = 24400 (10.462 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.53762\n",
            "INFO:tensorflow:loss = 0.043681134, step = 24500 (10.483 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.51207\n",
            "INFO:tensorflow:loss = 0.009323865, step = 24600 (10.514 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54881\n",
            "INFO:tensorflow:loss = 0.008969232, step = 24700 (10.475 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.43599\n",
            "INFO:tensorflow:loss = 0.008897651, step = 24800 (10.598 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.48931\n",
            "INFO:tensorflow:loss = 0.009034656, step = 24900 (10.537 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.48116\n",
            "INFO:tensorflow:loss = 0.042201366, step = 25000 (10.548 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.50441\n",
            "INFO:tensorflow:loss = 0.046096683, step = 25100 (10.522 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.45168\n",
            "INFO:tensorflow:loss = 0.042274803, step = 25200 (10.578 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.50056\n",
            "INFO:tensorflow:loss = 0.040430054, step = 25300 (10.524 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.5047\n",
            "INFO:tensorflow:loss = 0.04830643, step = 25400 (10.525 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52555\n",
            "INFO:tensorflow:loss = 0.03799015, step = 25500 (10.498 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.52945\n",
            "INFO:tensorflow:loss = 0.011758738, step = 25600 (10.494 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.55894\n",
            "INFO:tensorflow:loss = 0.008690923, step = 25700 (10.458 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.54158\n",
            "INFO:tensorflow:loss = 0.007980375, step = 25800 (10.479 sec)\n",
            "INFO:tensorflow:global_step/sec: 9.58556\n",
            "INFO:tensorflow:loss = 0.008100148, step = 25900 (10.435 sec)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 25972...\n",
            "INFO:tensorflow:Saving checkpoints for 25972 into /tmp/tmpct35zelt/model.ckpt.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:970: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 25972...\n",
            "INFO:tensorflow:Loss for final step: 0.007890874.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.canned.linear.LinearClassifierV2 at 0x7f1408162410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-03-02T21:06:56Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmpct35zelt/model.ckpt-25972\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Inference Time : 1413.60249s\n",
            "INFO:tensorflow:Finished evaluation at 2021-03-02-21:30:30\n",
            "INFO:tensorflow:Saving dict for global step 25972: accuracy = 0.7411525, accuracy_baseline = 0.84903985, auc = 0.61487967, auc_precision_recall = 0.34246373, average_loss = 0.75202763, global_step = 25972, label/mean = 0.15096018, loss = 0.7520281, precision = 0.28304592, prediction/mean = 0.34225115, recall = 0.4662135\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 25972: /tmp/tmpct35zelt/model.ckpt-25972\n",
            "accuracy: 0.7411525\n",
            "accuracy_baseline: 0.84903985\n",
            "auc: 0.61487967\n",
            "auc_precision_recall: 0.34246373\n",
            "average_loss: 0.75202763\n",
            "global_step: 25972\n",
            "label/mean: 0.15096018\n",
            "loss: 0.7520281\n",
            "precision: 0.28304592\n",
            "prediction/mean: 0.34225115\n",
            "recall: 0.4662135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVgfMuCHyy_i"
      },
      "source": [
        "## Simple Deep Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkviWgqOxZ_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "904690c2-0caa-4b23-cf6f-6c6c85e9d1d0"
      },
      "source": [
        "dm = tf.estimator.DNNClassifier(\n",
        "    hidden_units=[512, 256, 128], \n",
        "    feature_columns=deep_columns, \n",
        "    model_dir=dmodel, \n",
        "    n_classes=2,\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate = lr)\n",
        ")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpkbmplq0c\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpkbmplq0c', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeKiv4Tp6Se6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66fc01a7-6893-4e61-ce1d-87881e5db551"
      },
      "source": [
        "dm.train(train_dl_train, steps=n_steps)\n",
        "dm_results = dm.evaluate(test_dl_train, steps=None)\n",
        "for key in sorted(dm_results):\n",
        "  print(\"%s: %s\" % (key, dm_results[key]))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpkbmplq0c/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
            "INFO:tensorflow:loss = 0.7091495, step = 0\n",
            "INFO:tensorflow:global_step/sec: 24.3188\n",
            "INFO:tensorflow:loss = 0.37694913, step = 100 (4.116 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0725\n",
            "INFO:tensorflow:loss = 0.39945105, step = 200 (3.986 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0234\n",
            "INFO:tensorflow:loss = 0.38545108, step = 300 (3.996 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1424\n",
            "INFO:tensorflow:loss = 0.4140769, step = 400 (3.979 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0113\n",
            "INFO:tensorflow:loss = 0.3987895, step = 500 (4.003 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1684\n",
            "INFO:tensorflow:loss = 0.39011037, step = 600 (3.971 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1381\n",
            "INFO:tensorflow:loss = 0.35897195, step = 700 (3.976 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0628\n",
            "INFO:tensorflow:loss = 0.38087285, step = 800 (3.990 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.034\n",
            "INFO:tensorflow:loss = 0.3830642, step = 900 (3.993 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0918\n",
            "INFO:tensorflow:loss = 0.3777687, step = 1000 (3.990 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0252\n",
            "INFO:tensorflow:loss = 0.38372836, step = 1100 (3.991 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.089\n",
            "INFO:tensorflow:loss = 0.3461867, step = 1200 (3.987 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9719\n",
            "INFO:tensorflow:loss = 0.35250455, step = 1300 (4.008 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.99\n",
            "INFO:tensorflow:loss = 0.38314795, step = 1400 (4.000 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.122\n",
            "INFO:tensorflow:loss = 0.31037095, step = 1500 (3.979 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.059\n",
            "INFO:tensorflow:loss = 0.3145305, step = 1600 (3.993 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9458\n",
            "INFO:tensorflow:loss = 0.34315655, step = 1700 (4.009 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0077\n",
            "INFO:tensorflow:loss = 0.3258981, step = 1800 (3.997 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9717\n",
            "INFO:tensorflow:loss = 0.32787895, step = 1900 (4.003 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0686\n",
            "INFO:tensorflow:loss = 0.30341274, step = 2000 (3.991 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0426\n",
            "INFO:tensorflow:loss = 0.2841039, step = 2100 (3.994 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3029\n",
            "INFO:tensorflow:loss = 0.21229102, step = 2200 (3.948 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.253\n",
            "INFO:tensorflow:loss = 0.16047196, step = 2300 (3.961 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2166\n",
            "INFO:tensorflow:loss = 0.16774431, step = 2400 (3.970 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9856\n",
            "INFO:tensorflow:loss = 0.20855016, step = 2500 (3.999 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9574\n",
            "INFO:tensorflow:loss = 0.17826916, step = 2600 (4.009 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.059\n",
            "INFO:tensorflow:loss = 0.1597202, step = 2700 (3.990 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.8388\n",
            "INFO:tensorflow:loss = 0.19247444, step = 2800 (4.024 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1674\n",
            "INFO:tensorflow:loss = 0.16786413, step = 2900 (3.976 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.8444\n",
            "INFO:tensorflow:loss = 0.16075529, step = 3000 (4.022 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.7162\n",
            "INFO:tensorflow:loss = 0.14379185, step = 3100 (4.047 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.8073\n",
            "INFO:tensorflow:loss = 0.059750475, step = 3200 (4.030 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.594\n",
            "INFO:tensorflow:loss = 0.06725519, step = 3300 (4.068 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0385\n",
            "INFO:tensorflow:loss = 0.050414093, step = 3400 (3.993 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.6793\n",
            "INFO:tensorflow:loss = 0.07593996, step = 3500 (4.055 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.4484\n",
            "INFO:tensorflow:loss = 0.09108291, step = 3600 (4.089 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.6827\n",
            "INFO:tensorflow:loss = 0.07925284, step = 3700 (4.050 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.5281\n",
            "INFO:tensorflow:loss = 0.09713142, step = 3800 (4.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.4997\n",
            "INFO:tensorflow:loss = 0.08235707, step = 3900 (4.085 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.5333\n",
            "INFO:tensorflow:loss = 0.07457291, step = 4000 (4.077 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.6506\n",
            "INFO:tensorflow:loss = 0.085432336, step = 4100 (4.055 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.6722\n",
            "INFO:tensorflow:loss = 0.048608646, step = 4200 (4.055 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9559\n",
            "INFO:tensorflow:loss = 0.038944095, step = 4300 (4.005 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0026\n",
            "INFO:tensorflow:loss = 0.032976367, step = 4400 (4.001 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9226\n",
            "INFO:tensorflow:loss = 0.06546354, step = 4500 (4.013 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0695\n",
            "INFO:tensorflow:loss = 0.036006857, step = 4600 (3.986 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0644\n",
            "INFO:tensorflow:loss = 0.032257445, step = 4700 (3.991 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9021\n",
            "INFO:tensorflow:loss = 0.0878573, step = 4800 (4.013 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1036\n",
            "INFO:tensorflow:loss = 0.011307265, step = 4900 (3.986 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0056\n",
            "INFO:tensorflow:loss = 0.02087678, step = 5000 (4.000 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0461\n",
            "INFO:tensorflow:loss = 0.0389537, step = 5100 (3.990 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1298\n",
            "INFO:tensorflow:loss = 0.07875639, step = 5200 (3.979 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2784\n",
            "INFO:tensorflow:loss = 0.0200479, step = 5300 (3.956 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2551\n",
            "INFO:tensorflow:loss = 0.011466638, step = 5400 (3.962 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1251\n",
            "INFO:tensorflow:loss = 0.018944254, step = 5500 (3.979 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1773\n",
            "INFO:tensorflow:loss = 0.006743115, step = 5600 (3.970 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1807\n",
            "INFO:tensorflow:loss = 0.017283913, step = 5700 (3.974 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3569\n",
            "INFO:tensorflow:loss = 0.017022705, step = 5800 (3.943 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9157\n",
            "INFO:tensorflow:loss = 0.023174565, step = 5900 (4.010 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0583\n",
            "INFO:tensorflow:loss = 0.034135085, step = 6000 (3.995 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9358\n",
            "INFO:tensorflow:loss = 0.01758102, step = 6100 (4.008 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.028\n",
            "INFO:tensorflow:loss = 0.020382736, step = 6200 (3.994 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0234\n",
            "INFO:tensorflow:loss = 0.0138643505, step = 6300 (3.999 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9716\n",
            "INFO:tensorflow:loss = 0.0066417544, step = 6400 (4.003 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0131\n",
            "INFO:tensorflow:loss = 0.0016456929, step = 6500 (3.998 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.851\n",
            "INFO:tensorflow:loss = 0.012375424, step = 6600 (4.025 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0941\n",
            "INFO:tensorflow:loss = 0.034681734, step = 6700 (3.986 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0885\n",
            "INFO:tensorflow:loss = 0.011513861, step = 6800 (3.985 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9755\n",
            "INFO:tensorflow:loss = 0.007385585, step = 6900 (4.004 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0254\n",
            "INFO:tensorflow:loss = 0.016064132, step = 7000 (3.996 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.017\n",
            "INFO:tensorflow:loss = 0.013855182, step = 7100 (4.000 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0446\n",
            "INFO:tensorflow:loss = 0.014012867, step = 7200 (3.990 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.7588\n",
            "INFO:tensorflow:loss = 0.016514441, step = 7300 (4.040 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0332\n",
            "INFO:tensorflow:loss = 0.0065320097, step = 7400 (3.994 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9135\n",
            "INFO:tensorflow:loss = 0.006726711, step = 7500 (4.014 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1657\n",
            "INFO:tensorflow:loss = 0.018575002, step = 7600 (3.977 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9118\n",
            "INFO:tensorflow:loss = 0.012663531, step = 7700 (4.011 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.132\n",
            "INFO:tensorflow:loss = 0.015913244, step = 7800 (3.979 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9921\n",
            "INFO:tensorflow:loss = 0.004257312, step = 7900 (4.001 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9551\n",
            "INFO:tensorflow:loss = 0.008207433, step = 8000 (4.009 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2525\n",
            "INFO:tensorflow:loss = 0.012800664, step = 8100 (3.960 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0427\n",
            "INFO:tensorflow:loss = 0.015689148, step = 8200 (3.991 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0131\n",
            "INFO:tensorflow:loss = 0.0038349708, step = 8300 (3.998 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0849\n",
            "INFO:tensorflow:loss = 0.006124165, step = 8400 (3.989 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9719\n",
            "INFO:tensorflow:loss = 0.0037575082, step = 8500 (4.003 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.124\n",
            "INFO:tensorflow:loss = 0.013421089, step = 8600 (3.979 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1875\n",
            "INFO:tensorflow:loss = 0.018852793, step = 8700 (3.968 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0459\n",
            "INFO:tensorflow:loss = 0.006263305, step = 8800 (3.996 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.122\n",
            "INFO:tensorflow:loss = 0.0056112735, step = 8900 (3.981 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1424\n",
            "INFO:tensorflow:loss = 0.0061510033, step = 9000 (3.974 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0993\n",
            "INFO:tensorflow:loss = 0.00796373, step = 9100 (3.985 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0634\n",
            "INFO:tensorflow:loss = 0.0048546623, step = 9200 (3.988 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1611\n",
            "INFO:tensorflow:loss = 0.009043065, step = 9300 (3.975 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1897\n",
            "INFO:tensorflow:loss = 0.011632912, step = 9400 (3.969 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.165\n",
            "INFO:tensorflow:loss = 0.01018903, step = 9500 (3.976 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.248\n",
            "INFO:tensorflow:loss = 0.0031499572, step = 9600 (3.960 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.065\n",
            "INFO:tensorflow:loss = 0.0070509994, step = 9700 (3.990 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2608\n",
            "INFO:tensorflow:loss = 0.013656785, step = 9800 (3.958 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1711\n",
            "INFO:tensorflow:loss = 0.0017379551, step = 9900 (3.971 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2143\n",
            "INFO:tensorflow:loss = 0.029320873, step = 10000 (3.970 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.6232\n",
            "INFO:tensorflow:loss = 0.008947842, step = 10100 (4.060 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1078\n",
            "INFO:tensorflow:loss = 0.0077095637, step = 10200 (3.983 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9788\n",
            "INFO:tensorflow:loss = 0.017814655, step = 10300 (4.004 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.097\n",
            "INFO:tensorflow:loss = 0.019197624, step = 10400 (3.982 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.8297\n",
            "INFO:tensorflow:loss = 0.0061352076, step = 10500 (4.027 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1603\n",
            "INFO:tensorflow:loss = 0.01147741, step = 10600 (3.975 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1502\n",
            "INFO:tensorflow:loss = 0.007418591, step = 10700 (3.977 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1573\n",
            "INFO:tensorflow:loss = 0.013631639, step = 10800 (3.974 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.8435\n",
            "INFO:tensorflow:loss = 0.009641578, step = 10900 (4.026 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0431\n",
            "INFO:tensorflow:loss = 0.009611871, step = 11000 (3.992 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0095\n",
            "INFO:tensorflow:loss = 0.008792272, step = 11100 (3.998 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.8568\n",
            "INFO:tensorflow:loss = 0.0032794592, step = 11200 (4.022 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.8265\n",
            "INFO:tensorflow:loss = 0.014105718, step = 11300 (4.030 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0365\n",
            "INFO:tensorflow:loss = 0.0018488398, step = 11400 (3.994 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0975\n",
            "INFO:tensorflow:loss = 0.0012596477, step = 11500 (3.987 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2056\n",
            "INFO:tensorflow:loss = 0.0051337434, step = 11600 (3.965 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.831\n",
            "INFO:tensorflow:loss = 0.0022003066, step = 11700 (4.029 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1558\n",
            "INFO:tensorflow:loss = 0.0082590645, step = 11800 (3.971 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3131\n",
            "INFO:tensorflow:loss = 0.0027615712, step = 11900 (3.955 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.8925\n",
            "INFO:tensorflow:loss = 0.00083584123, step = 12000 (4.015 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1944\n",
            "INFO:tensorflow:loss = 0.0068734246, step = 12100 (3.970 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0839\n",
            "INFO:tensorflow:loss = 0.008165707, step = 12200 (3.988 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.8325\n",
            "INFO:tensorflow:loss = 0.0018316513, step = 12300 (4.025 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0268\n",
            "INFO:tensorflow:loss = 0.011564087, step = 12400 (3.996 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1784\n",
            "INFO:tensorflow:loss = 0.001320761, step = 12500 (3.971 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2278\n",
            "INFO:tensorflow:loss = 0.005873061, step = 12600 (3.965 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3106\n",
            "INFO:tensorflow:loss = 0.002425472, step = 12700 (3.948 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3846\n",
            "INFO:tensorflow:loss = 0.0331039, step = 12800 (3.941 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3407\n",
            "INFO:tensorflow:loss = 0.002442114, step = 12900 (3.949 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4461\n",
            "INFO:tensorflow:loss = 0.008486322, step = 13000 (3.929 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3486\n",
            "INFO:tensorflow:loss = 0.0048802146, step = 13100 (3.942 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2357\n",
            "INFO:tensorflow:loss = 0.0031042453, step = 13200 (3.965 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4395\n",
            "INFO:tensorflow:loss = 0.0014922519, step = 13300 (3.931 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4106\n",
            "INFO:tensorflow:loss = 0.005301634, step = 13400 (3.934 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4806\n",
            "INFO:tensorflow:loss = 0.0010368483, step = 13500 (3.925 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4783\n",
            "INFO:tensorflow:loss = 0.009498046, step = 13600 (3.928 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.268\n",
            "INFO:tensorflow:loss = 0.004942204, step = 13700 (3.955 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4362\n",
            "INFO:tensorflow:loss = 0.00047252452, step = 13800 (3.931 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3336\n",
            "INFO:tensorflow:loss = 0.003945441, step = 13900 (3.947 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3466\n",
            "INFO:tensorflow:loss = 0.0013536304, step = 14000 (3.947 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.5184\n",
            "INFO:tensorflow:loss = 0.0011100315, step = 14100 (3.916 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.38\n",
            "INFO:tensorflow:loss = 0.011054628, step = 14200 (3.944 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2711\n",
            "INFO:tensorflow:loss = 0.0022407628, step = 14300 (3.954 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3519\n",
            "INFO:tensorflow:loss = 0.0040780893, step = 14400 (3.943 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4835\n",
            "INFO:tensorflow:loss = 0.011740719, step = 14500 (3.926 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.5247\n",
            "INFO:tensorflow:loss = 0.0066688843, step = 14600 (3.916 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4115\n",
            "INFO:tensorflow:loss = 0.0065371213, step = 14700 (3.941 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2294\n",
            "INFO:tensorflow:loss = 0.00074233965, step = 14800 (3.958 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4807\n",
            "INFO:tensorflow:loss = 0.014521798, step = 14900 (3.926 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.5252\n",
            "INFO:tensorflow:loss = 0.001156078, step = 15000 (3.920 sec)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 15031...\n",
            "INFO:tensorflow:Saving checkpoints for 15031 into /tmp/tmpkbmplq0c/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 15031...\n",
            "INFO:tensorflow:global_step/sec: 15.1111\n",
            "INFO:tensorflow:loss = 0.005931559, step = 15100 (6.618 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2225\n",
            "INFO:tensorflow:loss = 0.0026377535, step = 15200 (3.965 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.321\n",
            "INFO:tensorflow:loss = 0.002571111, step = 15300 (3.948 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3743\n",
            "INFO:tensorflow:loss = 0.00090411416, step = 15400 (3.940 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3688\n",
            "INFO:tensorflow:loss = 0.0018145952, step = 15500 (3.943 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2307\n",
            "INFO:tensorflow:loss = 0.0022996268, step = 15600 (3.962 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3569\n",
            "INFO:tensorflow:loss = 0.0018202872, step = 15700 (3.946 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3599\n",
            "INFO:tensorflow:loss = 0.0059587965, step = 15800 (3.941 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3368\n",
            "INFO:tensorflow:loss = 0.00042956742, step = 15900 (3.947 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3054\n",
            "INFO:tensorflow:loss = 0.0022723787, step = 16000 (3.950 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4519\n",
            "INFO:tensorflow:loss = 0.007440414, step = 16100 (3.929 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.5161\n",
            "INFO:tensorflow:loss = 0.00113129, step = 16200 (3.922 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3334\n",
            "INFO:tensorflow:loss = 0.016061045, step = 16300 (3.948 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3438\n",
            "INFO:tensorflow:loss = 0.004485693, step = 16400 (3.944 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1898\n",
            "INFO:tensorflow:loss = 0.001024327, step = 16500 (3.968 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3476\n",
            "INFO:tensorflow:loss = 0.005891331, step = 16600 (3.948 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4032\n",
            "INFO:tensorflow:loss = 0.007223493, step = 16700 (3.934 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4184\n",
            "INFO:tensorflow:loss = 0.0066710776, step = 16800 (3.934 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4288\n",
            "INFO:tensorflow:loss = 0.0010019133, step = 16900 (3.934 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.213\n",
            "INFO:tensorflow:loss = 0.0060566077, step = 17000 (3.966 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3901\n",
            "INFO:tensorflow:loss = 0.001658366, step = 17100 (3.941 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2077\n",
            "INFO:tensorflow:loss = 0.0009628348, step = 17200 (3.966 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4827\n",
            "INFO:tensorflow:loss = 0.0021229617, step = 17300 (3.922 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.5259\n",
            "INFO:tensorflow:loss = 0.0026178947, step = 17400 (3.922 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3743\n",
            "INFO:tensorflow:loss = 0.0018865803, step = 17500 (3.941 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2777\n",
            "INFO:tensorflow:loss = 0.00497172, step = 17600 (3.953 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3309\n",
            "INFO:tensorflow:loss = 0.0009144694, step = 17700 (3.948 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.435\n",
            "INFO:tensorflow:loss = 0.0010197818, step = 17800 (3.932 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4724\n",
            "INFO:tensorflow:loss = 0.0026497324, step = 17900 (3.926 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2786\n",
            "INFO:tensorflow:loss = 0.009289053, step = 18000 (3.955 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2707\n",
            "INFO:tensorflow:loss = 0.0017237123, step = 18100 (3.960 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4213\n",
            "INFO:tensorflow:loss = 0.0066303583, step = 18200 (3.931 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4041\n",
            "INFO:tensorflow:loss = 0.00320052, step = 18300 (3.939 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2507\n",
            "INFO:tensorflow:loss = 0.0032979052, step = 18400 (3.958 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2408\n",
            "INFO:tensorflow:loss = 0.0008873848, step = 18500 (3.961 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2954\n",
            "INFO:tensorflow:loss = 0.0004384984, step = 18600 (3.958 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1772\n",
            "INFO:tensorflow:loss = 0.0016866747, step = 18700 (3.970 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3807\n",
            "INFO:tensorflow:loss = 0.0038910352, step = 18800 (3.941 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3829\n",
            "INFO:tensorflow:loss = 0.0013554486, step = 18900 (3.939 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3892\n",
            "INFO:tensorflow:loss = 0.017427199, step = 19000 (3.935 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3817\n",
            "INFO:tensorflow:loss = 0.004436868, step = 19100 (3.940 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.408\n",
            "INFO:tensorflow:loss = 0.00083341694, step = 19200 (3.937 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1028\n",
            "INFO:tensorflow:loss = 0.0003325781, step = 19300 (3.982 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.325\n",
            "INFO:tensorflow:loss = 0.00575012, step = 19400 (3.949 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4111\n",
            "INFO:tensorflow:loss = 0.00012912584, step = 19500 (3.939 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.414\n",
            "INFO:tensorflow:loss = 0.00049064134, step = 19600 (3.933 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3572\n",
            "INFO:tensorflow:loss = 0.0013569511, step = 19700 (3.942 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4834\n",
            "INFO:tensorflow:loss = 0.013714705, step = 19800 (3.924 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.6345\n",
            "INFO:tensorflow:loss = 0.0065068128, step = 19900 (3.904 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.5558\n",
            "INFO:tensorflow:loss = 0.0011149602, step = 20000 (3.911 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3987\n",
            "INFO:tensorflow:loss = 0.0023744479, step = 20100 (3.937 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3401\n",
            "INFO:tensorflow:loss = 0.0015220047, step = 20200 (3.949 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1708\n",
            "INFO:tensorflow:loss = 0.00039702829, step = 20300 (3.969 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2488\n",
            "INFO:tensorflow:loss = 0.0049255225, step = 20400 (3.964 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2788\n",
            "INFO:tensorflow:loss = 0.013277128, step = 20500 (3.954 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2349\n",
            "INFO:tensorflow:loss = 0.0015336009, step = 20600 (3.966 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0833\n",
            "INFO:tensorflow:loss = 0.000888997, step = 20700 (3.987 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3541\n",
            "INFO:tensorflow:loss = 0.0026741493, step = 20800 (3.941 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2108\n",
            "INFO:tensorflow:loss = 0.00055743544, step = 20900 (3.969 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3527\n",
            "INFO:tensorflow:loss = 0.00022972518, step = 21000 (3.944 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3198\n",
            "INFO:tensorflow:loss = 0.00026274205, step = 21100 (3.949 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2352\n",
            "INFO:tensorflow:loss = 0.0011768327, step = 21200 (3.961 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0525\n",
            "INFO:tensorflow:loss = 0.001696983, step = 21300 (3.993 sec)\n",
            "INFO:tensorflow:global_step/sec: 24.9631\n",
            "INFO:tensorflow:loss = 0.0018496957, step = 21400 (4.007 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3961\n",
            "INFO:tensorflow:loss = 0.0013949887, step = 21500 (3.937 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1349\n",
            "INFO:tensorflow:loss = 0.0015853309, step = 21600 (3.978 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0916\n",
            "INFO:tensorflow:loss = 0.00095847825, step = 21700 (3.984 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1604\n",
            "INFO:tensorflow:loss = 0.0029309418, step = 21800 (3.975 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2643\n",
            "INFO:tensorflow:loss = 0.0110664815, step = 21900 (3.959 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3061\n",
            "INFO:tensorflow:loss = 0.005315342, step = 22000 (3.952 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4392\n",
            "INFO:tensorflow:loss = 0.0011597634, step = 22100 (3.931 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1303\n",
            "INFO:tensorflow:loss = 0.0021871699, step = 22200 (3.978 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3587\n",
            "INFO:tensorflow:loss = 0.00048193912, step = 22300 (3.943 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.5064\n",
            "INFO:tensorflow:loss = 0.00059704017, step = 22400 (3.922 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3383\n",
            "INFO:tensorflow:loss = 0.0042370697, step = 22500 (3.948 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4009\n",
            "INFO:tensorflow:loss = 0.0038020313, step = 22600 (3.936 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4873\n",
            "INFO:tensorflow:loss = 0.0002411638, step = 22700 (3.922 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3799\n",
            "INFO:tensorflow:loss = 0.00012435016, step = 22800 (3.940 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3201\n",
            "INFO:tensorflow:loss = 0.0059003104, step = 22900 (3.950 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.5007\n",
            "INFO:tensorflow:loss = 0.00092007255, step = 23000 (3.922 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4823\n",
            "INFO:tensorflow:loss = 0.0077341516, step = 23100 (3.927 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.374\n",
            "INFO:tensorflow:loss = 0.0032692363, step = 23200 (3.937 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4046\n",
            "INFO:tensorflow:loss = 0.0025491903, step = 23300 (3.940 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3913\n",
            "INFO:tensorflow:loss = 0.0030873842, step = 23400 (3.936 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4142\n",
            "INFO:tensorflow:loss = 0.0034316676, step = 23500 (3.934 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4006\n",
            "INFO:tensorflow:loss = 0.0020628273, step = 23600 (3.940 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3768\n",
            "INFO:tensorflow:loss = 0.0026296712, step = 23700 (3.937 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2644\n",
            "INFO:tensorflow:loss = 0.0008673047, step = 23800 (3.958 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4307\n",
            "INFO:tensorflow:loss = 0.0007156675, step = 23900 (3.936 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4934\n",
            "INFO:tensorflow:loss = 0.0031336464, step = 24000 (3.922 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4011\n",
            "INFO:tensorflow:loss = 0.0023068546, step = 24100 (3.932 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.5888\n",
            "INFO:tensorflow:loss = 0.0006924799, step = 24200 (3.909 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4349\n",
            "INFO:tensorflow:loss = 0.0008766753, step = 24300 (3.935 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4116\n",
            "INFO:tensorflow:loss = 0.003136747, step = 24400 (3.936 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.1913\n",
            "INFO:tensorflow:loss = 0.0019031379, step = 24500 (3.966 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3704\n",
            "INFO:tensorflow:loss = 0.0014914288, step = 24600 (3.942 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4556\n",
            "INFO:tensorflow:loss = 0.0013604725, step = 24700 (3.936 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.5386\n",
            "INFO:tensorflow:loss = 0.0005124636, step = 24800 (3.908 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4292\n",
            "INFO:tensorflow:loss = 0.0044776076, step = 24900 (3.932 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4388\n",
            "INFO:tensorflow:loss = 0.002308597, step = 25000 (3.930 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.5582\n",
            "INFO:tensorflow:loss = 0.0007407457, step = 25100 (3.916 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4455\n",
            "INFO:tensorflow:loss = 0.004921269, step = 25200 (3.931 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.3197\n",
            "INFO:tensorflow:loss = 0.004997777, step = 25300 (3.948 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2857\n",
            "INFO:tensorflow:loss = 0.0003344352, step = 25400 (3.952 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.0689\n",
            "INFO:tensorflow:loss = 0.004334498, step = 25500 (3.992 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2678\n",
            "INFO:tensorflow:loss = 0.0016804592, step = 25600 (3.955 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.4719\n",
            "INFO:tensorflow:loss = 0.0025541415, step = 25700 (3.928 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.2179\n",
            "INFO:tensorflow:loss = 0.00019301484, step = 25800 (3.967 sec)\n",
            "INFO:tensorflow:global_step/sec: 25.349\n",
            "INFO:tensorflow:loss = 0.00095220905, step = 25900 (3.940 sec)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 25972...\n",
            "INFO:tensorflow:Saving checkpoints for 25972 into /tmp/tmpkbmplq0c/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 25972...\n",
            "INFO:tensorflow:Loss for final step: 4.565333e-05.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifierV2 at 0x7f12f4f4b5d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-03-02T21:47:53Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmpkbmplq0c/model.ckpt-25972\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Inference Time : 1078.22848s\n",
            "INFO:tensorflow:Finished evaluation at 2021-03-02-22:05:51\n",
            "INFO:tensorflow:Saving dict for global step 25972: accuracy = 0.81409216, accuracy_baseline = 0.84903985, auc = 0.69695014, auc_precision_recall = 0.35091102, average_loss = 1.634197, global_step = 25972, label/mean = 0.15096018, loss = 1.6342001, precision = 0.39633992, prediction/mean = 0.1735402, recall = 0.4425631\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 25972: /tmp/tmpkbmplq0c/model.ckpt-25972\n",
            "accuracy: 0.81409216\n",
            "accuracy_baseline: 0.84903985\n",
            "auc: 0.69695014\n",
            "auc_precision_recall: 0.35091102\n",
            "average_loss: 1.634197\n",
            "global_step: 25972\n",
            "label/mean: 0.15096018\n",
            "loss: 1.6342001\n",
            "precision: 0.39633992\n",
            "prediction/mean: 0.1735402\n",
            "recall: 0.4425631\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yazi2YYBy0Cz"
      },
      "source": [
        "## Simple Wide & Deep Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vI8AnbtxLun"
      },
      "source": [
        "wdm = tf.estimator.DNNLinearCombinedClassifier(\n",
        "    model_dir=wdmodel, \n",
        "    linear_feature_columns=crossed_columns,\n",
        "    dnn_feature_columns=deep_columns,\n",
        "    dnn_hidden_units=[512, 256, 128],\n",
        "    n_classes=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c0bs-rv6THC"
      },
      "source": [
        "wdm.train(input_fn=train_dl_train, steps=n_steps)\n",
        "wdm_results = wdm.evaluate(input_fn=test_dl_train, steps=None)\n",
        "for key in sorted(wdm_results):\n",
        "  print(\"%s: %s\" % (key, wdm_results[key]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSqPiNAgLNU9"
      },
      "source": [
        "## Bayesian Wide & Deep Model\n",
        "\n",
        "1.   Wide & Deep Models are not connected to common output nodes @ the end.\n",
        "2.   They are first connected to a common dropout layer which can be turned on during inference time as well.\n",
        "3.   The dropout layer is then connected to output nodes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HvmVQdfTyQrY",
        "outputId": "1947d5d3-60e6-4c3b-ec5a-c340dc1ee4a1"
      },
      "source": [
        "def wide_and_deep_model(wide_inputs, wide_feature_columns, \n",
        "                        deep_inputs, dnn_feature_columns, dnn_hidden_units, \n",
        "                        multihead_count = 64, p_value=0.5):\n",
        "\n",
        "    #Build the Deep Network\n",
        "    #deep = tf.keras.layers.DenseFeatures(dnn_feature_columns, name='deep_inputs')(deep_inputs)\n",
        "    deep_input_layer = tf.keras.layers.DenseFeatures(dnn_feature_columns, name='deep_inputs')\n",
        "    deep = deep_input_layer(deep_inputs)\n",
        "\n",
        "    for layerno, numnodes in enumerate(dnn_feature_columns):\n",
        "        deep = tf.keras.layers.Dense(numnodes, activation='relu', name='dnn_{}'.format(layerno+1))(deep)        \n",
        "    \n",
        "    #Build the Wide Network\n",
        "    wide_input_layer = tf.keras.layers.DenseFeatures(wide_feature_columns, name='wide_inputs')\n",
        "    wide = wide_input_layer(wide_inputs)\n",
        "\n",
        "    #Concatenate the Wide & Deep\n",
        "    both = tf.keras.layers.concatenate([deep, wide], name='both')\n",
        "\n",
        "    #Create the multi-head layer\n",
        "    multihead_pre_dropout = tf.keras.layers.dropout(p_value)(both, training=True)\n",
        "    multihead = tf.keras.layers.Dense(multihead_count, activation='relu', name='multihead')(multihead_pre_dropout)\n",
        "    multihead_dropout = tf.keras.layers.dropout(p_value)(multihead, training=True)\n",
        "\n",
        "    #Create the output layer\n",
        "    output = tf.keras.layers.Dense(2, activation='softmax', name='optimal_action')(multihead_dropout)\n",
        "    model = tf.keras.Model(inputs, output)\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "bwdmodel = wide_and_deep_model(wide_inputs = crossed_columns_input, wide_feature_columns = crossed_columns, \n",
        "                               deep_inputs = deep_columns_input, dnn_feature_columns = deep_columns, \n",
        "                               dnn_hidden_units = [512, 256, 128], multihead_count = 64, p_value=0.5)\n",
        "tf.keras.utils.plot_model(bwdmodel, '/content/drive/MyDrive/Bandit_Project/models/bayesian_w&d.png', show_shapes=False, rankdir='LR')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/feature_column/dense_features.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, features, cols_to_output_tensors, training)\u001b[0m\n\u001b[1;32m    165\u001b[0m           tensor = column.get_dense_tensor(\n\u001b[0;32m--> 166\u001b[0;31m               transformation_cache, self._state_manager, training=training)\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: get_dense_tensor() got an unexpected keyword argument 'training'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/feature_column/feature_column_v2.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key, state_manager, training)\u001b[0m\n\u001b[1;32m   2352\u001b[0m       transformed = column.transform_feature(\n\u001b[0;32m-> 2353\u001b[0;31m           self, state_manager, training=training)\n\u001b[0m\u001b[1;32m   2354\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: transform_feature() got an unexpected keyword argument 'training'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/feature_column/feature_column_v2.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key, state_manager, training)\u001b[0m\n\u001b[1;32m   2352\u001b[0m       transformed = column.transform_feature(\n\u001b[0;32m-> 2353\u001b[0;31m           self, state_manager, training=training)\n\u001b[0m\u001b[1;32m   2354\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: transform_feature() got an unexpected keyword argument 'training'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-7b355fb0cf48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m bwdmodel = wide_and_deep_model(wide_inputs = crossed_columns_input, wide_feature_columns = crossed_columns, \n\u001b[1;32m     34\u001b[0m                                \u001b[0mdeep_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_columns_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnn_feature_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                                dnn_hidden_units = [512, 256, 128], multihead_count = 64, p_value=0.5)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbwdmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Bandit_Project/models/bayesian_w&d.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-7b355fb0cf48>\u001b[0m in \u001b[0;36mwide_and_deep_model\u001b[0;34m(wide_inputs, wide_feature_columns, deep_inputs, dnn_feature_columns, dnn_hidden_units, multihead_count, p_value)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#deep = tf.keras.layers.DenseFeatures(dnn_feature_columns, name='deep_inputs')(deep_inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdeep_input_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn_feature_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'deep_inputs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdeep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_input_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayerno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumnodes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn_feature_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 952\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1091\u001b[0;31m             inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    348\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Allowlisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/feature_column/dense_features.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, features, cols_to_output_tensors, training)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m           tensor = column.get_dense_tensor(transformation_cache,\n\u001b[0;32m--> 169\u001b[0;31m                                            self._state_manager)\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0mprocessed_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_dense_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcols_to_output_tensors\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/feature_column/feature_column_v2.py\u001b[0m in \u001b[0;36mget_dense_tensor\u001b[0;34m(self, transformation_cache, state_manager)\u001b[0m\n\u001b[1;32m   4156\u001b[0m     \u001b[0;31m# Feature has been already transformed. Return the intermediate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4157\u001b[0m     \u001b[0;31m# representation created by transform_feature.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4158\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtransformation_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4160\u001b[0m   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/feature_column/feature_column_v2.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key, state_manager, training)\u001b[0m\n\u001b[1;32m   2353\u001b[0m           self, state_manager, training=training)\n\u001b[1;32m   2354\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m       \u001b[0mtransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Column {} is not supported.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/feature_column/feature_column_v2.py\u001b[0m in \u001b[0;36mtransform_feature\u001b[0;34m(self, transformation_cache, state_manager)\u001b[0m\n\u001b[1;32m   4093\u001b[0m     \"\"\"\n\u001b[1;32m   4094\u001b[0m     id_weight_pair = self.categorical_column.get_sparse_tensors(\n\u001b[0;32m-> 4095\u001b[0;31m         transformation_cache, state_manager)\n\u001b[0m\u001b[1;32m   4096\u001b[0m     return self._transform_id_weight_pair(id_weight_pair,\n\u001b[1;32m   4097\u001b[0m                                           self.variable_shape[-1])\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/feature_column/feature_column_v2.py\u001b[0m in \u001b[0;36mget_sparse_tensors\u001b[0;34m(self, transformation_cache, state_manager)\u001b[0m\n\u001b[1;32m   3580\u001b[0m     \u001b[0;34m\"\"\"See `CategoricalColumn` base class.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3581\u001b[0m     return CategoricalColumn.IdWeightPair(\n\u001b[0;32m-> 3582\u001b[0;31m         transformation_cache.get(self, state_manager), None)\n\u001b[0m\u001b[1;32m   3583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3584\u001b[0m   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/feature_column/feature_column_v2.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key, state_manager, training)\u001b[0m\n\u001b[1;32m   2353\u001b[0m           self, state_manager, training=training)\n\u001b[1;32m   2354\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m       \u001b[0mtransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Column {} is not supported.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/feature_column/feature_column_v2.py\u001b[0m in \u001b[0;36mtransform_feature\u001b[0;34m(self, transformation_cache, state_manager)\u001b[0m\n\u001b[1;32m   3558\u001b[0m     input_tensor = _to_sparse_input_and_drop_ignore_values(\n\u001b[1;32m   3559\u001b[0m         transformation_cache.get(self.key, state_manager))\n\u001b[0;32m-> 3560\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_input_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3562\u001b[0m   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/feature_column/feature_column_v2.py\u001b[0m in \u001b[0;36m_transform_input_tensor\u001b[0;34m(self, input_tensor, state_manager)\u001b[0m\n\u001b[1;32m   3526\u001b[0m           \u001b[0;34m'Column dtype and SparseTensors dtype must be compatible. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m           'key: {}, column dtype: {}, tensor dtype: {}'.format(\n\u001b[0;32m-> 3528\u001b[0;31m               self.key, self.dtype, input_tensor.dtype))\n\u001b[0m\u001b[1;32m   3529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3530\u001b[0m     fc_utils.assert_string_or_int(\n",
            "\u001b[0;31mValueError\u001b[0m: Column dtype and SparseTensors dtype must be compatible. key: campaign_category, column dtype: <dtype: 'string'>, tensor dtype: <dtype: 'int64'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJuCY7jBkae7",
        "outputId": "0f37ed24-020e-4267-cd95-24a0a3dea34b"
      },
      "source": [
        "pprint(deep_columns)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[NumericColumn(key='retention_score', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=11.467980895825553, std=11.35391986430546)),\n",
            " NumericColumn(key='recency_score', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=1.23904221901564, std=2.216794122042123)),\n",
            " NumericColumn(key='frequency_score', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=14.977138288600283, std=20.754428265423773)),\n",
            " NumericColumn(key='sent_week', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=28.594960628048973, std=14.377041994557581)),\n",
            " NumericColumn(key='sent_dayofweek', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=2.638369149867888, std=2.163983074344224)),\n",
            " NumericColumn(key='sent_hr', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=17.291778162586947, std=3.9303658889007997)),\n",
            " NumericColumn(key='discount', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=11.300244227332426, std=19.60056341212265)),\n",
            " NumericColumn(key='sends_since_last_open', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f1410855b90>, mean=11.36886785974351, std=14.997835900332081)),\n",
            " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='promo', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0)),\n",
            " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='sale', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0)),\n",
            " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='campaign_category', vocabulary_list=('Trend', 'NewArrivals', 'Dedicated', 'InnovationSpotlight', 'Core', 'Replen', 'ProductSpotlight', 'Other', 'Brand', 'Tops'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
            " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='is_one_for_free', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0)),\n",
            " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='free_shipping', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0)),\n",
            " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='is_exclusive', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0)),\n",
            " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='has_urgency', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0)),\n",
            " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='sl_contains_price', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0)),\n",
            " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='is_discount_mentioned', vocabulary_list=(0, 1), dtype=tf.int64, default_value=-1, num_oov_buckets=0)),\n",
            " EmbeddingColumn(categorical_column=HashedCategoricalColumn(key='riid', hash_bucket_size=2000000, dtype=tf.int64), dimension=32, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f141067cd10>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True, use_safe_embedding_lookup=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8JrOmA5nJuD"
      },
      "source": [
        "tf.keras.layers.DenseFeatures([deep_columns[10]])(train_dl_fit.take(1)).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}