{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_W_D_W&D.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOUO+weBjMdtW7NIe3j/+dt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fellowship/deep-and-wide-bandit/blob/dev/TensorFlow_W_D_W%26D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2n6dXxu3CjB"
      },
      "source": [
        "# Required Installs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjlUj5ABCuck"
      },
      "source": [
        "# Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oFqkg0Mp_hu4",
        "outputId": "9df0e234-9661-4954-dbaf-ed7c8a31790a"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ladp5N5YYeP3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPSINg6-DWp7",
        "outputId": "bc167698-0469-45e5-bee6-54d580cb847f"
      },
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grwgxGGsELGO"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from zipfile import ZipFile\n",
        "import re\n",
        "import json\n",
        "import pickle as pkl\n",
        "import re\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from functools import partial\n",
        "import random\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "from pprint import pprint\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from IPython.core.interactiveshell import InteractiveShell  \n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "#Makes panda and numpy easier to read\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "#import torch\n",
        "#from torch.utils.data import Dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_saGcV11C0Ol"
      },
      "source": [
        "# Get the data ready\n",
        "\n",
        "\n",
        "We extract the weekly dataset CSVs & shortlisted train+valid index CSVs s.t. we have 1 train and 1 valid index CSV per weekly CSV.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6ZYrGnOG_T7"
      },
      "source": [
        "overwrite_zip_extract = False\n",
        "\n",
        "#Build the list of train_valid items\n",
        "train_valid_l = [Path(\"/content/drive/MyDrive/Bandit_Project/BanditsData/Train_Valid_Jul_Dec_2019.zip\"),\n",
        "                 Path(\"/content/drive/MyDrive/Bandit_Project/BanditsData/Train_Valid_Jan_Jun_2020.zip\")]\n",
        "\n",
        "#Extract items to a folder in your GDrive\n",
        "folder = Path(\"/content/drive/MyDrive/Bandit_Project/aleksey\")\n",
        "\n",
        "#Create a train & valid folder\n",
        "train_folder = (folder/'train')\n",
        "train_folder.mkdir(exist_ok=True)\n",
        "\n",
        "valid_folder = (folder/'valid')\n",
        "valid_folder.mkdir(exist_ok=True)\n",
        "\n",
        "#Iterate through the list\n",
        "if not(train_folder.exists() or valid_folder.exists()) or overwrite_zip_extract:\n",
        "  \n",
        "  for path in train_valid_l:    \n",
        "      with ZipFile(path, 'r') as zip_obj:\n",
        "        zip_obj.extractall(folder)\n",
        "\n",
        "#TBD: We need to implement code that determines action when overwrite is set to True"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z9jUTvmcUzq"
      },
      "source": [
        "## Subsetting Train & Valid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e4ZXk7DRLH_"
      },
      "source": [
        "#Function to export data corresponding to chosen indices to train/valid folder\n",
        "def export_data_subset(data_l, idx_l, save_folder, overwrite_flag = False):\n",
        "\n",
        "  cnt = 0\n",
        "\n",
        "  #If the folder is not empty and overwrite_flag is set to True, delete all files in the folder\n",
        "  if overwrite_flag:\n",
        "    \n",
        "    print(f\"\\n[INFO] Deleting files in {save_folder.name} directory...\")\n",
        "    for file_path in save_folder.iterdir():\n",
        "      \n",
        "      print(f\"[INFO] Deleting {file_path.name}\")\n",
        "      file_path.unlink()\n",
        "  \n",
        "  #Iterate over the (weekly data path, training set indices path) zipped object\n",
        "  for (data_path, idx_path) in zip(data_l, idx_l):\n",
        "\n",
        "    if (cnt + 1) % 5 == 0:\n",
        "      print(f\"[INFO] Building {save_folder.name} file from {data_path.name}\")\n",
        "    \n",
        "    #Use pandas to read weekly data + corresponding index CSV files\n",
        "    data = pd.read_csv(data_path)\n",
        "    idx = pd.read_csv(idx_path, header=None, squeeze=True).tolist()\n",
        "\n",
        "    #Subset the data and save it to the appropriate csv file\n",
        "    data_subset = data.iloc[idx, :]\n",
        "\n",
        "    #Save the data subset\n",
        "    data_subset.to_csv(save_folder/(data_path.name), index=False, compression=\"gzip\", header=True)\n",
        "\n",
        "    #Increment Counter\n",
        "    cnt += 1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8azEEnPWI87F"
      },
      "source": [
        "def process_input(source, dest, x_cols, y_col, overwrite=True):\n",
        "  \n",
        "  filename = source.name + \".csv.gz\"\n",
        "  dest_path = dest/filename\n",
        "  \n",
        "  #Check whether files exist in destination + should not overwrite - If they do, print an error message  \n",
        "  if dest_path.exists():\n",
        "    if not(overwrite):\n",
        "      print(f\"[ERROR] {dest_path.name} currently exists. Pls set overwrite flag to True!\")\n",
        "      return dest_path\n",
        "    else:\n",
        "      #Delete current files in the dest folder\n",
        "      for file_path in dest.iterdir():\n",
        "        file_path.unlink()\n",
        "\n",
        "  cnt = 0\n",
        "\n",
        "  #Iterate over each file in source\n",
        "  for file_path in source.iterdir():\n",
        "\n",
        "    #Print update\n",
        "    print(f\"[INFO] Currently working on {source.name}: {file_path.name}\")\n",
        "    \n",
        "    #Read in the data\n",
        "    data = pd.read_csv(file_path, compression=\"gzip\", header=[0])\n",
        "\n",
        "    #Shortlist columns to get the overall CSV\n",
        "    cols = x_cols + y_col\n",
        "    subset = data[cols]\n",
        "\n",
        "    \"\"\"\n",
        "    #Process numeric columns\n",
        "    for col in x_cols:\n",
        "\n",
        "      if col in stats.keys():\n",
        "\n",
        "        mean = stats[col][\"mean\"]\n",
        "        std = stats[col][\"std\"]\n",
        "        subset.loc[:, col] = (subset.loc[:, col] - mean)/std\n",
        "    \"\"\"        \n",
        "\n",
        "    #Check whether first CSV file â€”> include header, otherwise ignore\n",
        "    header_flag = True if not(cnt) else False\n",
        "\n",
        "    #Save to dest    \n",
        "    subset.to_csv(dest_path, mode='a', compression=\"gzip\", header=header_flag, index=False)\n",
        "\n",
        "    #Increment counter\n",
        "    cnt += 1\n",
        "  \n",
        "  #Return the path to the processed input file\n",
        "  return dest_path"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWlsPxcLfpNY",
        "outputId": "e7534b20-a84d-4d21-ecbe-3d415a70bcf7"
      },
      "source": [
        "#Setting overwrite flag\n",
        "overwrite_dset = False\n",
        "\n",
        "#We need to connect a file like \"sends_2019_wk26.csv\" WITH \"selected_rows_sends_2019_wk26.csv\"\n",
        "weekly_data_path_l = sorted([i for i in folder.iterdir() if re.search(\"/sends\", str(i), re.I)], key=lambda x: str(x))\n",
        "train_indices_path_l = sorted([i for i in folder.iterdir() if re.search(\"/selected_rows_sends\", str(i), re.I)], key=lambda x: str(x))\n",
        "valid_indices_path_l = sorted([i for i in folder.iterdir() if re.search(\"/selected_rows_valid\", str(i), re.I)], key=lambda x: str(x))\n",
        "\n",
        "print(f\"[INFO] Displaying indices to build training data for {str(weekly_data_path_l[0])}: {train_indices_path_l[0]}\")\n",
        "print(f\"[INFO] Displaying indices to build validation data for {str(weekly_data_path_l[0])}: {valid_indices_path_l[0]}\")\n",
        "\n",
        "#Check if training folder is empty\n",
        "if not(list(train_folder.iterdir())) or overwrite_dset:\n",
        "\n",
        "  #Run the export function\n",
        "  export_data_subset(data_l = weekly_data_path_l, idx_l = train_indices_path_l, \n",
        "                     save_folder = train_folder, overwrite_flag = False)\n",
        "\n",
        "else:\n",
        "  print(\"\\n[INFO] Training Data Already Created...\")\n",
        "\n",
        "#Execute only if valid folder is empty\n",
        "if not(list(valid_folder.iterdir())) or overwrite_dset:\n",
        "\n",
        "  #Run the export function\n",
        "  export_data_subset(data_l = weekly_data_path_l, idx_l = valid_indices_path_l, \n",
        "                     save_folder = valid_folder, overwrite_flag = overwrite_dset)\n",
        "\n",
        "else:\n",
        "  print(\"[INFO] Validation Data Already Created...\\n\")\n",
        "\n",
        "train_list = sorted([file_path for file_path in train_folder.iterdir()], key = lambda x: str(x))\n",
        "valid_list = sorted([file_path for file_path in valid_folder.iterdir()], key = lambda x: str(x))\n",
        "\n",
        "print(\"\\n[INFO] Displaying the first 5 elements of train_list:\")\n",
        "pprint(train_list[:5])\n",
        "print(\"\\n[INFO] Displaying the first 5 elements of train_list:\")\n",
        "pprint(valid_list[:5])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Displaying indices to build training data for /content/drive/MyDrive/Bandit_Project/aleksey/sends_2019_wk26.csv: /content/drive/MyDrive/Bandit_Project/aleksey/selected_rows_sends_2019_wk26.csv\n",
            "[INFO] Displaying indices to build validation data for /content/drive/MyDrive/Bandit_Project/aleksey/sends_2019_wk26.csv: /content/drive/MyDrive/Bandit_Project/aleksey/selected_rows_valid_sends_2019_wk26.csv\n",
            "\n",
            "[INFO] Training Data Already Created...\n",
            "[INFO] Validation Data Already Created...\n",
            "\n",
            "\n",
            "[INFO] Displaying the first 5 elements of train_list:\n",
            "[PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/train/sends_2019_wk26.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/train/sends_2019_wk27.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/train/sends_2019_wk28.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/train/sends_2019_wk29.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/train/sends_2019_wk30.csv')]\n",
            "\n",
            "[INFO] Displaying the first 5 elements of train_list:\n",
            "[PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/valid/sends_2019_wk26.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/valid/sends_2019_wk27.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/valid/sends_2019_wk28.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/valid/sends_2019_wk29.csv'),\n",
            " PosixPath('/content/drive/MyDrive/Bandit_Project/aleksey/valid/sends_2019_wk30.csv')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZggU1qg8xvVv",
        "outputId": "4d753e91-36fd-4893-cb96-f34508a2b85b"
      },
      "source": [
        "#Display head of first element of both training & validation subset\n",
        "train_sample = pd.read_csv(train_list[0], compression=\"gzip\", header=[0])\n",
        "print(\"[INFO] Sample Training Data\")\n",
        "train_sample.head()\n",
        "valid_sample = pd.read_csv(valid_list[0], compression=\"gzip\", header=[0])\n",
        "print(\"[INFO] Sample Validation Data\")\n",
        "valid_sample.head()\n",
        "\n",
        "#Get the column names of the data\n",
        "data_col_names = train_sample.columns.tolist()\n",
        "print(\"[INFO] The full list of column names include:\")\n",
        "pprint(data_col_names)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Sample Training Data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>riid</th>\n",
              "      <th>retention_score</th>\n",
              "      <th>frequency_score</th>\n",
              "      <th>recency_score</th>\n",
              "      <th>sends_since_last_open</th>\n",
              "      <th>days_subscr</th>\n",
              "      <th>aq_year</th>\n",
              "      <th>aq_week</th>\n",
              "      <th>aq_dayofweek</th>\n",
              "      <th>aq_period</th>\n",
              "      <th>campaign_id</th>\n",
              "      <th>campaign_category</th>\n",
              "      <th>campaign_Brand</th>\n",
              "      <th>campaign_Core</th>\n",
              "      <th>campaign_Dedicated</th>\n",
              "      <th>campaign_InnovationSpotlight</th>\n",
              "      <th>campaign_NewArrivals</th>\n",
              "      <th>campaign_ProductSpotlight</th>\n",
              "      <th>campaign_Replen</th>\n",
              "      <th>campaign_Tops</th>\n",
              "      <th>campaign_Trend</th>\n",
              "      <th>campaign_Other</th>\n",
              "      <th>discount</th>\n",
              "      <th>promo</th>\n",
              "      <th>sale</th>\n",
              "      <th>is_one_for_free</th>\n",
              "      <th>free_shipping</th>\n",
              "      <th>is_exclusive</th>\n",
              "      <th>has_urgency</th>\n",
              "      <th>sl_contains_price</th>\n",
              "      <th>is_discount_mentioned</th>\n",
              "      <th>message_size</th>\n",
              "      <th>sent_week</th>\n",
              "      <th>sent_dayofweek</th>\n",
              "      <th>sent_hr</th>\n",
              "      <th>opened</th>\n",
              "      <th>unsub</th>\n",
              "      <th>rev_3dv2</th>\n",
              "      <th>reward</th>\n",
              "      <th>optimal_action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>193066422</td>\n",
              "      <td>28.000</td>\n",
              "      <td>74</td>\n",
              "      <td>9.947</td>\n",
              "      <td>1</td>\n",
              "      <td>1413</td>\n",
              "      <td>2015</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>158124</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>261648242</td>\n",
              "      <td>1.077</td>\n",
              "      <td>14</td>\n",
              "      <td>0.636</td>\n",
              "      <td>26</td>\n",
              "      <td>173</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>158358</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>236488582</td>\n",
              "      <td>4.667</td>\n",
              "      <td>2</td>\n",
              "      <td>0.351</td>\n",
              "      <td>6</td>\n",
              "      <td>487</td>\n",
              "      <td>2018</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>157864</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2589382</td>\n",
              "      <td>1.867</td>\n",
              "      <td>4</td>\n",
              "      <td>0.255</td>\n",
              "      <td>15</td>\n",
              "      <td>3172</td>\n",
              "      <td>2010</td>\n",
              "      <td>42</td>\n",
              "      <td>6</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>154873</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>238665702</td>\n",
              "      <td>28.000</td>\n",
              "      <td>29</td>\n",
              "      <td>7.410</td>\n",
              "      <td>0</td>\n",
              "      <td>468</td>\n",
              "      <td>2018</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>156448</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        riid  retention_score  ...  reward  optimal_action\n",
              "0  193066422           28.000  ...      -2               1\n",
              "1  261648242            1.077  ...     -27               0\n",
              "2  236488582            4.667  ...      -7               0\n",
              "3    2589382            1.867  ...     -16               0\n",
              "4  238665702           28.000  ...      -1               1\n",
              "\n",
              "[5 rows x 40 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] Sample Validation Data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>riid</th>\n",
              "      <th>retention_score</th>\n",
              "      <th>frequency_score</th>\n",
              "      <th>recency_score</th>\n",
              "      <th>sends_since_last_open</th>\n",
              "      <th>days_subscr</th>\n",
              "      <th>aq_year</th>\n",
              "      <th>aq_week</th>\n",
              "      <th>aq_dayofweek</th>\n",
              "      <th>aq_period</th>\n",
              "      <th>campaign_id</th>\n",
              "      <th>campaign_category</th>\n",
              "      <th>campaign_Brand</th>\n",
              "      <th>campaign_Core</th>\n",
              "      <th>campaign_Dedicated</th>\n",
              "      <th>campaign_InnovationSpotlight</th>\n",
              "      <th>campaign_NewArrivals</th>\n",
              "      <th>campaign_ProductSpotlight</th>\n",
              "      <th>campaign_Replen</th>\n",
              "      <th>campaign_Tops</th>\n",
              "      <th>campaign_Trend</th>\n",
              "      <th>campaign_Other</th>\n",
              "      <th>discount</th>\n",
              "      <th>promo</th>\n",
              "      <th>sale</th>\n",
              "      <th>is_one_for_free</th>\n",
              "      <th>free_shipping</th>\n",
              "      <th>is_exclusive</th>\n",
              "      <th>has_urgency</th>\n",
              "      <th>sl_contains_price</th>\n",
              "      <th>is_discount_mentioned</th>\n",
              "      <th>message_size</th>\n",
              "      <th>sent_week</th>\n",
              "      <th>sent_dayofweek</th>\n",
              "      <th>sent_hr</th>\n",
              "      <th>opened</th>\n",
              "      <th>unsub</th>\n",
              "      <th>rev_3dv2</th>\n",
              "      <th>reward</th>\n",
              "      <th>optimal_action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>184980282</td>\n",
              "      <td>28.000</td>\n",
              "      <td>36</td>\n",
              "      <td>4.705</td>\n",
              "      <td>0</td>\n",
              "      <td>1474</td>\n",
              "      <td>2015</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>155587</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>156790362</td>\n",
              "      <td>14.000</td>\n",
              "      <td>5</td>\n",
              "      <td>2.982</td>\n",
              "      <td>2</td>\n",
              "      <td>1474</td>\n",
              "      <td>2015</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>155511</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>253723882</td>\n",
              "      <td>28.000</td>\n",
              "      <td>81</td>\n",
              "      <td>12.276</td>\n",
              "      <td>0</td>\n",
              "      <td>301</td>\n",
              "      <td>2018</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>156725</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8735422</td>\n",
              "      <td>28.000</td>\n",
              "      <td>79</td>\n",
              "      <td>12.276</td>\n",
              "      <td>0</td>\n",
              "      <td>1940</td>\n",
              "      <td>2014</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>155137</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>181205722</td>\n",
              "      <td>14.000</td>\n",
              "      <td>49</td>\n",
              "      <td>5.056</td>\n",
              "      <td>2</td>\n",
              "      <td>1474</td>\n",
              "      <td>2015</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "      <td>Non-Holiday</td>\n",
              "      <td>59090182</td>\n",
              "      <td>Tops</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>155094</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        riid  retention_score  ...  reward  optimal_action\n",
              "0  184980282           28.000  ...      -1               0\n",
              "1  156790362           14.000  ...      -3               0\n",
              "2  253723882           28.000  ...      -1               1\n",
              "3    8735422           28.000  ...      -1               1\n",
              "4  181205722           14.000  ...      -3               0\n",
              "\n",
              "[5 rows x 40 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] The full list of column names include:\n",
            "['riid',\n",
            " 'retention_score',\n",
            " 'frequency_score',\n",
            " 'recency_score',\n",
            " 'sends_since_last_open',\n",
            " 'days_subscr',\n",
            " 'aq_year',\n",
            " 'aq_week',\n",
            " 'aq_dayofweek',\n",
            " 'aq_period',\n",
            " 'campaign_id',\n",
            " 'campaign_category',\n",
            " 'campaign_Brand',\n",
            " 'campaign_Core',\n",
            " 'campaign_Dedicated',\n",
            " 'campaign_InnovationSpotlight',\n",
            " 'campaign_NewArrivals',\n",
            " 'campaign_ProductSpotlight',\n",
            " 'campaign_Replen',\n",
            " 'campaign_Tops',\n",
            " 'campaign_Trend',\n",
            " 'campaign_Other',\n",
            " 'discount',\n",
            " 'promo',\n",
            " 'sale',\n",
            " 'is_one_for_free',\n",
            " 'free_shipping',\n",
            " 'is_exclusive',\n",
            " 'has_urgency',\n",
            " 'sl_contains_price',\n",
            " 'is_discount_mentioned',\n",
            " 'message_size',\n",
            " 'sent_week',\n",
            " 'sent_dayofweek',\n",
            " 'sent_hr',\n",
            " 'opened',\n",
            " 'unsub',\n",
            " 'rev_3dv2',\n",
            " 'reward',\n",
            " 'optimal_action']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QTnk1mZnIxKe",
        "outputId": "649d18b5-8ab0-4718-deeb-1effc5895662"
      },
      "source": [
        "#Setting overwrite flag for Processed Data\n",
        "overwrite_processed = False\n",
        "\n",
        "#Create a folders to contain the processed records\n",
        "dest_folder = Path(\"/content/drive/MyDrive/Bandit_Project/aleksey/processed\")\n",
        "dest_folder.mkdir(exist_ok=True)\n",
        "\n",
        "#Build the context that you would like to keep track of\n",
        "user_context_cols = ['riid', 'retention_score', 'frequency_score', 'recency_score', 'sends_since_last_open'] #Not using any aquisition features\n",
        "\n",
        "campaign_context_cols = ['campaign_category', 'discount', 'promo', 'sale', 'is_one_for_free','free_shipping','is_exclusive','has_urgency']\n",
        "\n",
        "email_context_cols = ['sl_contains_price','is_discount_mentioned','sent_week','sent_dayofweek','sent_hr']\n",
        "\n",
        "context_cols = user_context_cols + campaign_context_cols + email_context_cols\n",
        "print(f\"[INFO] The context is {len(context_cols)} cols long...\")\n",
        "\n",
        "#Not sure how to handle train_y: For now, optimal_action classification \n",
        "outcomes_cols = [\"opened\", \"unsub\", \"rev_3dv2\"]\n",
        "reward_cols = [\"reward\"]\n",
        "action_cols = [\"optimal_action\"]\n",
        "\n",
        "#Process the files in both train & valid\n",
        "train_file_path = process_input(train_folder, dest_folder, context_cols, action_cols, overwrite_processed)\n",
        "val_file_path = process_input(valid_folder, dest_folder, context_cols, action_cols, overwrite_processed)\n",
        "\n",
        "#Print the heads of the training & validation data\n",
        "train = pd.read_csv(train_file_path)\n",
        "val = pd.read_csv(val_file_path)\n",
        "\n",
        "print(f\"\\n[INFO] We have {len(train)} elements in the Training Set\")\n",
        "print(\"[INFO] Printing the head of Training Set:\")\n",
        "train.head(5)\n",
        "print(f\"\\n[INFO] We have {len(val)} elements in the Validation Set\")\n",
        "print(\"[INFO] Printing the head of Validation Set:\")\n",
        "val.head(5)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] The context is 18 cols long...\n",
            "[INFO] Currently working on train: sends_2019_wk27.csv\n",
            "[INFO] Currently working on train: sends_2019_wk26.csv\n",
            "[INFO] Currently working on train: sends_2019_wk28.csv\n",
            "[INFO] Currently working on train: sends_2019_wk29.csv\n",
            "[INFO] Currently working on train: sends_2019_wk31.csv\n",
            "[INFO] Currently working on train: sends_2019_wk32.csv\n",
            "[INFO] Currently working on train: sends_2019_wk33.csv\n",
            "[INFO] Currently working on train: sends_2019_wk30.csv\n",
            "[INFO] Currently working on train: sends_2019_wk34.csv\n",
            "[INFO] Currently working on train: sends_2019_wk35.csv\n",
            "[INFO] Currently working on train: sends_2019_wk36.csv\n",
            "[INFO] Currently working on train: sends_2019_wk37.csv\n",
            "[INFO] Currently working on train: sends_2019_wk38.csv\n",
            "[INFO] Currently working on train: sends_2019_wk39.csv\n",
            "[INFO] Currently working on train: sends_2019_wk40.csv\n",
            "[INFO] Currently working on train: sends_2019_wk41.csv\n",
            "[INFO] Currently working on train: sends_2019_wk42.csv\n",
            "[INFO] Currently working on train: sends_2019_wk43.csv\n",
            "[INFO] Currently working on train: sends_2019_wk44.csv\n",
            "[INFO] Currently working on train: sends_2019_wk45.csv\n",
            "[INFO] Currently working on train: sends_2019_wk46.csv\n",
            "[INFO] Currently working on train: sends_2019_wk47.csv\n",
            "[INFO] Currently working on train: sends_2019_wk48.csv\n",
            "[INFO] Currently working on train: sends_2019_wk49.csv\n",
            "[INFO] Currently working on train: sends_2019_wk50.csv\n",
            "[INFO] Currently working on train: sends_2019_wk51.csv\n",
            "[INFO] Currently working on train: sends_2019_wk52.csv\n",
            "[INFO] Currently working on train: sends_2020_wk00.csv\n",
            "[INFO] Currently working on train: sends_2020_wk01.csv\n",
            "[INFO] Currently working on train: sends_2020_wk02.csv\n",
            "[INFO] Currently working on train: sends_2020_wk03.csv\n",
            "[INFO] Currently working on train: sends_2020_wk04.csv\n",
            "[INFO] Currently working on train: sends_2020_wk05.csv\n",
            "[INFO] Currently working on train: sends_2020_wk06.csv\n",
            "[INFO] Currently working on train: sends_2020_wk07.csv\n",
            "[INFO] Currently working on train: sends_2020_wk08.csv\n",
            "[INFO] Currently working on train: sends_2020_wk09.csv\n",
            "[INFO] Currently working on train: sends_2020_wk10.csv\n",
            "[INFO] Currently working on train: sends_2020_wk11.csv\n",
            "[INFO] Currently working on train: sends_2020_wk12.csv\n",
            "[INFO] Currently working on train: sends_2020_wk13.csv\n",
            "[INFO] Currently working on train: sends_2020_wk14.csv\n",
            "[INFO] Currently working on train: sends_2020_wk15.csv\n",
            "[INFO] Currently working on train: sends_2020_wk16.csv\n",
            "[INFO] Currently working on train: sends_2020_wk17.csv\n",
            "[INFO] Currently working on train: sends_2020_wk18.csv\n",
            "[INFO] Currently working on train: sends_2020_wk19.csv\n",
            "[INFO] Currently working on train: sends_2020_wk20.csv\n",
            "[INFO] Currently working on train: sends_2020_wk21.csv\n",
            "[INFO] Currently working on train: sends_2020_wk22.csv\n",
            "[INFO] Currently working on train: sends_2020_wk23.csv\n",
            "[INFO] Currently working on train: sends_2020_wk24.csv\n",
            "[INFO] Currently working on train: sends_2020_wk25.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk26.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk27.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk28.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk30.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk29.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk31.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk32.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk33.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk34.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk35.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk36.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk37.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk38.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk39.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk40.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk41.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk42.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk43.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk44.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk45.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk46.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk47.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk48.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk49.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk50.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk51.csv\n",
            "[INFO] Currently working on valid: sends_2019_wk52.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk00.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk01.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk02.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk03.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk04.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk05.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk06.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk07.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk08.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk09.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk10.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk11.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk12.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk13.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk14.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk15.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk16.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk17.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk18.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk19.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk20.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk21.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk22.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk23.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk24.csv\n",
            "[INFO] Currently working on valid: sends_2020_wk25.csv\n",
            "\n",
            "[INFO] We have 704811 elements in the Training Set\n",
            "[INFO] Printing the head of Training Set:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>riid</th>\n",
              "      <th>retention_score</th>\n",
              "      <th>frequency_score</th>\n",
              "      <th>recency_score</th>\n",
              "      <th>sends_since_last_open</th>\n",
              "      <th>campaign_category</th>\n",
              "      <th>discount</th>\n",
              "      <th>promo</th>\n",
              "      <th>sale</th>\n",
              "      <th>is_one_for_free</th>\n",
              "      <th>free_shipping</th>\n",
              "      <th>is_exclusive</th>\n",
              "      <th>has_urgency</th>\n",
              "      <th>sl_contains_price</th>\n",
              "      <th>is_discount_mentioned</th>\n",
              "      <th>sent_week</th>\n",
              "      <th>sent_dayofweek</th>\n",
              "      <th>sent_hr</th>\n",
              "      <th>optimal_action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>259865082</td>\n",
              "      <td>9.333</td>\n",
              "      <td>18</td>\n",
              "      <td>1.555</td>\n",
              "      <td>3</td>\n",
              "      <td>Trend</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7138122</td>\n",
              "      <td>28.000</td>\n",
              "      <td>25</td>\n",
              "      <td>7.071</td>\n",
              "      <td>0</td>\n",
              "      <td>Trend</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>223293342</td>\n",
              "      <td>0.737</td>\n",
              "      <td>1</td>\n",
              "      <td>0.022</td>\n",
              "      <td>38</td>\n",
              "      <td>Trend</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>211545802</td>\n",
              "      <td>0.824</td>\n",
              "      <td>1</td>\n",
              "      <td>0.130</td>\n",
              "      <td>34</td>\n",
              "      <td>Trend</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>163348802</td>\n",
              "      <td>28.000</td>\n",
              "      <td>31</td>\n",
              "      <td>7.040</td>\n",
              "      <td>1</td>\n",
              "      <td>Trend</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        riid  retention_score  ...  sent_hr  optimal_action\n",
              "0  259865082            9.333  ...       21               1\n",
              "1    7138122           28.000  ...       21               1\n",
              "2  223293342            0.737  ...       21               0\n",
              "3  211545802            0.824  ...       21               0\n",
              "4  163348802           28.000  ...       21               0\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[INFO] We have 212245 elements in the Validation Set\n",
            "[INFO] Printing the head of Validation Set:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>riid</th>\n",
              "      <th>retention_score</th>\n",
              "      <th>frequency_score</th>\n",
              "      <th>recency_score</th>\n",
              "      <th>sends_since_last_open</th>\n",
              "      <th>campaign_category</th>\n",
              "      <th>discount</th>\n",
              "      <th>promo</th>\n",
              "      <th>sale</th>\n",
              "      <th>is_one_for_free</th>\n",
              "      <th>free_shipping</th>\n",
              "      <th>is_exclusive</th>\n",
              "      <th>has_urgency</th>\n",
              "      <th>sl_contains_price</th>\n",
              "      <th>is_discount_mentioned</th>\n",
              "      <th>sent_week</th>\n",
              "      <th>sent_dayofweek</th>\n",
              "      <th>sent_hr</th>\n",
              "      <th>optimal_action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>184980282</td>\n",
              "      <td>28.000</td>\n",
              "      <td>36</td>\n",
              "      <td>4.705</td>\n",
              "      <td>0</td>\n",
              "      <td>Tops</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>156790362</td>\n",
              "      <td>14.000</td>\n",
              "      <td>5</td>\n",
              "      <td>2.982</td>\n",
              "      <td>2</td>\n",
              "      <td>Tops</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>253723882</td>\n",
              "      <td>28.000</td>\n",
              "      <td>81</td>\n",
              "      <td>12.276</td>\n",
              "      <td>0</td>\n",
              "      <td>Tops</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8735422</td>\n",
              "      <td>28.000</td>\n",
              "      <td>79</td>\n",
              "      <td>12.276</td>\n",
              "      <td>0</td>\n",
              "      <td>Tops</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>181205722</td>\n",
              "      <td>14.000</td>\n",
              "      <td>49</td>\n",
              "      <td>5.056</td>\n",
              "      <td>2</td>\n",
              "      <td>Tops</td>\n",
              "      <td>40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        riid  retention_score  ...  sent_hr  optimal_action\n",
              "0  184980282           28.000  ...       17               0\n",
              "1  156790362           14.000  ...       17               0\n",
              "2  253723882           28.000  ...       17               1\n",
              "3    8735422           28.000  ...       17               1\n",
              "4  181205722           14.000  ...       17               0\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ennp6DLmgNs"
      },
      "source": [
        "# Statistical Analysis Of Outcomes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dBKxRgBpjy5"
      },
      "source": [
        "[DONE] In this section, we have confirmed 3 things:\n",
        "\n",
        "\n",
        "1.   How many people opened vs did not open\n",
        "2.   How many people unsubscribed vs did not unsubscribe\n",
        "3.   Whether optimal action is 1 for open OR purchase and 0 for not open OR unsub\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "xuhMtUMFp8-I",
        "outputId": "f37392f6-cfd1-4ac1-93e4-f10043f9cfc1"
      },
      "source": [
        "\"\"\"\n",
        "length_l = []\n",
        "opens_l = []\n",
        "unsubs_l = []\n",
        "unique_users_s = set()\n",
        "campaign_types_s = set()\n",
        "\n",
        "#Iterate through the weekly data\n",
        "for file_path in weekly_data_path_l:\n",
        "\n",
        "  #Print status update\n",
        "  print(f\"[INFO] Working on {file_path.name}\")\n",
        "  \n",
        "  #Count the number of elements\n",
        "  df = pd.read_csv(file_path)\n",
        "  length = len(df)\n",
        "  length_l.append(length)\n",
        "\n",
        "  #Count Opens vs Not Opens\n",
        "  opens = (df[\"opened\"] == 1).astype(int).sum()\n",
        "  opens_l.append(opens)\n",
        "\n",
        "  #Count Unsubs vs Not Unsubs\n",
        "  unsubs = (df[\"unsub\"] == 1).astype(int).sum()\n",
        "  unsubs_l.append(unsubs)\n",
        "\n",
        "  #Assert whether optimal action follows the rule\n",
        "  #Create a Panda Series that follows this rule and assert\n",
        "  check_series = pd.Series(np.ones_like(df[\"optimal_action\"].values, dtype=int))\n",
        "\n",
        "  check_series[df[\"opened\"] == 0] = 0\n",
        "  check_series[df[\"unsub\"] == 1] = 0\n",
        "  assert (df[\"optimal_action\"] == check_series).all()\n",
        "\n",
        "  #Update the unique user set with user IDs from the df\n",
        "  unique_users = list(df[\"riid\"].unique())\n",
        "  unique_users_s.update(unique_users)\n",
        "\n",
        "  #Update the unique campaign type set from the df\n",
        "  campaign_types = list(df['campaign_category'].unique())\n",
        "  campaign_types_s.update(campaign_types)\n",
        "\n",
        "total_length = sum(length_l)\n",
        "print(f\"[INFO] Total number of data - {total_length}\")\n",
        "\n",
        "opened = sum(opens_l)/total_length\n",
        "unsub = sum(unsubs_l)/total_length\n",
        "\n",
        "print(f\"[INFO] % of opened - {opened}\")\n",
        "print(f\"[INFO] % of unsubscribed - {unsub}\")\n",
        "print(f\"[INFO] # of unique users - {len(unique_users_s)}\")\n",
        "print(f\"[INFO] # of unique campaign types - {len(campaign_types_s)}\")\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nlength_l = []\\nopens_l = []\\nunsubs_l = []\\nunique_users_s = set()\\ncampaign_types_s = set()\\n\\n#Iterate through the weekly data\\nfor file_path in weekly_data_path_l:\\n\\n  #Print status update\\n  print(f\"[INFO] Working on {file_path.name}\")\\n  \\n  #Count the number of elements\\n  df = pd.read_csv(file_path)\\n  length = len(df)\\n  length_l.append(length)\\n\\n  #Count Opens vs Not Opens\\n  opens = (df[\"opened\"] == 1).astype(int).sum()\\n  opens_l.append(opens)\\n\\n  #Count Unsubs vs Not Unsubs\\n  unsubs = (df[\"unsub\"] == 1).astype(int).sum()\\n  unsubs_l.append(unsubs)\\n\\n  #Assert whether optimal action follows the rule\\n  #Create a Panda Series that follows this rule and assert\\n  check_series = pd.Series(np.ones_like(df[\"optimal_action\"].values, dtype=int))\\n\\n  check_series[df[\"opened\"] == 0] = 0\\n  check_series[df[\"unsub\"] == 1] = 0\\n  assert (df[\"optimal_action\"] == check_series).all()\\n\\n  #Update the unique user set with user IDs from the df\\n  unique_users = list(df[\"riid\"].unique())\\n  unique_users_s.update(unique_users)\\n\\n  #Update the unique campaign type set from the df\\n  campaign_types = list(df[\\'campaign_category\\'].unique())\\n  campaign_types_s.update(campaign_types)\\n\\ntotal_length = sum(length_l)\\nprint(f\"[INFO] Total number of data - {total_length}\")\\n\\nopened = sum(opens_l)/total_length\\nunsub = sum(unsubs_l)/total_length\\n\\nprint(f\"[INFO] % of opened - {opened}\")\\nprint(f\"[INFO] % of unsubscribed - {unsub}\")\\nprint(f\"[INFO] # of unique users - {len(unique_users_s)}\")\\nprint(f\"[INFO] # of unique campaign types - {len(campaign_types_s)}\")\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcoRpQ7bAF2a"
      },
      "source": [
        "# Preparing the Dataset for Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFaTHGQwxgls"
      },
      "source": [
        "## Creating the Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2BSe0utxpvq"
      },
      "source": [
        "#Given a dataframe in memory\n",
        "def df_to_dataloader(dataframe, target, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop(target)\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rmvg6BBIMug"
      },
      "source": [
        "def set_shape(value):\n",
        "    value.set_shape((17, ))\n",
        "    return value"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "ysesaBYW--TX",
        "outputId": "50952997-096a-4117-c299-ffea57e5e678"
      },
      "source": [
        "\"\"\"\n",
        "def input_fn(df_data, target, num_epochs = 50, shuffle = True, batch_size = 32):\n",
        "  \n",
        "  #df_data = pd.read_csv(data_file, header=[0], skiprows=1)\n",
        "  # remove NaN elements\n",
        "  #df_data = df_data.dropna(how=\"any\", axis=0)\n",
        "  df_data = df_data.copy()\n",
        "  labels = df_data.pop(target)\n",
        "  return tf.compat.v1.estimator.inputs.pandas_input_fn(\n",
        "      x=df_data,\n",
        "      y=labels,\n",
        "      batch_size=batch_size,\n",
        "      num_epochs=num_epochs,\n",
        "      shuffle=shuffle)\n",
        "\"\"\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef input_fn(df_data, target, num_epochs = 50, shuffle = True, batch_size = 32):\\n  \\n  #df_data = pd.read_csv(data_file, header=[0], skiprows=1)\\n  # remove NaN elements\\n  #df_data = df_data.dropna(how=\"any\", axis=0)\\n  df_data = df_data.copy()\\n  labels = df_data.pop(target)\\n  return tf.compat.v1.estimator.inputs.pandas_input_fn(\\n      x=df_data,\\n      y=labels,\\n      batch_size=batch_size,\\n      num_epochs=num_epochs,\\n      shuffle=shuffle)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPDdpqjh6ZYb"
      },
      "source": [
        "batch_size=512\n",
        "\n",
        "#train_dl = input_fn(train, \"optimal_action\", batch_size=batch_size)\n",
        "#val_dl = input_fn(val, \"optimal_action\", shuffle=False, batch_size=batch_size)\n",
        "train_dl = lambda : df_to_dataloader(train, \"optimal_action\", batch_size=batch_size)\n",
        "#train_dl = train_dl.map(set_shape).batch(batch_size)\n",
        "\n",
        "val_dl = lambda : df_to_dataloader(val, \"optimal_action\", shuffle=False, batch_size=batch_size)\n",
        "#val_dl = val_dl.map(set_shape).batch(batch_size)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iU4WWkdVh99"
      },
      "source": [
        "## Base Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oos9W28ePJqH"
      },
      "source": [
        "### Numeric Columns\n",
        "\n",
        "1. 'retention_score'\n",
        "2. 'frequency_score'\n",
        "3. 'recency_score'\n",
        "4. 'sends_since_last_open'\n",
        "5. 'discount'\n",
        "6. 'sent_week'\n",
        "7. 'sent_dayofweek'\n",
        "8. 'sent_hr'\n",
        "\n",
        "To bucketize or not?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFSVOXh0Vs59"
      },
      "source": [
        "#Get the global mean & std statistics\n",
        "rolling_stats_filepath = Path(\"/content/drive/MyDrive/Bandit_Project/rolling_statistics.pkl\")\n",
        "with rolling_stats_filepath.open(mode='rb') as rolling_stats_file:\n",
        "  rolling_stats = pkl.load(rolling_stats_file)\n",
        "\n",
        "#Create a function that standardizes the column to N(0, 1)\n",
        "def standardize_column(data, mean, std):                       \n",
        "  data = (tf.cast(data, dtype=tf.float32) - mean)/std\n",
        "  return tf.reshape(data, [-1, 1])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg29RlFLhLRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281eebad-0ca2-421f-cfe0-576d26c9619f"
      },
      "source": [
        "#Initialize a list to contain the numeric feature columns\n",
        "numeric_feature_layer = []\n",
        "numeric_feature_layer_input = {}\n",
        "\n",
        "#Create 2 dictionaries with key as numeric feature column name\n",
        "#and val as the value of the numeric feature column name\n",
        "numeric_feature_col_names = ['retention_score','recency_score','frequency_score',\n",
        "                             'sent_week','sent_dayofweek','sent_hr','discount',\n",
        "                             'sends_since_last_open']\n",
        "MEANS = {feature: rolling_stats[feature][\"mean\"] for feature in numeric_feature_col_names}\n",
        "STDS = {feature: rolling_stats[feature][\"std\"] for feature in numeric_feature_col_names}\n",
        "\n",
        "#Generate numeric cols\n",
        "for feature in numeric_feature_col_names:\n",
        "  numeric_feature_col = tf.feature_column.numeric_column(feature, \n",
        "                                                     normalizer_fn=partial(standardize_column, mean=MEANS[feature], std=STDS[feature]))\n",
        "  numeric_feature_layer.append(numeric_feature_col)\n",
        "  numeric_feature_layer_input[feature] = tf.keras.Input(shape=(1,), name=feature)\n",
        "\n",
        "#Display the columns\n",
        "print(\"[INFO] The numeric feature columns are:\")\n",
        "pprint(numeric_feature_layer)\n",
        "\n",
        "print(\"\\n[INFO] The inputs to numeric feature columns are:\")\n",
        "pprint(numeric_feature_layer_input)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] The numeric feature columns are:\n",
            "[NumericColumn(key='retention_score', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f25d982ab00>, mean=11.467980895825553, std=11.35391986430546)),\n",
            " NumericColumn(key='recency_score', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f25d982ab00>, mean=1.23904221901564, std=2.216794122042123)),\n",
            " NumericColumn(key='frequency_score', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f25d982ab00>, mean=14.977138288600283, std=20.754428265423773)),\n",
            " NumericColumn(key='sent_week', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f25d982ab00>, mean=28.594960628048973, std=14.377041994557581)),\n",
            " NumericColumn(key='sent_dayofweek', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f25d982ab00>, mean=2.638369149867888, std=2.163983074344224)),\n",
            " NumericColumn(key='sent_hr', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f25d982ab00>, mean=17.291778162586947, std=3.9303658889007997)),\n",
            " NumericColumn(key='discount', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f25d982ab00>, mean=11.300244227332426, std=19.60056341212265)),\n",
            " NumericColumn(key='sends_since_last_open', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function standardize_column at 0x7f25d982ab00>, mean=11.36886785974351, std=14.997835900332081))]\n",
            "\n",
            "[INFO] The inputs to numeric feature columns are:\n",
            "{'discount': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'discount')>,\n",
            " 'frequency_score': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'frequency_score')>,\n",
            " 'recency_score': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'recency_score')>,\n",
            " 'retention_score': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'retention_score')>,\n",
            " 'sends_since_last_open': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'sends_since_last_open')>,\n",
            " 'sent_dayofweek': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'sent_dayofweek')>,\n",
            " 'sent_hr': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'sent_hr')>,\n",
            " 'sent_week': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'sent_week')>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK-7fdg8VXLK"
      },
      "source": [
        "### Categorical Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yioe3gpreW65"
      },
      "source": [
        "1. 'riid'\n",
        "2. 'campaign_category'\n",
        "3. 'promo'\n",
        "4. 'sale'\n",
        "5. 'is_one_for_free'\n",
        "6. 'free_shipping'\n",
        "7. 'is_exclusive'\n",
        "8. 'has_urgency'\n",
        "9. 'sl_contains_price'\n",
        "10.'is_discount_mentioned'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASAhKBYMktBu"
      },
      "source": [
        "#Initialize a list to contain the categorical feature columns\n",
        "categorical_feature_layer = []\n",
        "categorical_feature_layer_input = {}\n",
        "\n",
        "CATEGORIES = {\n",
        "    'promo' : [0, 1],\n",
        "    'sale' : [0, 1],\n",
        "    'campaign_category': ['Trend', 'NewArrivals', 'Dedicated', 'InnovationSpotlight', 'Core', 'Replen', 'ProductSpotlight', 'Other', 'Brand', 'Tops'],\n",
        "    'is_one_for_free': [0, 1],\n",
        "    'free_shipping': [0, 1],\n",
        "    'is_exclusive': [0, 1],\n",
        "    'has_urgency': [0, 1],\n",
        "    'sl_contains_price': [0, 1],\n",
        "    'is_discount_mentioned': [0, 1],\n",
        "}\n",
        "\n",
        "#Generate categorical cols\n",
        "for (feature, vocab) in CATEGORIES.items():\n",
        "  categorical_feature_col = tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list(feature, vocab))\n",
        "  categorical_feature_layer.append(categorical_feature_col)\n",
        "  categorical_feature_layer_input[feature] = tf.keras.Input(shape=(1,), name=feature)\n",
        "\n",
        "#Create the riid categorical col\n",
        "riid = tf.feature_column.categorical_column_with_hash_bucket(\"riid\", hash_bucket_size=2000000, dtype=tf.int64)\n",
        "#categorical_feature_layer.append(riid)\n",
        "#categorical_feature_layer_input[\"riid\"] = tf.keras.Input(shape=(1,), name=\"riid\")"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xu73TFwVnET"
      },
      "source": [
        "## For Wide Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlFehJ-ZVbbn"
      },
      "source": [
        "### Crossed Columns\n",
        "\n",
        "Crossing the following feature-combinations:\n",
        "\n",
        "1.   'riid' vs. 'campaign_category'\n",
        "2.   'riid' vs. 'discount'\n",
        "3.   'riid' vs. 'is_one_for_free'\n",
        "4.   'riid' vs. 'free_shipping'\n",
        "5.   'riid' vs. 'is_exclusive'\n",
        "6.   'riid' vs. 'has_urgency',\n",
        "7.   'riid' vs. 'sl_contains_price',\n",
        "8. 'riid' vs. 'is_discount_mentioned',\n",
        "9. 'riid' vs. 'sent_week',\n",
        "10. 'riid' vs. 'sent_dayofweek',\n",
        "11. 'riid' vs. 'sent_hr'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y527yMOStXRa"
      },
      "source": [
        "crossed_columns = [\n",
        "  tf.feature_column.crossed_column([\"riid\", 'campaign_category'], hash_bucket_size=20000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'discount'], hash_bucket_size=10000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'is_one_for_free'], hash_bucket_size=4000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'free_shipping'], hash_bucket_size=4000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'is_exclusive'], hash_bucket_size=4000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'has_urgency'], hash_bucket_size=4000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'sl_contains_price'], hash_bucket_size=4000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'is_discount_mentioned'], hash_bucket_size=4000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'sent_week'], hash_bucket_size=10000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'sent_dayofweek'], hash_bucket_size=60000000),\n",
        "  tf.feature_column.crossed_column([\"riid\", 'sent_hr'], hash_bucket_size=50000000),\n",
        "]\n",
        "wide_columns = numeric_feature_layer + crossed_columns"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOwuJPTiVplF"
      },
      "source": [
        "## For Deep Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szxxbbWfvMaC"
      },
      "source": [
        "deep_columns = numeric_feature_layer + categorical_feature_layer\n",
        "deep_columns.append(tf.feature_column.embedding_column(riid, dimension=20))"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whQg0MKDyqZ-"
      },
      "source": [
        "# Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQWxDAhCyZG1"
      },
      "source": [
        "models_dir = Path(\"/content/drive/MyDrive/Bandit_Project/models\")\n",
        "wmodel = (models_dir/\"Wide\").mkdir(exist_ok=True)\n",
        "dmodel = (models_dir/\"Deep\").mkdir(exist_ok=True)\n",
        "wdmodel = (models_dir/\"W&D\").mkdir(exist_ok=True)\n",
        "\n",
        "#Hyperparameters\n",
        "n_epochs=25\n",
        "lr = 1e-3\n",
        "n_steps = int(n_epochs * len(train) / batch_size)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjxXAMxOyx-t"
      },
      "source": [
        "## Wide Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6ukiLoh_xh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2476da53-eccc-4c13-d7d3-75ec71109897"
      },
      "source": [
        "wm = tf.estimator.LinearClassifier(\n",
        "    model_dir=wmodel, \n",
        "    feature_columns=wide_columns,\n",
        "    n_classes=2,\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate = lr))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpmezi3063\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpmezi3063', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHoaO7tZ6Ryl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf5fb548-a562-4df7-cced-c550a86f5e56"
      },
      "source": [
        "wm.train(train_dl, steps=n_steps)\n",
        "wm_results = wm.evaluate(val_dl, steps=None)\n",
        "for key in sorted(wm_results):\n",
        "  print(\"%s: %s\" % (key, wm_results[key]))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.84693164\n",
            "accuracy_baseline: 0.795835\n",
            "auc: 0.83979183\n",
            "auc_precision_recall: 0.63070023\n",
            "average_loss: 0.39133254\n",
            "global_step: 1377\n",
            "label/mean: 0.204165\n",
            "loss: 0.39142895\n",
            "precision: 0.66245246\n",
            "prediction/mean: 0.28447124\n",
            "recall: 0.51028085\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVgfMuCHyy_i"
      },
      "source": [
        "## Deep Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkviWgqOxZ_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1e27a78-805a-41d9-9f80-02b7dcf11eed"
      },
      "source": [
        "dm = tf.estimator.DNNClassifier(\n",
        "    hidden_units=[512, 256, 128], \n",
        "    feature_columns=deep_columns, \n",
        "    model_dir=dmodel, \n",
        "    n_classes=2,\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate = lr)\n",
        ")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpdp8gqx52\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpdp8gqx52', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeKiv4Tp6Se6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95725ac8-8251-4e6f-cd30-bc10ea78a33e"
      },
      "source": [
        "dm.train(train_dl, steps=n_steps)\n",
        "dm_results = dm.evaluate(val_dl, steps=None)\n",
        "for key in sorted(dm_results):\n",
        "  print(\"%s: %s\" % (key, dm_results[key]))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpdp8gqx52/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
            "INFO:tensorflow:loss = 0.6626018, step = 0\n",
            "INFO:tensorflow:global_step/sec: 34.8699\n",
            "INFO:tensorflow:loss = 0.3518513, step = 100 (2.870 sec)\n",
            "INFO:tensorflow:global_step/sec: 36.3829\n",
            "INFO:tensorflow:loss = 0.32728338, step = 200 (2.745 sec)\n",
            "INFO:tensorflow:global_step/sec: 35.4342\n",
            "INFO:tensorflow:loss = 0.3174025, step = 300 (2.825 sec)\n",
            "INFO:tensorflow:global_step/sec: 36.612\n",
            "INFO:tensorflow:loss = 0.31972384, step = 400 (2.732 sec)\n",
            "INFO:tensorflow:global_step/sec: 36.8281\n",
            "INFO:tensorflow:loss = 0.34913072, step = 500 (2.717 sec)\n",
            "INFO:tensorflow:global_step/sec: 35.9367\n",
            "INFO:tensorflow:loss = 0.3545159, step = 600 (2.779 sec)\n",
            "INFO:tensorflow:global_step/sec: 35.8044\n",
            "INFO:tensorflow:loss = 0.3267228, step = 700 (2.792 sec)\n",
            "INFO:tensorflow:global_step/sec: 35.3118\n",
            "INFO:tensorflow:loss = 0.37021118, step = 800 (2.835 sec)\n",
            "INFO:tensorflow:global_step/sec: 37.2799\n",
            "INFO:tensorflow:loss = 0.3507153, step = 900 (2.681 sec)\n",
            "INFO:tensorflow:global_step/sec: 37.5058\n",
            "INFO:tensorflow:loss = 0.38592732, step = 1000 (2.667 sec)\n",
            "INFO:tensorflow:global_step/sec: 37.1528\n",
            "INFO:tensorflow:loss = 0.3339947, step = 1100 (2.692 sec)\n",
            "INFO:tensorflow:global_step/sec: 36.3316\n",
            "INFO:tensorflow:loss = 0.34031588, step = 1200 (2.754 sec)\n",
            "INFO:tensorflow:global_step/sec: 34.8457\n",
            "INFO:tensorflow:loss = 0.36950767, step = 1300 (2.870 sec)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1377...\n",
            "INFO:tensorflow:Saving checkpoints for 1377 into /tmp/tmpdp8gqx52/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1377...\n",
            "INFO:tensorflow:Loss for final step: 0.4610357.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifierV2 at 0x7f22c2b73c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-03-01T17:24:26Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmpdp8gqx52/model.ckpt-1377\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Inference Time : 6.52355s\n",
            "INFO:tensorflow:Finished evaluation at 2021-03-01-17:24:33\n",
            "INFO:tensorflow:Saving dict for global step 1377: accuracy = 0.8544229, accuracy_baseline = 0.795835, auc = 0.85163444, auc_precision_recall = 0.66007787, average_loss = 0.35322976, global_step = 1377, label/mean = 0.204165, loss = 0.3533473, precision = 0.71287704, prediction/mean = 0.21013147, recall = 0.4804883\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1377: /tmp/tmpdp8gqx52/model.ckpt-1377\n",
            "accuracy: 0.8544229\n",
            "accuracy_baseline: 0.795835\n",
            "auc: 0.85163444\n",
            "auc_precision_recall: 0.66007787\n",
            "average_loss: 0.35322976\n",
            "global_step: 1377\n",
            "label/mean: 0.204165\n",
            "loss: 0.3533473\n",
            "precision: 0.71287704\n",
            "prediction/mean: 0.21013147\n",
            "recall: 0.4804883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yazi2YYBy0Cz"
      },
      "source": [
        "## Wide & Deep Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vI8AnbtxLun",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "755ddd8b-d6cd-42d7-f52a-63741547943d"
      },
      "source": [
        "wdm = tf.estimator.DNNLinearCombinedClassifier(\n",
        "    model_dir=wdmodel, \n",
        "    linear_feature_columns=crossed_columns,\n",
        "    dnn_feature_columns=deep_columns,\n",
        "    dnn_hidden_units=[512, 256, 128],\n",
        "    n_classes=2)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp6e1qqhhw\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp6e1qqhhw', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c0bs-rv6THC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72a3ed3a-2730-4569-da70-a48f11b1dcf8"
      },
      "source": [
        "wdm.train(input_fn=train_dl, steps=n_steps)\n",
        "wdm_results = wdm.evaluate(input_fn=val_dl, steps=None)\n",
        "for key in sorted(wdm_results):\n",
        "  print(\"%s: %s\" % (key, wdm_results[key]))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/adagrad.py:83: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp6e1qqhhw/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
            "INFO:tensorflow:loss = 0.7212872, step = 0\n",
            "INFO:tensorflow:global_step/sec: 28.1737\n",
            "INFO:tensorflow:loss = 0.5517755, step = 100 (3.551 sec)\n",
            "INFO:tensorflow:global_step/sec: 36.4099\n",
            "INFO:tensorflow:loss = 0.4467112, step = 200 (2.743 sec)\n",
            "INFO:tensorflow:global_step/sec: 35.6876\n",
            "INFO:tensorflow:loss = 0.4185835, step = 300 (2.802 sec)\n",
            "INFO:tensorflow:global_step/sec: 35.0818\n",
            "INFO:tensorflow:loss = 0.3979456, step = 400 (2.852 sec)\n",
            "INFO:tensorflow:global_step/sec: 32.4154\n",
            "INFO:tensorflow:loss = 0.41806763, step = 500 (3.082 sec)\n",
            "INFO:tensorflow:global_step/sec: 31.904\n",
            "INFO:tensorflow:loss = 0.34985387, step = 600 (3.136 sec)\n",
            "INFO:tensorflow:global_step/sec: 34.1813\n",
            "INFO:tensorflow:loss = 0.3220057, step = 700 (2.927 sec)\n",
            "INFO:tensorflow:global_step/sec: 33.998\n",
            "INFO:tensorflow:loss = 0.3779166, step = 800 (2.938 sec)\n",
            "INFO:tensorflow:global_step/sec: 33.582\n",
            "INFO:tensorflow:loss = 0.37185916, step = 900 (2.980 sec)\n",
            "INFO:tensorflow:global_step/sec: 33.7614\n",
            "INFO:tensorflow:loss = 0.36242288, step = 1000 (2.961 sec)\n",
            "INFO:tensorflow:global_step/sec: 32.0533\n",
            "INFO:tensorflow:loss = 0.32551703, step = 1100 (3.119 sec)\n",
            "INFO:tensorflow:global_step/sec: 32.6356\n",
            "INFO:tensorflow:loss = 0.36357498, step = 1200 (3.066 sec)\n",
            "INFO:tensorflow:global_step/sec: 32.3545\n",
            "INFO:tensorflow:loss = 0.32160226, step = 1300 (3.090 sec)\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1377...\n",
            "INFO:tensorflow:Saving checkpoints for 1377 into /tmp/tmp6e1qqhhw/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1377...\n",
            "INFO:tensorflow:Loss for final step: 0.36877295.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.canned.dnn_linear_combined.DNNLinearCombinedClassifierV2 at 0x7f25d86434d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-03-01T17:27:25Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmp6e1qqhhw/model.ckpt-1377\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Inference Time : 35.72161s\n",
            "INFO:tensorflow:Finished evaluation at 2021-03-01-17:28:01\n",
            "INFO:tensorflow:Saving dict for global step 1377: accuracy = 0.85342413, accuracy_baseline = 0.795835, auc = 0.8427185, auc_precision_recall = 0.63138276, average_loss = 0.3625078, global_step = 1377, label/mean = 0.204165, loss = 0.36262825, precision = 0.71225643, prediction/mean = 0.20428027, recall = 0.47326517\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1377: /tmp/tmp6e1qqhhw/model.ckpt-1377\n",
            "accuracy: 0.85342413\n",
            "accuracy_baseline: 0.795835\n",
            "auc: 0.8427185\n",
            "auc_precision_recall: 0.63138276\n",
            "average_loss: 0.3625078\n",
            "global_step: 1377\n",
            "label/mean: 0.204165\n",
            "loss: 0.36262825\n",
            "precision: 0.71225643\n",
            "prediction/mean: 0.20428027\n",
            "recall: 0.47326517\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}