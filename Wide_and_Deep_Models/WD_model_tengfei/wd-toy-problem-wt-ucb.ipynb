{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport torch.optim as optim\nimport numpy as np\nfrom collections import OrderedDict\n\ndevice = 'cuda'if torch.cuda.is_available() else 'cpu'\nprint(device)","execution_count":1,"outputs":[{"output_type":"stream","text":"cuda\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndef get_customer(ctype=None):\n    \"\"\"Customers come from two feature distributions.\n    Class 1: mean age 25, var 5 years, min age 18\n             mean ARPU 100, var 15\n    Class 2: mean age 45, var 6 years\n             mean ARPU 50, var 25\n    \"\"\"\n    if ctype is None:\n        if np.random.random() > .5: #coin toss\n            ctype = 1\n        else:\n            ctype = 2\n    age = 0\n    ft = -1\n    if ctype == 1:\n        while age < 18:\n            age = np.random.normal(25, 5)\n        while ft < 0:\n            ft = np.random.normal(100, 15)\n    if ctype == 2:\n        while age < 18:\n            age = np.random.normal(45, 6)\n        while ft < 0:\n            ft = np.random.normal(50, 25)\n    age = round(age)\n    return ctype, (age, ft)\n\ndef get_rewards(customer):\n    \"\"\"\n    There are three actions:\n    promo 1: low value. 10 dollar if accept\n    promo 2: mid value. 25 dollar if accept\n    promo 3: high value. 100 dollar if accept\n    Both groups are unlikely to accept promo 2.\n    Group 1 is more likely to accept promo 1.\n    Group 2 is slightly more likely to accept promo 3.\n    The optimal choice for group 1 is promo 1; 90% acceptance for\n    an expected reward of 9 dollars each.\n    Group 2 accepts with 25% rate for expected 2.5 dollar reward\n    The optimal choice for group 2 is promo 3; 20% acceptance for an expected\n    reward of 20 dollars each.\n    Group 1 accepts with 2% for expected reward of 2 dollars.\n    The least optimal choice in all cases is promo 2; 10% acceptance rate for both groups\n    for an expected reward of 2.5 dollars.\n    \"\"\"\n    if customer[0] == 1: #group 1 customer\n        if np.random.random() > .1:\n            reward1 = 10\n        else:\n            reward1 = 0\n        if np.random.random() > .90:\n            reward2 = 25\n        else:\n            reward2 = 0\n        if np.random.random() > .98:\n            reward3 = 100\n        else:\n            reward3 = 0\n    if customer[0] == 2: #group 2 customer\n        if np.random.random() > .75:\n            reward1 = 10\n        else:\n            reward1 = 0\n        if np.random.random() > .90:\n            reward2 = 25\n        else:\n            reward2 = 0\n        if np.random.random() > .80:\n            reward3 = 100\n        else:\n            reward3 = 0\n    return np.array([reward1, reward2, reward3])\n\ndef generate_data(n_rows):\n    contexts = []\n    actions = []\n    rewards = []\n    opt_actions = []\n    reward_vectors = []\n    for i in range(n_rows):\n        ctype, (age, ARPU) = get_customer()\n        reward_vector = get_rewards((ctype, (age, ARPU)))\n        action = np.random.randint(0,3)\n        reward = reward_vector[action]\n        if ctype == 1:\n            user_id = np.random.randint(0,20)\n            opt_action = 0\n        else:\n            user_id = np.random.randint(20,40)\n            opt_action = 2\n        contexts.append([user_id, age, ARPU])\n        actions.append(action)\n        rewards.append(reward)\n        opt_actions.append(opt_action)\n        reward_vectors.append(reward_vector)\n    return np.array(contexts), np.array(actions), np.array(rewards), np.array(opt_actions), np.array(reward_vectors)\n\n\nnum_data_train = 10240\ndata_train = generate_data(num_data_train)\nnum_data_test = 1024\ndata_valid = generate_data(num_data_test)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Toydataset(data.Dataset):\n    def __init__(self, data_input):\n        contexts, actions, rewards, opt_actions, reward_vectors = data_input\n        self.contexts = contexts\n        self.rewards = rewards\n        self.actions = actions\n        self.opt_actions = opt_actions\n        self.reward_vectors = reward_vectors\n\n    def __getitem__(self, index):\n        return self.contexts[index], self.actions[index], self.rewards[index], self.opt_actions[index], self.reward_vectors[index]\n\n    def __len__(self):\n        return self.contexts.shape[0]\n    \ndataset_train = Toydataset(data_train)\ndataloader_train = data.DataLoader(dataset=dataset_train, batch_size=64, shuffle=True)\ndataset_valid = Toydataset(data_valid)\ndataloader_valid = data.DataLoader(dataset=dataset_valid, batch_size=64, shuffle=True)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class WDmodel(nn.Module):\n    def __init__(self,         \n                 deep_in_dim=63, \n                 action_dim=3, \n                 num_embedding=100, \n                 embed_dim=16, \n                 deep_neurons=[32, 16], \n                 activation=nn.ReLU()):\n        \n        super(WDmodel, self).__init__()\n        \n        self.action_dim = action_dim\n        self.z_dim = embed_dim + deep_neurons[-1]\n        self.activation = activation\n        \n        self.wide = nn.Embedding(num_embedding, embed_dim)\n        \n        deep_dict = OrderedDict([])\n        in_features = deep_in_dim\n        for i, out_features in enumerate(deep_neurons):\n            deep_dict[f\"fc{i}\"] = nn.Linear(in_features, out_features)\n            deep_dict[f\"activation{i}\"] = activation\n            in_features = out_features\n\n        self.deep = nn.Sequential(deep_dict)\n        # define the final layer\n        self.lastlayer = nn.Linear(self.z_dim, self.action_dim)\n        \n    def forward(self, x):\n        \n        w_in = x[:,0].long()\n        w_out = self.wide(w_in)\n        d_in = x[:,1:]\n        d_out = self.deep(d_in)\n        lastlayer_in = torch.cat((w_out, d_out),dim=1)\n        out = self.activation(self.lastlayer(lastlayer_in))\n        return out\n    \nmodel = WDmodel(deep_in_dim=2, action_dim=3, num_embedding=40, deep_neurons=[128, 64]).to(device)\nloss_func = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.005)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(device, model, dataloader_train, dataloader_valid, loss_func, optimizer, num_epoches):\n    training_losses = []\n    valid_losses = []\n    for i in range(num_epoches):\n        running_loss = 0.0\n        training_average_loss = 0.0\n        for contexts, actions, rewards, _, _ in dataloader_train:\n            contexts = contexts.float().to(device)\n            actions = actions.long().to(device)\n            rewards = rewards.float().to(device)\n            outputs = model(contexts)\n            outs = outputs[range(outputs.shape[0]),actions]\n            loss = loss_func(outs, rewards)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * len(contexts)\n        training_average_loss = running_loss/float(len(dataloader_train.dataset))\n        training_losses.append(training_average_loss)\n        \n        running_loss = 0.0\n        valid_average_loss = 0.0\n        for contexts, actions, rewards, _, _ in dataloader_valid:\n            contexts = contexts.float().to(device)\n            actions = actions.long().to(device)\n            rewards = rewards.float().to(device)\n            outputs = model(contexts)\n            outs = outputs[range(outputs.shape[0]),actions]\n            loss = loss_func(outs, rewards)\n            running_loss += loss.item() * len(contexts)\n        valid_average_loss = running_loss/float(len(dataloader_valid.dataset))\n        valid_losses.append(valid_average_loss)\n        \n        if (i+1) % 10 == 0:\n            print(\"Epoch: {}\\t training loss: {}\\t validation loss: {}\".format(i+1, training_average_loss, valid_average_loss))\n\n    return training_losses, valid_losses","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epoches = 500\nlosses = train(device, model, dataloader_train, dataloader_valid, loss_func, optimizer, num_epoches)","execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch: 10\t training loss: 334.2028476715088\t validation loss: 323.0696449279785\nEpoch: 20\t training loss: 331.6629333734512\t validation loss: 325.9755868911743\nEpoch: 30\t training loss: 331.688236284256\t validation loss: 323.2209825515747\nEpoch: 40\t training loss: 332.96882009506226\t validation loss: 319.73777770996094\nEpoch: 50\t training loss: 329.80093042850496\t validation loss: 316.89665699005127\nEpoch: 60\t training loss: 328.91173403263093\t validation loss: 316.6520676612854\nEpoch: 70\t training loss: 329.14373111724854\t validation loss: 320.7127480506897\nEpoch: 80\t training loss: 328.37717678546903\t validation loss: 316.04652643203735\nEpoch: 90\t training loss: 329.0718920946121\t validation loss: 316.6720042228699\nEpoch: 100\t training loss: 329.1048558712006\t validation loss: 315.6602544784546\nEpoch: 110\t training loss: 329.0717516183853\t validation loss: 316.2718596458435\nEpoch: 120\t training loss: 328.8794364929199\t validation loss: 316.32476329803467\nEpoch: 130\t training loss: 328.5709607839584\t validation loss: 324.382728099823\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}