{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_snippet_wide_deep.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymxbKuwAlvza"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import sys\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt \n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhazojizmCpB"
      },
      "source": [
        "!pip install space-bandits\n",
        "from space_bandits import BanditAlgorithm\n",
        "clear_output()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw61Ffk4zkPH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6d3877-7850-420e-a9ac-70ede0cbc403"
      },
      "source": [
        "## plug in gdrive to load the data from gdrive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVnXPIwCmG11"
      },
      "source": [
        "## path to the contextual_dataset_wu.py file\n",
        "path = '/content/drive/MyDrive/Fellowship_Deep_and_Wide_Bandit/'\n",
        "sys.path.append(path)\n",
        "\n",
        "from contextual_dataset_wu import ContextualDataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4N08OiAmXz9"
      },
      "source": [
        "## Load a subset of the dataset\n",
        "\n",
        "p = 0.01  # Read 1% of the lines\n",
        "# keep the header, then take only 1% of lines\n",
        "# if random from [0,1] interval is greater than 0.01 the row will be skipped\n",
        "data_snippet = pd.read_csv(\n",
        "         path+'data_snippet.csv',\n",
        "         header=0, \n",
        "         skiprows=lambda i: i>0 and random.random() > p ## skip rows unless randomly generated number < p\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEjSgV4rmYZG"
      },
      "source": [
        "## Use label encoding for campaign_type\n",
        "data_snippet[\"campaign_type\"] = data_snippet[\"campaign_type\"].astype('category')\n",
        "data_snippet[\"campaign_type_cat\"] = data_snippet[\"campaign_type\"].cat.codes\n",
        "\n",
        "## Set rewards\n",
        "data_snippet['rewards'] = data_snippet['opened'] \n",
        "data_snippet['rewards'][data_snippet['unsub']==1] = -2\n",
        "data_snippet['rewards'][data_snippet['opened']==0] = -1\n",
        "data_snippet['rewards'] = data_snippet['rewards'] + data_snippet['rev_3dv2'] / 100.\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_btl-y0mtlH"
      },
      "source": [
        "## Define Dataset for data snippet\n",
        "class BanditTestDataset(Dataset):\n",
        "    def __init__(self, data, user_col, context_col, rewards_col, action_col=None):\n",
        "        super(BanditTestDataset, self).__init__()\n",
        "        ## data - pandas dataframe\n",
        "        ## user_col - user IDs column name\n",
        "        ## action_col - action column name\n",
        "        ## context_col - context column names\n",
        "        ## rewards_col - reward column name\n",
        "        self.user_ids = data[user_col]\n",
        "        self.context = data[context_col]\n",
        "        self.rewards = data[rewards_col]\n",
        "        if action_col != None:\n",
        "          self.actions = data[action_col]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.user_ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ## returns numpy arrays\n",
        "        user_id = self.user_ids.iloc[index].values[0]\n",
        "        context = self.context.iloc[index].values\n",
        "        reward = self.rewards.iloc[index].values[0]\n",
        "        reward_orig = reward ## copy the reward before changing it\n",
        "        if action_col != None:\n",
        "          action = self.actions.iloc[index].values[0]\n",
        "        else:\n",
        "          ## Randomly choose to send or not send email\n",
        "          randnum = random.random() ## draw a random number between 0 and 1\n",
        "          threshold = 0.5 ## e.g., if thresehold = 0.2, send email 80% of the time\n",
        "          if randnum >= threshold:\n",
        "            action = 1 ## send email if random number larger than threshold, get the associated reward\n",
        "          else:\n",
        "            action = 0 ## don't send email if random number smaller than threshold\n",
        "            reward = reward * -1. ## Get opposite reward compared to send email\n",
        "\n",
        "        return user_id, action, context, reward, reward_orig"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5vUnEs2myCq"
      },
      "source": [
        "## Split 80/20 into train and val. Create dataloaders.\n",
        "## val not used in this version, just trying to run space-bandits at this point...\n",
        "\n",
        "train, val = train_test_split(data_snippet, test_size=0.2)\n",
        "\n",
        "user_col = ['riid']\n",
        "context_col = ['retention_score','recency_score','frequency_score','campaign_type_cat']\n",
        "rewards_col = ['rewards']\n",
        "action_col = None\n",
        "\n",
        "batch_size = 2048\n",
        "\n",
        "train_dataset = BanditTestDataset(train, user_col, context_col, rewards_col, action_col=None)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "del train\n",
        "\n",
        "val_dataset = BanditTestDataset(val, user_col, context_col, rewards_col, action_col=None)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "del val"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVgaDzMYnHP_",
        "outputId": "32c69cdb-a036-4dfa-a3fc-f8aaf09b78ac"
      },
      "source": [
        "num_actions = 2 ## Send email or not send\n",
        "num_features = len(context_col)\n",
        "num_users = data_snippet[user_col].nunique()[0]\n",
        "print(\"Number of actions:\", num_actions)\n",
        "print(\"Number of features:\", num_features)\n",
        "print(\"Number of users:\", num_users)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of actions: 2\n",
            "Number of features: 4\n",
            "Number of users: 57792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g1bvLKdnNtQ"
      },
      "source": [
        "class Wide_Model(nn.Module):\n",
        "    def __init__(self, embed_size=100, n_action=2, embed_dim=64):\n",
        "        ## Learns expected reward for each action given User ID\n",
        "        ## Uses embeddings to 'memorize' individual users\n",
        "        ## embed_size - size of the dictionary of embeddings\n",
        "        ## embed_dim -  size of each embedding vector\n",
        "        ## n_action - number of possible actions\n",
        "        super(Wide_Model, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.n_action = n_action\n",
        "        self.embed_dim = embed_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(self.embed_size, self.embed_dim)\n",
        "        self.lr = nn.Linear(self.embed_dim, self.n_action)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        ## Input: user ID\n",
        "        x = self.embedding(x)\n",
        "        x = self.lr(x)\n",
        "        return x.squeeze(axis=0)\n",
        "    \n",
        "\n",
        "class Deep_Model(nn.Module):\n",
        "    def __init__(self, context_size=5, layer_sizes=[50], n_action=2):\n",
        "        ## Learns expected reward for each action given context\n",
        "        ## layer_sizes (list of integers): defines neural network architecture: n_layers = len(layer_sizes), \n",
        "        ## value is per-layer width. (default [50])\n",
        "        super(Deep_Model, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.n_action = n_action\n",
        "\n",
        "        self.layers = []\n",
        "        self.build_model()\n",
        "        self.activation = nn.ReLU()\n",
        "        ##self.activation = nn.Sigmoid()\n",
        "    \n",
        "    def build_layer(self, inp_dim, out_dim):\n",
        "        \"\"\"Builds a layer in deep model \"\"\"\n",
        "\n",
        "        layer = nn.modules.linear.Linear(inp_dim,out_dim)\n",
        "        nn.init.uniform_(layer.weight)\n",
        "        name = f'layer {len(self.layers)}'\n",
        "        self.add_module(name, layer)\n",
        "        return layer\n",
        "  \n",
        "    def build_norm_layer(self, dim):\n",
        "        \"\"\"Builds a layer in deep model \"\"\"\n",
        "\n",
        "        layer = nn.BatchNorm1d(dim)\n",
        "        name = f'layer {len(self.layers)}'\n",
        "        self.add_module(name, layer)\n",
        "        return layer\n",
        "    \n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Defines the actual NN model with fully connected layers.\n",
        "        \"\"\"\n",
        "        for i, layer in enumerate(self.layer_sizes):\n",
        "            if i==0:\n",
        "                inp_dim = self.context_size\n",
        "            else:\n",
        "                inp_dim = self.layer_sizes[i-1]\n",
        "            out_dim = self.layer_sizes[i]\n",
        "            new_layer = self.build_layer(inp_dim, out_dim)\n",
        "            self.layers.append(new_layer)\n",
        "\n",
        "        #norm_layer = self.build_norm_layer(out_dim)\n",
        "        #self.layers.append(norm_layer)\n",
        "\n",
        "        output_layer = self.build_layer(out_dim, self.n_action)\n",
        "        self.layers.append(output_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"forward pass of the neural network\"\"\"\n",
        "        ## Input: context\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            if i != len(self.layers)-1:\n",
        "                x = self.activation(x)\n",
        "        return x.squeeze(axis=0)\n",
        "    \n",
        "\n",
        "class Wide_and_Deep_Model(nn.Module):\n",
        "    def __init__(self, context_size=5, deep_layer_sizes=[50,100], n_action=2, embed_size=100, wide_embed_dim=64):\n",
        "        super(Wide_and_Deep_Model, self).__init__()\n",
        "        ## Combines the Wide model and Deep model\n",
        "        self.n_action = n_action\n",
        "        self.context_size = context_size\n",
        "        self.deep_layer_sizes = deep_layer_sizes\n",
        "        self.embed_size = embed_size\n",
        "        self.wide_embed_dim = wide_embed_dim\n",
        "\n",
        "        self.wide_model = Wide_Model(embed_size=self.embed_size, n_action=self.n_action, embed_dim=self.wide_embed_dim)\n",
        "        self.deep_model = Deep_Model(context_size=self.context_size, layer_sizes=self.deep_layer_sizes, n_action=self.n_action)\n",
        "    \n",
        "    def forward(self, wide_input, deep_input):\n",
        "        x_wide = self.wide_model(wide_input)\n",
        "        x_deep = self.deep_model(deep_input)\n",
        "        ## Add the output from wide and deep models\n",
        "        x = x_wide + x_deep\n",
        "\n",
        "        return x.squeeze(-1), x_wide.squeeze(-1), x_deep.squeeze(-1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DypklaKAnR8j"
      },
      "source": [
        "class Test_Wide_Deep_Bandits(BanditAlgorithm):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_actions,\n",
        "        num_features,\n",
        "        wide_embed_size=100, ## Size of embedding dictionary for the wide model\n",
        "        wide_embed_dim=64, ## Dimension of embedding for the wide model\n",
        "        update_freq_nn = 1, ## Frequency to update the model, default updates model for every data point\n",
        "        num_epochs = 1, ## Number of steps to Train for each update\n",
        "        model_type = 'wide_deep', ## model_type = 'wide', 'deep', or 'wide_deep'\n",
        "        name='test_deep_bandits'):\n",
        "      \n",
        "        hparams = {\n",
        "                    'num_actions':num_actions,\n",
        "                    'context_dim':num_features,\n",
        "                    'max_grad_norm':5.0,\n",
        "        }\n",
        "\n",
        "        ## Raise error if model_type is not one of the available models\n",
        "        possible_models = ['deep','wide','wide_deep']\n",
        "        if model_type not in possible_models:\n",
        "          raise NameError('model_type must be \"deep\", \"wide\", or \"wide_deep\"')\n",
        "\n",
        "        self.name = name\n",
        "        self.model_type = model_type\n",
        "        self.wide_embed_dim = wide_embed_dim\n",
        "        self.wide_embed_size = wide_embed_size\n",
        "        self.hparams = hparams\n",
        "\n",
        "        ## Initialize model and optimizer depending on model_type\n",
        "        if self.model_type == 'deep':\n",
        "          self.deep_model = Deep_Model(context_size=self.hparams['context_dim'],\n",
        "                                       n_action=self.hparams['num_actions'])\n",
        "          self.optim = torch.optim.RMSprop(self.deep_model.parameters())\n",
        "\n",
        "        if self.model_type == 'wide':\n",
        "          self.wide_model = Wide_Model(embed_size=self.wide_embed_size, \n",
        "                                      n_action=self.hparams['num_actions'], \n",
        "                                      embed_dim=self.wide_embed_dim)\n",
        "          self.optim = torch.optim.RMSprop(self.wide_model.parameters())\n",
        "        \n",
        "\n",
        "        if self.model_type == 'wide_deep':\n",
        "          self.wide_deep_model = Wide_and_Deep_Model(context_size=self.hparams['context_dim'],\n",
        "                                                    embed_size=self.wide_embed_size, \n",
        "                                                    n_action=self.hparams['num_actions'], \n",
        "                                                    wide_embed_dim=self.wide_embed_dim) \n",
        "          self.optim = torch.optim.RMSprop(self.wide_deep_model.parameters())\n",
        "        \n",
        "        self.loss = nn.modules.loss.MSELoss()\n",
        "\n",
        "        self.t = 0\n",
        "        self.update_freq_nn = update_freq_nn \n",
        "        self.num_epochs = num_epochs  \n",
        "        self.data_h = ContextualDataset(self.hparams['context_dim'],\n",
        "                                        self.hparams['num_actions'],\n",
        "                                        intercept=False)\n",
        "        \n",
        "        ## Keep a dictionary of users that matches user's riid to indexes between 0 and num_users\n",
        "        ## Initialize dicitonary with a \"dummy user\" that will be used for prediction when the user has never been seen\n",
        "        self.user_dict = {0:0} \n",
        "        self.current_user_size = 1\n",
        "\n",
        "    def expected_values(self, user_id, context):\n",
        "\n",
        "        context = torch.tensor(context).float()\n",
        "        user_id = self.lookup_one_user(user_id)\n",
        "\n",
        "        if self.model_type == 'deep':\n",
        "          x = self.deep_model(context)\n",
        "        if self.model_type == 'wide':\n",
        "          x = self.wide_model(user_id)\n",
        "        if self.model_type == 'wide_deep':\n",
        "          x, x_wide, x_deep = self.wide_deep_model(user_id, context)\n",
        "          if user_id == 0:\n",
        "            x = x_deep ## Return only the deep model output if user is a \"dummy user\" \n",
        "        #return x, x_wide, x_deep\n",
        "        return x\n",
        "\n",
        "    def action(self, user_id, context):\n",
        "        ## Select and action based on expected values of reward\n",
        "        if self.model_type == 'deep':\n",
        "          vals = self.expected_values(user_id, context)  ## for deep model\n",
        "        if self.model_type == 'wide':\n",
        "          vals = self.expected_values(user_id, context)   ## for wide model\n",
        "        if self.model_type == 'wide_deep':\n",
        "          vals = self.expected_values(user_id, context)  ## for wide and deep model\n",
        "        return np.argmax(vals.detach().numpy())\n",
        "        \n",
        "    def update(self, user_id, context, action, reward):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          user_id: Last User ID\n",
        "          context: Last observed context.\n",
        "          action: Last observed action.\n",
        "          reward: Last observed reward.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.t += 1\n",
        "        self.data_h.add(user_id, context, action, reward)\n",
        "        self.update_user_dict(user_id)\n",
        "\n",
        "        if self.t % self.update_freq_nn == 0:\n",
        "          self.train(self.data_h, self.num_epochs)\n",
        "        \n",
        "        \n",
        "    def do_step(self, u, x, y, w, step):\n",
        "        if self.model_type == 'deep':\n",
        "          y_hat = self.deep_model.forward(x.float())\n",
        "        if self.model_type == 'wide':\n",
        "          y_hat = self.wide_model.forward(u)\n",
        "        if self.model_type == 'wide_deep':\n",
        "          y_hat, y_wide, y_deep = self.wide_deep_model(u,x.float())\n",
        "        \n",
        "        y_hat *= w\n",
        "        ls = self.loss(y_hat, y.float())\n",
        "        ls.backward()\n",
        "\n",
        "        clip = self.hparams['max_grad_norm']\n",
        "\n",
        "        if self.model_type == 'deep':\n",
        "          torch.nn.utils.clip_grad_norm_(self.deep_model.parameters(), clip)\n",
        "        if self.model_type == 'wide':\n",
        "          torch.nn.utils.clip_grad_norm_(self.wide_model.parameters(), clip)\n",
        "        if self.model_type == 'wide_deep':\n",
        "          torch.nn.utils.clip_grad_norm_(self.wide_deep_model.parameters(), clip)\n",
        "\n",
        "        self.optim.step()\n",
        "        self.optim.zero_grad()\n",
        "\n",
        "    def train(self, data, num_steps):\n",
        "        \"\"\"Trains the network for num_steps, using the provided data.\n",
        "        Args:\n",
        "          data: ContextualDataset object that provides the data.\n",
        "          num_steps: Number of minibatches to train the network for.\n",
        "        Takes longer to get batch data and train model as the data size increase\n",
        "        \"\"\"\n",
        "        #print(\"Training at time {} for {} steps...\".format(self.t, num_steps))\n",
        "\n",
        "        batch_size = 512\n",
        "        \n",
        "        data.scale_contexts() ## have to scale the data first if scaled=True in data.get_batch_with_weights()\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            u, x, y, w = data.get_batch_with_weights(batch_size, scaled=True)\n",
        "            u = self.lookup_users(u)\n",
        "\n",
        "            ## Training at time step 1 will cause problem if scaled=True, \n",
        "            ## because standard deviation=0, and scaled_context will equal nan\n",
        "            if self.t != 1:\n",
        "              self.do_step(u, x, y, w, step)\n",
        "        \n",
        "\n",
        "    def lookup_one_user(self, user_id):\n",
        "      user_id = user_id.tolist()\n",
        "      if user_id not in self.user_dict.keys():\n",
        "        user_index = 0\n",
        "      else:\n",
        "        user_index = self.user_dict[user_id]\n",
        "      return torch.tensor(user_index)\n",
        "\n",
        "    def lookup_users(self, user_ids):\n",
        "      ## Returns a list of user indexes for input to the wide network\n",
        "      user_ids = user_ids.tolist()\n",
        "      user_index = [self.user_dict[u] for u in user_ids]\n",
        "      return torch.tensor(user_index)\n",
        "    \n",
        "    def update_user_dict(self, user_id):\n",
        "      ## Create/update a lookup dictionary that matches user ID to a user index between 0 and num_users\n",
        "      user_id = user_id.tolist()\n",
        "      if user_id not in self.user_dict:\n",
        "        self.user_dict.update({user_id:self.current_user_size})\n",
        "        self.current_user_size += 1\n",
        "              \n",
        "    def save(self, path):\n",
        "        \"\"\"saves model to path\"\"\"\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(self, f)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLzg8Zognmuj"
      },
      "source": [
        "model_wide = Test_Wide_Deep_Bandits(num_actions, num_features, wide_embed_size=num_users, model_type='wide', update_freq_nn = 100, num_epochs = 1)\n",
        "model_deep = Test_Wide_Deep_Bandits(num_actions, num_features, wide_embed_size=num_users, model_type='deep', update_freq_nn = 100, num_epochs = 1)\n",
        "model_wide_deep = Test_Wide_Deep_Bandits(num_actions, num_features, wide_embed_size=num_users, model_type='wide_deep', update_freq_nn = 100, num_epochs = 1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR0qhFoxnnM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1056c7aa-c736-4172-bddc-d0e3b7a47d23"
      },
      "source": [
        "%%time\n",
        "tbar = tqdm(train_dataloader)\n",
        "for item in tbar:\n",
        "  for i in range(batch_size):\n",
        "    try:\n",
        "      user1 = item[0][i]\n",
        "      action1 = item[1][i]\n",
        "      context1 = item[2][i]\n",
        "      reward1 = item[3][i]\n",
        "    except:\n",
        "      break\n",
        "    \n",
        "    model_wide.update(torch.tensor(user1),context1,action1,reward1)\n",
        "    model_deep.update(torch.tensor(user1),context1,action1,reward1)\n",
        "    model_wide_deep.update(torch.tensor(user1),context1,action1,reward1)\n",
        "\n",
        "#clear_output()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/25 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "100%|██████████| 25/25 [1:05:22<00:00, 156.91s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1h 4min 27s, sys: 46.8 s, total: 1h 5min 14s\n",
            "Wall time: 1h 5min 22s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOX6E3g_tqKC"
      },
      "source": [
        "## Save models\n",
        "model_wide.save(path+'ds_model_wide.pkl')\n",
        "model_deep.save(path+'ds_model_deep.pkl')\n",
        "model_wide_deep.save(path+'ds_model_wide_deep.pkl')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo7jqoNxuQUf"
      },
      "source": [
        "## Load existing models\n",
        "#model_wide = load_model(path+'ds_model_wide.pkl')\n",
        "#model_deep = load_model(path+'ds_model_deep.pkl')\n",
        "#model_wide_deep = load_model(path+'ds_model_wide_deep.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SqgzB_e0ynS",
        "outputId": "3b126546-e1ae-459b-c5de-4d2495900f92"
      },
      "source": [
        "tbar = tqdm(val_dataloader)\n",
        "\n",
        "y_true = []\n",
        "y_pred_deep = []\n",
        "y_pred_wide = []\n",
        "y_pred_wide_deep = []\n",
        "\n",
        "num_correct_deep = 0\n",
        "num_correct_wide = 0\n",
        "num_correct_wide_deep = 0\n",
        "total = 0\n",
        "\n",
        "## Context means and std for scaling \n",
        "context_means = data_snippet[context_col].mean().values\n",
        "context_std = data_snippet[context_col].std().values\n",
        "\n",
        "for item in tbar:\n",
        "  for i in range(batch_size):\n",
        "    try:\n",
        "      user1 = item[0][i]\n",
        "      action1 = item[1][i]\n",
        "      context1 = item[2][i]\n",
        "      context1_scaled = (context1 - context_means) / context_std ## Scale context for input \n",
        "      reward_1 = item[3][i]\n",
        "      reward_o = item[4][i]\n",
        "    except:\n",
        "      break\n",
        "\n",
        "    if reward_o >= 0.0:\n",
        "      best_action = 1\n",
        "    else:\n",
        "      best_action = 0\n",
        "      \n",
        "    y_true.append(best_action)\n",
        "\n",
        "    ## Predict best action: REMEMBER TO SCALE CONTEXT!!!!\n",
        "    model_action_deep = model_deep.action(torch.tensor(user1),context1_scaled) \n",
        "    model_action_wide = model_wide.action(torch.tensor(user1),context1_scaled) \n",
        "    model_action_wide_deep = model_wide_deep.action(torch.tensor(user1),context1_scaled) \n",
        "\n",
        "    if best_action == model_action_deep:\n",
        "      num_correct_deep += 1\n",
        "    if best_action == model_action_wide:\n",
        "      num_correct_wide += 1\n",
        "    if best_action == model_action_wide_deep:\n",
        "      num_correct_wide_deep += 1\n",
        "    \n",
        "    total += 1\n",
        "\n",
        "    y_pred_deep.append(model_action_deep)\n",
        "    y_pred_wide.append(model_action_wide)\n",
        "    y_pred_wide_deep.append(model_action_wide_deep)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(\"Accuracy of wide model = \", num_correct_wide / total)\n",
        "print(\"Accuracy of deep model = \", num_correct_deep / total)\n",
        "print(\"Accuracy of wide and deep model = \", num_correct_wide_deep / total)\n",
        "    "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of wide model =  0.5916606757728253\n",
            "Accuracy of deep model =  0.7808131639907341\n",
            "Accuracy of wide and deep model =  0.7717868839364167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GueEcZN1MIe3",
        "outputId": "d37c8bf5-9031-4334-9c2e-9d77bfa6c95f"
      },
      "source": [
        "confusion_matrix(y_true,y_pred_deep)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3828, 1289],\n",
              "       [1455, 5947]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzdAKhpwPCjI",
        "outputId": "9254874e-a5d7-484e-e7c5-e67f31f1b10f"
      },
      "source": [
        "confusion_matrix(y_true,y_pred_wide)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 132, 4985],\n",
              "       [ 127, 7275]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXI1fdu_PFD0",
        "outputId": "6aa32004-6093-4a57-e9a5-d9bbe9409349"
      },
      "source": [
        "confusion_matrix(y_true,y_pred_wide_deep)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4019, 1098],\n",
              "       [1759, 5643]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}