{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_snippet_wide_deep.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMmt7ZYOnfmQ"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymxbKuwAlvza"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import sys\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt \n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG_OEnIdnkP6"
      },
      "source": [
        "Install space-bandits and import BanditAlgorithm class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhazojizmCpB"
      },
      "source": [
        "!pip install space-bandits\n",
        "from space_bandits import BanditAlgorithm\n",
        "clear_output()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG3jWJTKnqdV"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw61Ffk4zkPH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6d3877-7850-420e-a9ac-70ede0cbc403"
      },
      "source": [
        "## plug in gdrive to load the data from gdrive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btQxEKhunuHM"
      },
      "source": [
        "Import modeified contextual_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVnXPIwCmG11"
      },
      "source": [
        "## path to the contextual_dataset_wu.py file\n",
        "path = '/content/drive/MyDrive/Fellowship_Deep_and_Wide_Bandit/'\n",
        "sys.path.append(path)\n",
        "\n",
        "from contextual_dataset_wu import ContextualDataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhGhyp1anxHy"
      },
      "source": [
        "Load a subset of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4N08OiAmXz9"
      },
      "source": [
        "## Load a subset of the dataset\n",
        "\n",
        "p = 0.01  # Read 1% of the lines\n",
        "# keep the header, then take only 1% of lines\n",
        "# if random from [0,1] interval is greater than 0.01 the row will be skipped\n",
        "data_snippet = pd.read_csv(\n",
        "         path+'data_snippet.csv',\n",
        "         header=0, \n",
        "         skiprows=lambda i: i>0 and random.random() > p ## skip rows unless randomly generated number < p\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlehzV_fn0Yz"
      },
      "source": [
        "Set rewards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEjSgV4rmYZG"
      },
      "source": [
        "## Use label encoding for campaign_type\n",
        "data_snippet[\"campaign_type\"] = data_snippet[\"campaign_type\"].astype('category')\n",
        "data_snippet[\"campaign_type_cat\"] = data_snippet[\"campaign_type\"].cat.codes\n",
        "\n",
        "## Set rewards\n",
        "data_snippet['rewards'] = data_snippet['opened'] \n",
        "data_snippet['rewards'][data_snippet['unsub']==1] = -2\n",
        "data_snippet['rewards'][data_snippet['opened']==0] = -0.95\n",
        "data_snippet['rewards'] = data_snippet['rewards'] + data_snippet['rev_3dv2'] / 100.\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl6hNJ7Bn4co"
      },
      "source": [
        "Define Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_btl-y0mtlH"
      },
      "source": [
        "## Define Dataset for data snippet\n",
        "class BanditTestDataset(Dataset):\n",
        "    def __init__(self, data, user_col, context_col, rewards_col, action_col=None):\n",
        "        super(BanditTestDataset, self).__init__()\n",
        "        ## data - pandas dataframe\n",
        "        ## user_col - user IDs column name\n",
        "        ## action_col - action column name\n",
        "        ## context_col - context column names\n",
        "        ## rewards_col - reward column name\n",
        "        self.user_ids = data[user_col]\n",
        "        self.context = data[context_col]\n",
        "        self.rewards = data[rewards_col]\n",
        "        if action_col != None:\n",
        "          self.actions = data[action_col]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.user_ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ## returns numpy arrays\n",
        "        user_id = self.user_ids.iloc[index].values[0]\n",
        "        context = self.context.iloc[index].values\n",
        "        reward = self.rewards.iloc[index].values[0]\n",
        "        reward_orig = reward ## copy the reward before changing it\n",
        "        if action_col != None:\n",
        "          action = self.actions.iloc[index].values[0]\n",
        "        else:\n",
        "          ## Randomly choose to send or not send email\n",
        "          randnum = random.random() ## draw a random number between 0 and 1\n",
        "          threshold = 0.5 ## e.g., if thresehold = 0.2, send email 80% of the time\n",
        "          if randnum >= threshold:\n",
        "            action = 1 ## send email if random number larger than threshold, get the associated reward\n",
        "          else:\n",
        "            action = 0 ## don't send email if random number smaller than threshold\n",
        "            reward = reward * -1. ## Get opposite reward compared to send email\n",
        "\n",
        "        return user_id, action, context, reward, reward_orig"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwLEPUzRn9up"
      },
      "source": [
        "Define dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5vUnEs2myCq"
      },
      "source": [
        "## Split 80/20 into train and val. Create dataloaders.\n",
        "## val not used in this version, just trying to run space-bandits at this point...\n",
        "\n",
        "train, val = train_test_split(data_snippet, test_size=0.2)\n",
        "\n",
        "user_col = ['riid']\n",
        "context_col = ['retention_score','recency_score','frequency_score','campaign_type_cat']\n",
        "rewards_col = ['rewards']\n",
        "action_col = None\n",
        "\n",
        "batch_size = 2048\n",
        "\n",
        "train_dataset = BanditTestDataset(train, user_col, context_col, rewards_col, action_col=None)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "del train\n",
        "\n",
        "val_dataset = BanditTestDataset(val, user_col, context_col, rewards_col, action_col=None)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "del val"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVgaDzMYnHP_",
        "outputId": "6452525a-ba36-4abb-a233-096a32b54532"
      },
      "source": [
        "num_actions = 2 ## Send email or not send\n",
        "num_features = len(context_col)\n",
        "num_users = data_snippet[user_col].nunique()[0]\n",
        "print(\"Number of actions:\", num_actions)\n",
        "print(\"Number of features:\", num_features)\n",
        "print(\"Number of users:\", num_users)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of actions: 2\n",
            "Number of features: 4\n",
            "Number of users: 57050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmnGJjE3oBzp"
      },
      "source": [
        "Model Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g1bvLKdnNtQ"
      },
      "source": [
        "class Wide_Model(nn.Module):\n",
        "    def __init__(self, embed_size=100, n_action=2, embed_dim=64):\n",
        "        ## Learns expected reward for each action given User ID\n",
        "        ## Uses embeddings to 'memorize' individual users\n",
        "        ## embed_size - size of the dictionary of embeddings\n",
        "        ## embed_dim -  size of each embedding vector\n",
        "        ## n_action - number of possible actions\n",
        "        super(Wide_Model, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.n_action = n_action\n",
        "        self.embed_dim = embed_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(self.embed_size, self.embed_dim)\n",
        "        self.lr = nn.Linear(self.embed_dim, self.n_action)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        ## Input: user ID\n",
        "        x = self.embedding(x)\n",
        "        x = self.lr(x)\n",
        "        return x.squeeze(axis=0)\n",
        "    \n",
        "\n",
        "class Deep_Model(nn.Module):\n",
        "    def __init__(self, context_size=5, layer_sizes=[50], n_action=2):\n",
        "        ## Learns expected reward for each action given context\n",
        "        ## layer_sizes (list of integers): defines neural network architecture: n_layers = len(layer_sizes), \n",
        "        ## value is per-layer width. (default [50])\n",
        "        super(Deep_Model, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.n_action = n_action\n",
        "\n",
        "        self.layers = []\n",
        "        self.build_model()\n",
        "        self.activation = nn.ReLU()\n",
        "        #self.activation = nn.Sigmoid()\n",
        "    \n",
        "    def build_layer(self, inp_dim, out_dim):\n",
        "        \"\"\"Builds a layer in deep model \"\"\"\n",
        "\n",
        "        layer = nn.modules.linear.Linear(inp_dim,out_dim)\n",
        "        nn.init.uniform_(layer.weight)\n",
        "        name = f'layer {len(self.layers)}'\n",
        "        self.add_module(name, layer)\n",
        "        return layer\n",
        "  \n",
        "    \n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Defines the actual NN model with fully connected layers.\n",
        "        \"\"\"\n",
        "        for i, layer in enumerate(self.layer_sizes):\n",
        "            if i==0:\n",
        "                inp_dim = self.context_size\n",
        "            else:\n",
        "                inp_dim = self.layer_sizes[i-1]\n",
        "            out_dim = self.layer_sizes[i]\n",
        "            new_layer = self.build_layer(inp_dim, out_dim)\n",
        "            self.layers.append(new_layer)\n",
        "\n",
        "        output_layer = self.build_layer(out_dim, self.n_action)\n",
        "        self.layers.append(output_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"forward pass of the neural network\"\"\"\n",
        "        ## Input: context\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            if i != len(self.layers)-1:\n",
        "                x = self.activation(x)\n",
        "        return x.squeeze(axis=0)\n",
        "    \n",
        "\n",
        "class Wide_and_Deep_Model(nn.Module):\n",
        "    def __init__(self, context_size=5, deep_layer_sizes=[50,100], n_action=2, \n",
        "                 embed_size=100, wide_embed_dim=64):\n",
        "        super(Wide_and_Deep_Model, self).__init__()\n",
        "        ## Combines the Wide model and Deep model\n",
        "        self.n_action = n_action\n",
        "        self.context_size = context_size\n",
        "        self.deep_layer_sizes = deep_layer_sizes\n",
        "        self.embed_size = embed_size\n",
        "        self.wide_embed_dim = wide_embed_dim\n",
        "\n",
        "        self.wide_model = Wide_Model(embed_size=self.embed_size, n_action=self.n_action, embed_dim=self.wide_embed_dim)\n",
        "        self.deep_model = Deep_Model(context_size=self.context_size, layer_sizes=self.deep_layer_sizes, n_action=self.n_action)\n",
        "    \n",
        "    def forward(self, wide_input, deep_input):\n",
        "        x_wide = self.wide_model(wide_input)\n",
        "        x_deep = self.deep_model(deep_input)\n",
        "        ## Concatenate output from wide and deep model\n",
        "        if len(x_wide.size()) == 1:\n",
        "          x = torch.cat((x_wide,x_deep))\n",
        "        elif len(x_wide.size()) > 1:\n",
        "          x = torch.cat((x_wide,x_deep), dim=1)\n",
        "\n",
        "        return x.squeeze(-1)#, x_wide.squeeze(-1), x_deep.squeeze(-1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBAu1VQsoEPR"
      },
      "source": [
        "Test the wide and deep model with BanditAlgorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DypklaKAnR8j"
      },
      "source": [
        "class Test_Wide_Deep_Bandits(BanditAlgorithm):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_actions,\n",
        "        num_features,\n",
        "        wide_embed_size=100, ## Size of embedding dictionary for the wide model\n",
        "        wide_embed_dim=64, ## Dimension of embedding for the wide model\n",
        "        update_freq_nn = 1, ## Frequency to update the model, default updates model for every data point\n",
        "        num_epochs = 1, ## Number of steps to Train for each update\n",
        "        model_type = 'wide_deep', ## model_type = 'wide', 'deep', or 'wide_deep'\n",
        "        name='test_deep_bandits'):\n",
        "      \n",
        "        hparams = {\n",
        "                    'num_actions':num_actions,\n",
        "                    'context_dim':num_features,\n",
        "                    'max_grad_norm':5.0,\n",
        "        }\n",
        "\n",
        "        ## Raise error if model_type is not one of the available models\n",
        "        possible_models = ['deep','wide','wide_deep']\n",
        "        if model_type not in possible_models:\n",
        "          raise NameError('model_type must be \"deep\", \"wide\", or \"wide_deep\"')\n",
        "\n",
        "        self.name = name\n",
        "        self.model_type = model_type\n",
        "        self.wide_embed_dim = wide_embed_dim\n",
        "        self.wide_embed_size = wide_embed_size\n",
        "        self.hparams = hparams\n",
        "\n",
        "        ## Initialize model and optimizer depending on model_type\n",
        "        if self.model_type == 'deep':\n",
        "          self.deep_model = Deep_Model(context_size=self.hparams['context_dim'],\n",
        "                                       n_action=self.hparams['num_actions'])\n",
        "          self.optim = torch.optim.RMSprop(self.deep_model.parameters())\n",
        "\n",
        "        if self.model_type == 'wide':\n",
        "          self.wide_model = Wide_Model(embed_size=self.wide_embed_size, \n",
        "                                      n_action=self.hparams['num_actions'], \n",
        "                                      embed_dim=self.wide_embed_dim)\n",
        "          self.optim = torch.optim.RMSprop(self.wide_model.parameters())\n",
        "        \n",
        "\n",
        "        if self.model_type == 'wide_deep':\n",
        "          self.wide_deep_model = Wide_and_Deep_Model(context_size=self.hparams['context_dim'],\n",
        "                                                    embed_size=self.wide_embed_size, \n",
        "                                                    n_action=self.hparams['num_actions'], \n",
        "                                                    wide_embed_dim=self.wide_embed_dim) \n",
        "          self.optim = torch.optim.RMSprop(self.wide_deep_model.parameters())\n",
        "        \n",
        "        self.loss = nn.modules.loss.MSELoss()\n",
        "\n",
        "        self.t = 0\n",
        "        self.update_freq_nn = update_freq_nn \n",
        "        self.num_epochs = num_epochs  \n",
        "        self.data_h = ContextualDataset(self.hparams['context_dim'],\n",
        "                                        self.hparams['num_actions'],\n",
        "                                        intercept=False)\n",
        "        \n",
        "        ## Keep a dictionary of users that matches user's riid to indexes between 0 and num_users\n",
        "        ## Initialize dicitonary with a \"dummy user\" that will be used for prediction when the user has never been seen\n",
        "        self.user_dict = {0:0} \n",
        "\n",
        "        ## Dictionary of how many times a user has been seen by model\n",
        "        self.user_freq = {0:0}\n",
        "\n",
        "        ## Number of users seen by model\n",
        "        self.current_user_size = 1\n",
        "\n",
        "    def expected_values(self, user_id, context):\n",
        "\n",
        "        if not torch.is_tensor(user_id):\n",
        "          user_id = torch.tensor(user_id)\n",
        "\n",
        "        context = torch.tensor(context).float()\n",
        "        user_freq = self.lookup_one_user_freq(user_id)\n",
        "        user_idx = self.lookup_one_user_idx(user_id)\n",
        "\n",
        "        n_act=self.hparams['num_actions'] ## Number of actions for combining wide and deep outputs\n",
        "\n",
        "        if self.model_type == 'deep':\n",
        "          x = self.deep_model(context)\n",
        "        if self.model_type == 'wide':\n",
        "          x = self.wide_model(user_idx)\n",
        "        if self.model_type == 'wide_deep':\n",
        "          if user_freq == 0:\n",
        "            wide_scale = 0\n",
        "          else:\n",
        "            wide_scale = 1 + (user_freq - 1)*0.1 ## Give heavier weight to wide model output if user has been seen more than once\n",
        "          x = self.wide_deep_model(user_idx, context)\n",
        "          x = wide_scale*x[0:n_act]+x[n_act:n_act*2] ## Add the output from wide and deep model\n",
        "        return x\n",
        "\n",
        "    def action(self, user_id, context):\n",
        "        ## Select an action based on expected values of reward\n",
        "        if self.model_type == 'deep':\n",
        "          vals = self.expected_values(user_id, context)  ## for deep model\n",
        "        if self.model_type == 'wide':\n",
        "          vals = self.expected_values(user_id, context)   ## for wide model\n",
        "        if self.model_type == 'wide_deep':\n",
        "          vals = self.expected_values(user_id, context)  ## for wide and deep model\n",
        "        return np.argmax(vals.detach().numpy())\n",
        "        \n",
        "    def update(self, user_id, context, action, reward):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          user_id: Last User ID\n",
        "          context: Last observed context.\n",
        "          action: Last observed action.\n",
        "          reward: Last observed reward.\n",
        "        \"\"\"\n",
        "        \n",
        "        if not torch.is_tensor(user_id):\n",
        "          user_id = torch.tensor(user_id)\n",
        "\n",
        "        self.t += 1\n",
        "        self.data_h.add(user_id, context, action, reward)\n",
        "        self.update_user_dict(user_id)\n",
        "\n",
        "        if self.t % self.update_freq_nn == 0:\n",
        "          self.train(self.data_h, self.num_epochs)\n",
        "        \n",
        "        \n",
        "    def do_step(self, u, x, y, w, step):\n",
        "        if self.model_type == 'deep':\n",
        "          y_hat = self.deep_model.forward(x.float())\n",
        "        if self.model_type == 'wide':\n",
        "          y_hat = self.wide_model.forward(u)\n",
        "        if self.model_type == 'wide_deep':\n",
        "          y_hat = self.wide_deep_model(u,x.float())\n",
        "          ## replicate y and w to compare with y_hat from wide_deep model\n",
        "          y = torch.cat((y,y), dim=1) \n",
        "          w = torch.cat((w,w), dim=1) \n",
        "        \n",
        "        y_hat *= w\n",
        "        ls = self.loss(y_hat, y.float())\n",
        "        ls.backward()\n",
        "\n",
        "        clip = self.hparams['max_grad_norm']\n",
        "\n",
        "        if self.model_type == 'deep':\n",
        "          torch.nn.utils.clip_grad_norm_(self.deep_model.parameters(), clip)\n",
        "        if self.model_type == 'wide':\n",
        "          torch.nn.utils.clip_grad_norm_(self.wide_model.parameters(), clip)\n",
        "        if self.model_type == 'wide_deep':\n",
        "          torch.nn.utils.clip_grad_norm_(self.wide_deep_model.parameters(), clip)\n",
        "\n",
        "        self.optim.step()\n",
        "        self.optim.zero_grad()\n",
        "\n",
        "    def train(self, data, num_steps):\n",
        "        \"\"\"Trains the network for num_steps, using the provided data.\n",
        "        Args:\n",
        "          data: ContextualDataset object that provides the data.\n",
        "          num_steps: Number of minibatches to train the network for.\n",
        "        Takes longer to get batch data and train model as the data size increase\n",
        "        \"\"\"\n",
        "        #print(\"Training at time {} for {} steps...\".format(self.t, num_steps))\n",
        "\n",
        "        batch_size = 512\n",
        "        \n",
        "        data.scale_contexts() ## have to scale the data first if scaled=True in data.get_batch_with_weights()\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            u, x, y, w = data.get_batch_with_weights(batch_size, scaled=True)\n",
        "            u = self.lookup_user_idxs(u) ## Lookup user indexes\n",
        "\n",
        "            ## Training at time step 1 will cause problem if scaled=True, \n",
        "            ## because standard deviation=0, and scaled_context will equal nan\n",
        "            if self.t != 1:\n",
        "              self.do_step(u, x, y, w, step)\n",
        "        \n",
        "    def lookup_one_user_idx(self, user_id):\n",
        "      ## Returns one user index \n",
        "      user_id = user_id.tolist()\n",
        "      if user_id not in self.user_dict.keys():\n",
        "        user_index = 0\n",
        "      else:\n",
        "        user_index = self.user_dict[user_id]\n",
        "      return torch.tensor(user_index)\n",
        "    \n",
        "    def lookup_one_user_freq(self, user_id):\n",
        "      ## Returns the number of times this user has been seen by the model\n",
        "      user_id = user_id.tolist()\n",
        "      if user_id not in self.user_freq.keys():\n",
        "        user_freq = 0\n",
        "      else:\n",
        "        user_freq = self.user_freq[user_id]\n",
        "      return user_freq\n",
        "\n",
        "    def lookup_user_idxs(self, user_ids):\n",
        "      ## Returns a list of user indexes for input to the wide network\n",
        "      user_ids = user_ids.tolist()\n",
        "      user_index = [self.user_dict[u] for u in user_ids]\n",
        "      return torch.tensor(user_index)\n",
        "    \n",
        "    def update_user_dict(self, user_id):\n",
        "      ## Create/update a lookup dictionary that matches user ID to a user index between 0 and num_users\n",
        "      user_id = user_id.tolist()\n",
        "      if user_id not in self.user_dict:\n",
        "        self.user_dict.update({user_id:self.current_user_size})\n",
        "        self.user_freq.update({user_id:1})\n",
        "        self.current_user_size += 1\n",
        "      elif user_id in self.user_freq:\n",
        "        self.user_freq[user_id] += 1\n",
        "              \n",
        "    def save(self, path):\n",
        "        \"\"\"saves model to path\"\"\"\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(self, f)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6bFtGQBoMvG"
      },
      "source": [
        "Initialize and train models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLzg8Zognmuj"
      },
      "source": [
        "model_wide = Test_Wide_Deep_Bandits(num_actions, num_features, wide_embed_size=num_users, model_type='wide', update_freq_nn = 100, num_epochs = 1)\n",
        "model_deep = Test_Wide_Deep_Bandits(num_actions, num_features, wide_embed_size=num_users, model_type='deep', update_freq_nn = 100, num_epochs = 1)\n",
        "model_wide_deep = Test_Wide_Deep_Bandits(num_actions, num_features, wide_embed_size=num_users, model_type='wide_deep', update_freq_nn = 100, num_epochs = 1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR0qhFoxnnM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f429a0b6-4c39-4208-b8e7-ff38bba520c2"
      },
      "source": [
        "%%time\n",
        "tbar = tqdm(train_dataloader)\n",
        "for item in tbar:\n",
        "  for i in range(batch_size):\n",
        "    try:\n",
        "      user1 = item[0][i]\n",
        "      action1 = item[1][i]\n",
        "      context1 = item[2][i]\n",
        "      reward1 = item[3][i]\n",
        "    except:\n",
        "      break\n",
        "    \n",
        "    model_wide.update(user1,context1,action1,reward1)\n",
        "    model_deep.update(user1,context1,action1,reward1)\n",
        "    model_wide_deep.update(user1,context1,action1,reward1)\n",
        "\n",
        "#clear_output()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 44s, sys: 3.2 s, total: 2min 48s\n",
            "Wall time: 2min 48s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOX6E3g_tqKC"
      },
      "source": [
        "## Save models\n",
        "#model_wide.save(path+'ds_model_wide.pkl')\n",
        "#model_deep.save(path+'ds_model_deep.pkl')\n",
        "#model_wide_deep.save(path+'ds_model_wide_deep.pkl')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jo7jqoNxuQUf"
      },
      "source": [
        "## Load existing models\n",
        "#model_wide = load_model(path+'ds_model_wide.pkl')\n",
        "#model_deep = load_model(path+'ds_model_deep.pkl')\n",
        "#model_wide_deep = load_model(path+'ds_model_wide_deep.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_M71aRjoO8B"
      },
      "source": [
        "Get training accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZkmSbwd0jD2",
        "outputId": "7c7796d7-aef0-4e51-dd2a-cdee64e05c0f"
      },
      "source": [
        "%%time\n",
        "tbar = tqdm(train_dataloader)\n",
        "\n",
        "y_true_train = []\n",
        "y_pred_deep_train = []\n",
        "y_pred_wide_train = []\n",
        "y_pred_wide_deep_train = []\n",
        "\n",
        "num_correct_deep_train = 0\n",
        "num_correct_wide_train = 0\n",
        "num_correct_wide_deep_train = 0\n",
        "total_train = 0\n",
        "\n",
        "## Context means and std for scaling \n",
        "context_means = data_snippet[context_col].mean().values\n",
        "context_std = data_snippet[context_col].std().values\n",
        "\n",
        "for item in tbar:\n",
        "  for i in range(batch_size):\n",
        "    try:\n",
        "      user1 = item[0][i]\n",
        "      action1 = item[1][i]\n",
        "      context1 = item[2][i]\n",
        "      context1_scaled = (context1 - context_means) / context_std ## Scale context for input \n",
        "      reward_1 = item[3][i]\n",
        "      reward_o = item[4][i]\n",
        "    except:\n",
        "      break\n",
        "\n",
        "    if reward_o >= 0.0:\n",
        "      best_action = 1\n",
        "    else:\n",
        "      best_action = 0\n",
        "      \n",
        "    y_true_train.append(best_action)\n",
        "\n",
        "    ## Get best action: REMEMBER TO SCALE CONTEXT!!!!\n",
        "    model_action_deep = model_deep.action(user1,context1_scaled) \n",
        "    model_action_wide = model_wide.action(user1,context1_scaled) \n",
        "    model_action_wide_deep = model_wide_deep.action(user1,context1_scaled) \n",
        "\n",
        "    if best_action == model_action_deep:\n",
        "      num_correct_deep_train += 1\n",
        "    if best_action == model_action_wide:\n",
        "      num_correct_wide_train += 1\n",
        "    if best_action == model_action_wide_deep:\n",
        "      num_correct_wide_deep_train += 1\n",
        "    \n",
        "    total_train += 1\n",
        "\n",
        "    y_pred_deep_train.append(model_action_deep)\n",
        "    y_pred_wide_train.append(model_action_wide)\n",
        "    y_pred_wide_deep_train.append(model_action_wide_deep)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(\"Accuracy of wide model = \", num_correct_wide_train / total_train)\n",
        "print(\"Accuracy of deep model = \", num_correct_deep_train / total_train)\n",
        "print(\"Accuracy of wide and deep model = \", num_correct_wide_deep_train / total_train)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of wide model =  0.7836607522204463\n",
            "Accuracy of deep model =  0.7804641188014648\n",
            "Accuracy of wide and deep model =  0.8475529568859126\n",
            "CPU times: user 1min, sys: 1.43 s, total: 1min 2s\n",
            "Wall time: 1min 2s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehHUDTbj13q3",
        "outputId": "34e991c9-a5dc-4bdf-b074-350ddca34da4"
      },
      "source": [
        "confusion_matrix(y_true_train,y_pred_deep_train)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[14326,  5701],\n",
              "       [ 5278, 24667]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af2FRvZz5NPS",
        "outputId": "ed4b18be-8891-4c91-98e2-301f48e45dbb"
      },
      "source": [
        "confusion_matrix(y_true_train,y_pred_wide_train)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[13626,  6401],\n",
              "       [ 3187, 26758]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0IUXtC_5OdL",
        "outputId": "9e718be0-b06b-4824-e9e5-1911f8789cfa"
      },
      "source": [
        "confusion_matrix(y_true_train,y_pred_wide_deep_train)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[15239,  4788],\n",
              "       [ 2510, 27435]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX21SjlPoRJf"
      },
      "source": [
        "Get validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SqgzB_e0ynS",
        "outputId": "8c1a4d40-24fd-47df-c0d0-b9f8b5d54fa3"
      },
      "source": [
        "%%time\n",
        "tbar = tqdm(val_dataloader)\n",
        "\n",
        "y_true_val = []\n",
        "y_pred_deep_val = []\n",
        "y_pred_wide_val = []\n",
        "y_pred_wide_deep_val = []\n",
        "\n",
        "num_correct_deep_val = 0\n",
        "num_correct_wide_val = 0\n",
        "num_correct_wide_deep_val = 0\n",
        "total_val = 0\n",
        "\n",
        "#n_user_unseen = 0\n",
        "\n",
        "## Context means and std for scaling \n",
        "context_means = data_snippet[context_col].mean().values\n",
        "context_std = data_snippet[context_col].std().values\n",
        "\n",
        "for item in tbar:\n",
        "  for i in range(batch_size):\n",
        "    try:\n",
        "      user1 = item[0][i]\n",
        "      action1 = item[1][i]\n",
        "      context1 = item[2][i]\n",
        "      context1_scaled = (context1 - context_means) / context_std ## Scale context for input \n",
        "      reward_1 = item[3][i]\n",
        "      reward_o = item[4][i]\n",
        "    except:\n",
        "      break\n",
        "\n",
        "    if reward_o >= 0.0:\n",
        "      best_action = 1\n",
        "    else:\n",
        "      best_action = 0\n",
        "      \n",
        "    y_true_val.append(best_action)\n",
        "\n",
        "    ## Predict best action: REMEMBER TO SCALE CONTEXT!!!!\n",
        "    model_action_deep = model_deep.action(user1,context1_scaled) \n",
        "    model_action_wide = model_wide.action(user1,context1_scaled) \n",
        "    model_action_wide_deep = model_wide_deep.action(user1,context1_scaled) \n",
        "\n",
        "    if best_action == model_action_deep:\n",
        "      num_correct_deep_val += 1\n",
        "    if best_action == model_action_wide:\n",
        "      num_correct_wide_val += 1\n",
        "    if best_action == model_action_wide_deep:\n",
        "      num_correct_wide_deep_val += 1\n",
        "    \n",
        "    total_val += 1\n",
        "\n",
        "    y_pred_deep_val.append(model_action_deep)\n",
        "    y_pred_wide_val.append(model_action_wide)\n",
        "    y_pred_wide_deep_val.append(model_action_wide_deep)\n",
        "\n",
        "    if model_wide_deep.lookup_one_user_freq(torch.tensor(user1)) == 0:\n",
        "      n_user_unseen += 1\n",
        "\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(\"Accuracy of wide model = \", num_correct_wide_val / total_val)\n",
        "print(\"Accuracy of deep model = \", num_correct_deep_val / total_val)\n",
        "print(\"Accuracy of wide and deep model = \", num_correct_wide_deep_val / total_val)\n",
        "#print(\"Number of users not seen by model = \", n_user_unseen)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of wide model =  0.5897871651695396\n",
            "Accuracy of deep model =  0.782471473658655\n",
            "Accuracy of wide and deep model =  0.7857085053006393\n",
            "Number of users not seen by model =  10898\n",
            "CPU times: user 15.4 s, sys: 334 ms, total: 15.7 s\n",
            "Wall time: 15.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GueEcZN1MIe3",
        "outputId": "4bc84f10-444a-4c1d-8073-b856b8745995"
      },
      "source": [
        "confusion_matrix(y_true_val,y_pred_deep_val)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3343, 1632],\n",
              "       [1056, 6326]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzdAKhpwPCjI",
        "outputId": "206f690d-3fef-4e98-f5f3-c98be779559c"
      },
      "source": [
        "confusion_matrix(y_true_val,y_pred_wide_val)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 110, 4865],\n",
              "       [ 204, 7178]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXI1fdu_PFD0",
        "outputId": "701e2ca3-9785-4efc-d8ad-0747faa57cd4"
      },
      "source": [
        "confusion_matrix(y_true_val,y_pred_wide_deep_val)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3571, 1404],\n",
              "       [1244, 6138]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}